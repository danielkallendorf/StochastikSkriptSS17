\documentclass[10pt,a4paper]{article}

\usepackage{luatex85}
\def\pgfsysdriver{pgfsys-pdftex.def}
\usepackage[utf8]{luainputenc}
\usepackage{fontspec}

\usepackage[german]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{tikz,pgf}
\usetikzlibrary{cd}
\usetikzlibrary{babel}
\usepackage{mathrsfs}
\usepackage{framed}
\usepackage{ulem}
\usepackage{tabularx}
\usepackage{csquotes}
\usepackage{dsfont}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage[hidelinks]{hyperref}

\setlist[enumerate,1]{label=\alph*)}
\setlist[enumerate,2]{label=(\roman*)}

\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\Q}{\ensuremath{\mathbb{Q}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\C}{\ensuremath{\mathbb{C}}}


\newcommand{\la}{\ensuremath{\lambda}}
\newcommand{\al}{\ensuremath{\alpha}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\ul}[1]{\underline{#1}}
\newcommand{\todomark}[1]{\fbox{\Large Hier könnte \sout{Ihre Werbung} #1 stehen}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\scp}[1]{\left\langle#1\right\rangle}
\newcommand{\mapsfrom}{\ensuremath{\mathrel{\reflectbox{\mapsto}}}}
\newcommand{\cha}{\mathds{1}}
\newcommand{\Bor}{\mathscr B}
\newcommand{\indep}{\perp\!\!\!\perp}
\newcommand{\bigtimes}{\operatornamewithlimits{\times}}
\newcommand{\identity}{\mathds{1}}
\newcommand{\Kern}{\operatorname{Kern}}
\newcommand{\Img}{\operatorname{Im}}

\newcommand{\Potset}{\mathscr P}
\newcommand{\Prb}{\mathbb P}
\newcommand{\Epv}{\ensuremath{\mathbb{E}}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Kor}{\rho}

\newcommand{\scA}{\mathscr A}
\newcommand{\scB}{\mathscr B}
\newcommand{\scE}{\mathscr E}
\newcommand{\scF}{\ensuremath{\mathscr{F}}}
\newcommand{\scG}{\mathscr G}
\newcommand{\scL}{\mathscr L}
\newcommand{\scO}{\mathscr O}

\newcommand{\Mltn}{\operatorname{M}}
\newcommand{\Bin}{\operatorname{B}}
\newcommand{\Hypg}{\operatorname{H}}
\newcommand{\Ber}{\operatorname{Ber}}
\newcommand{\Poi}{\operatorname{Poi}}
\newcommand{\Exp}{\operatorname{Exp}}
\newcommand{\Nv}{\mathscr N}

\newcommand{\Bias}{\operatorname{Bias}}
\newcommand{\MSE}{\operatorname{MSE}}

\newcommand{\SM}{\big(\Omega,\scF,(\Prb_\theta)_{\theta\in\Theta}\big)}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lem}[theorem]{Lemma}
\newtheorem{kor}[theorem]{Korollar}
\newtheorem{satz}[theorem]{Satz}
\newtheorem{rem}[theorem]{Erinnerung}
\newtheorem*{rem*}{Erinnerung}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{prop}[theorem]{Proposition}

\theoremstyle{remark}
\newtheorem{bem}[theorem]{Bemerkung}
\newtheorem*{bem*}{Bemerkung}
\newtheorem{exm}[theorem]{Beispiel}
\newtheorem*{exm*}{Beispiel}

\renewcommand{\thesubsection}{\thesection.\Alph{subsection}}

\newcounter{exmlistitem}
\newenvironment{exmlist}
	{\let\oldtheorem\thetheorem
	\renewcommand{\thetheorem}{\oldtheorem.\arabic{exmlistitem}}
	\stepcounter{theorem}
	\let\oldexm\exm
	\let\oldendexm\endexm
	\renewenvironment{exm}{\addtocounter{theorem}{-1}\stepcounter{exmlistitem}\oldexm}{\oldendexm}}
	{\setcounter{exmlistitem}{0}}

\newcommand{\autotag}{\stepcounter{equation}\tag{Gl. \arabic{section}.\arabic{equation}}}

\title{Einführung in die Stochastik}
\author{}

\begin{document}
	\maketitle
	\tableofcontents
	\newpage
%VL 18.04.2017
	
	\section{Grundlagen}
	\begin{rem}Naive Grundidee der Modellierung des Zufalls:\\
		\\
	\begin{tabularx}{\textwidth}{X|c|c}
		Konzept & mathematisches Objekt & Symbol\\ \hline
		\enquote{Alle denkbaren Ergebnisse eines zufälligen Geschehens} & Menge & $\Omega$\\
		Wahrscheinlichkeit,dass $\omega\in\Omega$ beobachtet wird & Abbildung $\Omega\to[0,1]$ &\\
		Alle denkbaren Ja-Nein-Fragen, die zum zufälligen Geschehen gestellt werden können & Potenzmenge von $\Omega$ & $\Potset(\Omega)$\\
		Wahrscheinlichkeit, dass die zu $a\in\Potset(\Omega)$ gehörige Frage mit ``ja'' beantwortet wird. & Abbildung $\Potset(\Omega)\to [0,1]$ & $\Prb$
	\end{tabularx}
	\end{rem}
	
	\begin{exmlist}
		\begin{exm}[6-Seitiger Würfel]
			$\Omega=\{1,2,3,4,5,6\}$ und $\foreach \i in {1,...,6} {p(\i)=}\frac{1}{6}$.\\
			Ein Beispiel für eine Ja-Nein-Frage: \enquote{Ist die gewürfelte Zahl durch $3$ teilbar?} dann ist $A\in\Potset(\Omega):A=\{3,6\}$ und $\Prb(A)=\frac{1}{3}$.
		\end{exm}
		\begin{exm}
			Speziell zufälllige natürliche Zahl: $\Omega=\N$, $p(1)=\frac{1}{2},p(2)=\frac{1}{4},...,p(n)=2^{-n}$.\\
			Dann gilt $\sum_{\omega=1}^{\infty}p(\omega)=1$.
			\begin{enumerate}
				\item Ja-Nein-Frage: \enquote{Ist die Zahl gerade?}\\
				Zugelassenen Menge: $A=\{2,3,6,...\}$
				\[\Prb(A)=\sum_{j=1}^{\infty}p(2j)=\sum_{j=1}^{\infty}2^{-2j}=\frac{1}{1-\frac{1}{4}}-1=\frac{1}{3}\]
				\item Ja-Nein-Frage: \enquote{Ist die Zahl Primzahl?}\\
				Zugelassene Menge: $B=\{n\in\N\mid\text{$n$ ist prim}\}$
				\[\Prb(B)=\sum_{j\in B}p(j)=???\]
				Abschätzung $\Prb(B)\leq 1-\Prb(A)+\Prb(\{2\})\leq \frac{2}{3}+\frac{1}{4}$
			\end{enumerate}
		\end{exm}
	\end{exmlist}
	
	\begin{definition}\label{0103def}
		Sei $\Omega$ eine abzählbare Menge.\\
		Eine Abbildung $p:\Omega\to[0,1]$ mit $\sum_{\omega\in\Omega}p(\omega)=1$ heißt \textbf{Zähldichte} oder \textbf{Wahrscheinlichkeitsdichte} auf $\Omega$.
	\end{definition}

	\begin{definition}\label{0104def}
		Man nennt dann $\Omega$ den \enquote{Ergebnisraum}, die \enquote{Grundmenge} oder \enquote{Grundgesamtheit}.\\
		Ein spezielles $A\in\Potset(\Omega)$ nennt man \enquote{Ergebnis} und falls $A=\{\omega\}$ \textbf{Elementarereignis}.
	\end{definition}

	\begin{definition}\label{0105def}
		Sei $\Omega$ eine abzählbare Menge und $\Potset(\Omega)$ die Potenzmenge, $p$ sei eine Zähldichte.\\
		Dann heißt die Abbildung 
		\[\Prb:\Potset(\Omega)\to[0,1],A\mapsto \sum_{\omega\in A}p(\omega)\]
		 das von $p$ erzeugte \textbf{Wahrscheinlichkeitsmaß} (kurz W-Maß).
	\end{definition}
	
	\begin{bem}\label{0106bem}
		Beachte: $p$ wird in der Notation unterdrückt. Alternativ schreibe $\Prb_p$.\\
		Außerdem: Statt $\Prb(\{\omega\})$ wird oft $\Prb(\omega)$ geschrieben.
	\end{bem}
	
	\begin{lem}
		Sei $p$ eine Zähldichte auf $\Omega$. Das von $p$ erzeugte Wahrscheinlichkeitsmaß hat folgende Eigenschaften:
		\begin{enumerate}
			\item $\Prb(\Omega)=1$
			\item Falls $(A_n)_{n\in\N}$ eine Folge von paarweise disjunkten Ereignissen ist, dann ist $\Prb(\bigcap_{n=1}^\infty A_n)=\sum_{n=1}^{\infty}\Prb(A_n)$.
		\end{enumerate}
	\end{lem}
	\begin{proof}
		Sei $p$ Zähldichte auf $\Omega$.
		\begin{enumerate}
			\item Nach Definition \ref{0105def}: $\Prb(\Omega)=\sum_{\omega\in\Omega}p(\omega)=1$
			\item Es gilt:
			\[\tag{1}
			\Prb\left(\bigcap_{n=1}^\infty A_n\right)=\Prb\big(\{\omega\in\Omega:\text{$\exists n\in\N$ mit $\omega\in A_n$}\}\big)
			=\sum_{\{\omega\in\Omega:\text{$\exists n\in\N$ mit $\omega\in A_n$}\}} p(\omega)
			\]
			Sei nun $N(\omega)=\sum_{n=1}^{\infty}\cha_{A_n}(\omega)=$\enquote{Anzahl der $A_n$ die  $\omega$ enthalten}. Dann ist 
			\begin{align*}
			\sum_{n=1}^\infty\Prb(A_n)=\sum_{n=1}^\infty\sum_{\omega\in A_n}	\tag{2} p(\omega)=\sum_{\{\omega\in\Omega:\text{$\exists n\in\N$ mit $\omega\in A_n$}\}}p(\omega)N(\omega)\\
			\end{align*}
			Da aber die $A_n$ paarweise disjunkt sind ist $N(\omega)=1$ für alle $\omega\in{\{\omega\in\Omega:\text{$\exists n\in\N$ mit $\omega\in A_n$}\}}$, ist ist (1)=(2).\qedhere
		\end{enumerate}
	\end{proof}

	\begin{definition}
		Für $A\subseteq\Omega$ heißt 
		\[\cha_A:\Omega\to\{0,1\},\quad \omega\mapsto\begin{cases}
		1&\omega\in A\\0&\text{sonst}
		\end{cases}\]
		die \textbf{Charakteristische Funktion} oder \textbf{Indikator} von $A$. Man schreibt auch $\mathds{1}_A$.
	\end{definition}
	
	\begin{exm}
		Sei $\Omega=\N$, $(a_n)_{n\in N}$ eine Folge mit $a_n\ge 0$, $a:=\sum_{n=1}^\infty a_n\le \infty$ und $a>0$.\\
		Dann ist $p:\Omega\to[0,1]$, $n\mapsto p(n):=\frac{a_n}{a}$ eine Zähldichte.\\
		Es gilt sogar die Isomorphie: 
		\[\{\text{Zähldichte auf $\N$}\}\cong \{\text{Nicht negative Folgen mit $\sum_{n=1}^{\infty}a_n=1$}\}\cong\{\text{Nichtnegative summierbare Folgen $\neq 0$}\}\]
	\end{exm}
	
	\begin{bem}[Notation]
		Sei $\Omega$ eine Menge .$|\Omega|\le\infty$ bezeichne tdie Anzahl der Elemente von $\Omega$ \enquote{Mächtigkeit der Menge}.
	\end{bem}

%VL 24.04.2017
	\begin{exmlist}
		\begin{exm}[Einfache Irrfahrt]\label{0111aexm}
			 Dimension $d$, $N$ Schritte. $\Omega=\{(x_0,x_1,...,x_N):X_j\in\Z^d\forall j, x_0=0,|x_{j+1}-x_j|=1\forall j\}$.\\
			Also ist $|\Omega_N|=(2d)^N$, Setze $p(\omega=\frac{1}{(2d)^N})\forall\omega\in\Omega$.\\\\
			Man kann nu folgende Fragestellungen formulieren:
			\begin{enumerate}
				\item $A_N:=\{(x_1,...,x_N)\}\in\Omega_N:\exists j>0$ mit $x_j=0\}$. (``Rückkehr zum Startpunkt'').\\
				Es ist klar, dass $\Prb(A_N)\ge\frac{1}{2d}>0$, falls $N\ge 2$.\\
				Es ist leicht zu zeigen, dass $N\mapsto\mathbb{P}(A_N)$ wächst monoton.\\
				\begin{description}
					\item[Knifflig:] Was ist $\lim\limits_{N\to\infty}$? $<1$? $=1$?
					\item[Antwort:] $=1$ für $d\leq 2$, $<1$ für $d\geq 3$.
				\end{description}
				\item $B_n,\al:=\left\{\omega=(x_0,x_1,...,x_N)\in\Omega_N:|x_N|\ge N^\alpha\right\}$ für $0<\alpha\leq 1$
				\begin{description}
					\item[Frage:] $\lim\limits_{N\to\infty}\Prb(B_{n,\al})$?
					\item[Antwort:] $0$, falls $\alpha>\frac{1}{2}$\\
					$1$, falls $\alpha< \frac{1}{2}$\\
					Für $\alpha=\frac{1}{2}$ gilt 
					\[\lim\limits_{N\to\infty}\Prb(B_{n,\al})=\frac{V_k(d)}{(2\pi)^{\frac{d}{2}}}\int_{1}^{\infty}r^{d-1}\exp(\frac{1}{2}r^2)dr\]
					(dabei ist $V_k(d)$ das Volumen der $d$-Dimensionalen Einheitskugel).
				\end{description}
			\end{enumerate}
		\end{exm}
		\begin{exm}[Selbstvermeidende Irrfahrt] Dimension $d$, $N$ Schritte.
			\begin{enumerate}
				\item $\Omega_N^0=\left\{(x_0,x_1,...,x_N)\in\Omega_N:x_i\neq x_j\text{ falls }i\neq j\right\}$
				Dann gilt für die Anzahl der Pfade:
				\[|\Omega_N^0|=\begin{cases}
				2,&\text{falls $d=1$}\\??,&\text{falls $d>1$}
				\end{cases}\]
				und es ist $p(\omega)=\frac{1}{|\Omega_N^0|\forall\omega\in\Omega_N^0}$.
				\item Wie in a)2. 
				\begin{description}
					\item[Frage] Was ist $\lim\limits_{N\to\infty}\Prb(B_{N,\al}^0)$.
					\item[Bekannt] $\exists\al_c>0$ mit \[\lim\limits_{N\to\infty}\mathbb{ P(B_{n,\al}^0)}=\begin{cases}
					0,&\text{falls $\al>\al_c$}\\1,&\text{falls $\al<\al_c$}
					\end{cases}\]
					\item[Bekannte Werte:]
					\begin{description}
						\item[$d=1$] $\al_c=1$
						\item[$d=2$] $\al_c=\frac{3}{4}$, falls SLE-Conjecture stimmt
						\item[$d=3$] $\al_c\approx 0,5876$ (Numerik)
						\item[$d\ge 4$] $\al_c=\frac{1}{2}$
					\end{description}
				\end{description}
			\end{enumerate}
		\end{exm}
	\end{exmlist}

	\begin{exm}
		Auswählen einer Zufälligen reellen Zahl in $[0,1]$, alle Zahlen sollen di gleich Wahrscheinlichkeit haben:\\
		$[0,1]$ ist nicht endlich, also ist Gleiche Wahrscheinlichkeit für alle Zahlen unmöglich.\\
		$[0,1)$ ist nicht abzählbar, also scheitert der bisherige Ansatz mit der Zähldichte.\\
		\begin{description}
			\item[Ein möglicher Ausweg]
			Definiere $\Prb([a,b])=\Prb((a,b))=\Prb([a,b))=\Prb((a,b])$.\\
			Die Erweiterung, sodass $\forall A\in\Potset([0,1])$ $\Prb(A)$ definiert ist, ist nicht möglich.
			\item[Lösung] Definiere $\Prb$ nicht auf allen Mengen $\Potset([0,1])$.
		\end{description}
	\end{exm}
	\begin{definition}
		Sei $\Omega$ eine nichtleere Menge.\\
		Ein Mengensystem $\mathscr F\subset \Potset(\Omega)$heißt \textbf{$\sigma$-Algebra}, falls
		\begin{enumerate}
			\item $\Omega\in\mathscr F$
			\item Falls $A\in\mathscr F$, dann auch $A^C\in\mathscr F$.
			\item Falls $A_1,A_2,...\in\mathscr F$, dann auch $\bigcap A_i\in\mathscr F$.
		\end{enumerate}
	$(\Omega,\mathscr F)$ heißt dann \textbf{messbarer Raum} oder \textbf{Ereignisraum}.
	\end{definition}

	\begin{bem}
		$\mathscr F$ ist ``die Menge aller Teilmengen von $\Omega$, für die die zugehörige Ja-Nein-Frage beantwortbar ist''.\\
		Daher meint
		\begin{enumerate}
			\item ``Ist $\omega\in\Omega$'' muss beantwortbar sein.
			\item Falls ``Ist $\omega\in A?$'' beantwortbar, so ist auch ``Ist $\omega\notin A?$'' beantwortbar.
			\item Falls ``Ist $\omega\in A_i?$'' beantwortbar für alle $i$, dann ist auch ``Ist $\omega$ in irgendeinem $A_i$?'' beantwortbar.
		\end{enumerate}
	\end{bem}

	\begin{exm}
		Sei $\Omega=[0,1)$, dann ist \begin{enumerate}
			\item $\mathscr F_0:=\{\emptyset,\Omega\}$
			\item $\mathscr F_1:=\{\emptyset,[0,\frac{1}{3}),[\frac{1}{3},1),\Omega\}$.\\
			Die Frage ``Ist $\omega\ge \frac{1}{2}$'' ist hier \underline{nicht} beantwortbar!
			\item $A_{j,n}:=\left[\frac{j}{n},\frac{j+1}{n}\right)$, $n$ ist fest, $j\ge n$.\\
			$\mathscr F_2=\left\{\bigcup_{k=1}^{n}B_{k,n}:B_{k,n}\in\{\emptyset,A_{k,n}\}\right\}$
			\item $\mathscr F_3=\Potset(\Omega)$ ist ebenfalls eine $\sigma$-Algebra.
		\end{enumerate}
	\end{exm}
	
	\begin{satz}
		Sei $\mathscr G\subset\Potset(\Omega)$ ein Mengensystem. Sei $\Sigma:=\{\mathscr A\subset \Potset(\Omega):\text{$\mathscr A$ ist $\sigma$-Algebra und $\mathscr G\subset\mathscr A$}$.\\
		Dann ist auch $\bigcap_{\mathscr A\in\Sigma}\mathscr A$ eine $\sigma$-Algebra.
	\end{satz}
	%TODO counter
	\addtocounter{theorem}{-1}
	\begin{definition}
		$\sigma(\mathscr G):= \bigcap_{\mathscr A\in\Sigma}\mathscr A$ heißt \textbf{die von $\mathscr G$ erzeugt $\sigma$-Algebra}.
	\end{definition}

	\begin{definition}
		Sei $\Omega=\R$, $\mathscr G:=\{[a,b]:a,b\in\R,a<b\}$.\\
		$\mathscr B:=\sigma(\mathscr G)$ heiß \textbf{Borel-$\sigma$-Algebra}.
	\end{definition}

	\begin{bem}
		\begin{enumerate}
			\item $\mathscr B$ enthält alle offenen Mengen, alle abbgeschlossen Mengen und alle halboffenen Intervalle.
			\item $\mathscr B\subsetneqq \Potset (\Omega)$.
			\item $\mathscr B$kann nicht abzählbar konstruiert werden.
			\item $\mathscr B=\sigma(\{(-\infty,c]\}:c\in\R)$.
			\item Falls $\Omega_o
			0\subset\R$, $\Omega_0\neq \emptyset$, dann ist
			\[\mathscr B_{\Omega_0}:=\{ A\cap\Omega_0:A\in \mathscr B(\R)\}\]
		eine $\sigma$-Algebra, die \textbf{Einschränkung} von $\mathscr B$ auf $\Omega_0$.
		\end{enumerate}
	\end{bem}

	\begin{definition}
		Seien $E_1,E_2,...,E_N$ Mengen, $N\leq \infty$.\\
		$\mathscr E_i$ seien $\sigma$-Algebren auf $E_i$ und es sei
		\[\Omega=\operatornamewithlimits{X}_{i=1}^NE_i=\left\{(e_1,...,e_N):e_i\in E_i\forall i\leq N\right\}\]
		Eine Menge der Form
		\[A_{j,B_j}=\left\{(e_1,...,e_N):e_j\in B_j,\text{ andere $e_k$ beliebig}\right\}\]
		mit $B_j\in\mathscr E_j,j\leq N$ heißt \textbf{Zylindermenge}.\\
	\end{definition}
	%TODO COUNTER
	\addtocounter{theorem}{-1}
	\begin{definition}
		Die $\sigma$-Algebra in $\Omega$ die von allen Zylindermengen Erzeugt wird heißt \textbf{Produkt-$\sigma$-Algebra}.
		Man nennt $\mathscr Z$ das System der Zylindermenge und $\mathscr E_1\otimes\mathscr E_2\otimes...\otimes \mathscr E_N:=\sigma(\mathscr Z)$.
	\end{definition}

%VL 25.04.2017
	\begin{definition}\label{0120def}
		Sei $\Omega,\scF$ ein messabere Raum. Eine Abbildung $\Prb:\scF\to [0,1]$ heiß \textbf{Wahrscheinlichkeitsmaß} (W-Maß) auf $\scF$ (teilweise auch \enquote{auf $\Omega$}), falls
		\begin{enumerate}[label=\alph*)]
			\item $\Prb(\Omega)=1$
			\item Für paarweise disjunkte $A_n\in\scF$ gilt $\Prb(\bigcap_{i=1}^\infty)=\sum_{i=1}^\infty \Prb(A_i)$ (\enquote{$\sigma$-Additivität}).
		\end{enumerate}
	Dann heißt $\Omega,\scF,\Prb$ \textbf{Wahrscheinlichkeitsraum}.
	\end{definition}

	\begin{exm}\label{0122exm}
		Sei $(\Omega,\scF,\Prb)=(\R,\Bor(\R),\delta_x)$. Dabei ist $\Prb(A)=\begin{cases}
		1;&\text{falls $x\in A$}\\0,&\text{sonst}.
		\end{cases}$ ein Wahrscheinlichkeitsmaß auf $\Bor(\R)$. \\
		Es Modelliert ein \enquote{Zufalls}-Experiment, welches sicher $x$ ergibt.
	\end{exm}

	\begin{satz}[Eigenschaften von Wahrscheinlichkeitsmaßen]\label{0122satz}
	Sei $(\Omega,\scF,\Prb)$ ein Wahrscheinlichkeitsraum und $A,B,A_1,A_2,...\in\scF$. Dann ist
	\begin{enumerate}[label=\alph*)]
		\item $\Prb(\emptyset)=0$
		\item $ẞ\Prb(A\cup B)=\Prb(A)+\Prb(B)-\Prb(A\cap B)$
		\subitem insbesondere $\Prb(A)+\Prb(A^C)=1$
		\item Falls $A\subseteq B$, dann ist $\Prb(A)\le\Prb(B)$ (\enquote{Monotonie})
		\item $\Prb\left(\bigcup_{i=1}^\infty A_i\right)\leq \sum_{i=1}^\infty \Prb(A_i)$ ($\sigma$-Subadditivität)
		\item Falls $A_n \nearrow A_n$ (d.h. $A_1\subseteq A_2\subseteq...$ und $\bigcap_{n=1}^\infty A_n=A$), \\
		dann ist $\lim\limits_{n\to\infty}\Prb(A_n)=\Prb(A)$ (\enquote{$\sigma$-Stetigkeit})\\
		Falls $A_n \searrow A_n$ (d.h. $A_1\supseteq A_2\supseteq...$ und $\bigcup_{n=1}^\infty A_n=A$), \\
		dann ist $\lim\limits_{n\to\infty}\Prb(A_n)=\Prb(A)$
	\end{enumerate}
	\end{satz}
	\begin{proof}
		\begin{enumerate}[label=\alph*)]
			\item $\Prb(\emptyset)=\Prb\left(\bigcup_{n=1}^\infty \emptyset\right)=\sum_{n=1}^{\infty}\Prb(\infty)$. Also $\Prb(\emptyset)=0$
			
			\item Falls $A\cap B=\emptyset$, dann ist 
			\[\Prb(A\cup B)=\Prb(A\cup B\cup\emptyset\cup...\cup\emptyset)=\Prb(A)+\Prb(B)+0\]
			Sei nun $A\cap B\neq 0$. Allgemein gilt
			\begin{align*}
			\Prb(A)&=\Prb(A\setminus B)+\Prb(A\cap B)\\
			\Prb(B)&=\Prb(B\setminus A)+\Prb(B\cap A)
			\end{align*}
			Außerdem gilt für disjunkte $A,B$
			\begin{align*}
			\Prb(A\cup B)&=\Prb\big((A\setminus B)\cup(B\setminus A)\cup(A\cap B)\big)\\
			\text{disjunkte Mengen}\,&=\Prb(A\setminus B)+\Prb(B\setminus A)+\Prb(A\cap B)\\
			&=\Prb(A)-\Prb(A\cap B)+\Prb(B)-\Prb(B\cap A)+\Prb(A\cap B)\\
			&=\Prb(A+\Prb(B)-\Prb(A\cap B)
			\end{align*}
			
			\item Aus $A\subseteq B$ folgt, dass 
			\[\Prb(B)=\Prb(A)+\Prb(B\setminus A)\]
			Dabei ist $\Prb(B\setminus A)\geq 0$, also $\Prb(B)\geq \Prb(A)$.
			
			\item Sei $B_i:= A_I\setminus\left(\bigcup_{j=1}^{i-1}A_j\right)$.\\
			Dann gilt für alle $N\in\N$, dass
			\begin{align*}
			\bigcup_{i=1}^N A_i=\bigcup_{i=1}^N B_i
			\end{align*}
			Da die $B_i$ disjunkt sind gilt
			\[\Prb\left(\bigcap_{i=1}^\infty A_i\right)=\Prb\left(\bigcap_{i=1}^\infty B_i\right)\overset{\ref{0120def}b)}{=}\sum_{i=1}^{\infty}\Prb(B_i)\overset{B_i\subseteq A_i}{\leq}\sum_{i=1}^{\infty}\Prb(A_i)\]
			
			\item Aus $A_n\nearrow$ folgt, dass $A_n=\bigcup_{i=1}^nA_i\setminus A_{i-1}$ als disjunkte Vereinigung darstellen lässt. Somit ist
			\begin{align*}
			\Prb(A)&=\sum_{i=1}^{\infty}A_i\setminus A_{i-1}
			=\liminf_{n\to\infty}\sum_{i=1}^{n}\Prb(A_i\setminus A_{i-1})\\
			\text{mit \ref{0120def}b)}&=\liminf_{n\to\infty}\Prb\left(\bigcup_{i=1}^n A_i\setminus A_{i-1}\right)=\liminf_{n\to\infty}\Prb(A_n).
			\end{align*}
			Falls $A_n\searrow A$ analog durch betrachten der Komplemente.
		\end{enumerate}
	\end{proof}
	
	\begin{bem*}[Frage]
		Wie kann man überprüfen ob zwei Wahrscheinlichkeitsmaße $\Prb,\tilde{\Prb}$ gleich sind?\\
		Nach Definition ist $\Prb=\tilde{\Prb}\Leftrightarrow\Prb(A)=\tilde\Prb(A)$ für alle $A\in\scF$.\\
		Da $\scF$ oft sehr groß ist eigene sich die folgenden Sätz besser:
	\end{bem*}

	\begin{definition}\label{0123adef}
		Sei $\scF$ eine $\sigma$-Algebra, $\scG\subset\scF$ ein Mengensystem.\\
		$\scG$ heißt \textbf{Erzeuger} von $\scF$, falls $\sigma(\scG)=\scF$.
	\end{definition}
	%TODO COUNTER
	\addtocounter{theorem}{-1}
	\begin{definition}\label{0123bdef}
		Ein Mengensystem $\scG$ heißt \textbf{schnitt-stabil} ($\cap$-stabil), falls für alle $A,B\in\scG$ auch $A\cap B\in \scG$.
	\end{definition}

	\begin{satz}\label{0124satz}
		Sei $\scF$ eine $\sigma$-Algebra, $\scG\subset \scF$ ein $\cap$-stabiler Erzeuger von $\scF$. Dann gilt für zwei Wahrscheinlichkeitsmaße $\Prb,\tilde\Prb$ auf $\scF$:\\
		Falls $\Prb(A)=\tilde\Prb(A)$ für alle $A\in\scG$, dann gilt $\Prb(A)=\tilde\Prb(A)$ für alle $A\in\scF$, also $\Prb=\tilde\Prb$.
	\end{satz}

	\begin{exmlist}
		\begin{exm}
			Sei $\scF=\Bor(\R)$, $\Prb,\tilde\Prb$ Wahrscheinlichkeitsmaße auf $\scF$. Dann folgt aus $\Prb\big((-\infty,x]\big)=\tilde\Prb\big((-\infty,x]\big)$ für alle $x\in\R$, dass $\Prb=\tilde\Prb$.
		\end{exm}
	
		\begin{exm}\label{0125bexm}
			Seien $\scE_j$ $\digamma$-Algebren und sei $\scF=\bigotimes_{j=1}^\infty\scE_j$.\\
			Nach Definition ist das System der Zylindermengen $Z$ ein Erzeuger von $\scF$, aber nicht $\cap$-stabil.\\
			Das System $\tilde Z$ der Mengen der Form
			\[A=\left\{w=(e_1,e_2,...,)\mid e_{j_1}\in B_{j_1},..., e_{j_m}\in B_{j_m}\right\}=(\star)\]
			mit $m\in\N$, $j_1,...,j_m\in\N$, $B_{j_1}\in \scE_{j_1},...,B_{j_m}\in\scE_{j_m}$ beliebig.
			Dann ist
			\[\left\{\{\underbrace{\omega=(e_1,e_2,...)}_{=(\star)}\}\mid m\in\N,j_1,...,j_m\N,B_{j_1}\in\scE_{j_1},...,B_{j_m}\in\scE_{j_m}\right\}\]
			ein $\cap$-stabiler Erzeuger (heißt auch \textbf{Zylindermenge}).\\
			$\tilde Z$ entsteht aus $Z$ durch bildung aller endlichen Schnitte. auf $\tilde Z$ gilt daher \ref{0124satz}.
		\end{exm}
	
		\begin{exm}[Unendlich oft wiederholter Münzwurf]
			Sei 
		\begin{align*}
			\Omega&=\bigtimes_{i=1}^\infty\{0,1\}=\big\{(a_1,a_2,...),a_i\in\{0,1\big\}\\
			\scF&=\bigotimes_{i=1}^\infty\Potset\big(\{0,1\}\big)\\
			\tilde Z&=\left\{\left\{(a_1,a_2,...)\mid a_{j_1}=\ol{a_1},...,a_m=\ol{a_{j_m}}\right\}\mid m\in\N,j_1,...,j_m\in\N,\ol{a_1},...,\ol{a_m}\in\{0,1\}\right\}\\
			&=\text{\enquote{Kenntnis endlich vieler Ergebnisse}}
		\end{align*}
		und $\Prb,\tilde \Prb$ Wahrscheinlichkeitsmaße auf $\scF$.\\
		Dann gilt $\Prb=\tilde\Prb$ genau dann wenn $\Prb(A)=\tilde{\Prb}(A)$ für alle $A\in\tilde Z$.
		\end{exm}
	
		\begin{exm*}[Verallgemeinerung von \ref{0125bexm}]
			Jeder Erzeuger $\scG$ kann zu einem $\cap$-stabiler Erzeuger vergrößert werden, indem man alle Durchschnitte $A_1\cap...\cap A_m$ für alle $A_i\in\scG$ und für alle $m\in\N$ hinzu nimmt.
		\end{exm*}
	\end{exmlist}

	\begin{bem}\label{0126bem}
		Sei $f:\R^n\to[0,\infty]$ eine Funktion und es gelte $\{x\in\R^n,f(x)\leq c\}\in\Bor(\R^n)$. (Dabei ist $\Bor(\R^n)=\bigotimes_{i=1}^n\Bor(\R)$.)\\
		Dann nennt man $f$ Borel-Messbar und das Lebesguq-Intgeral $\int f(x)dx\in\R$ existiert.\\
		Es gilt
		\begin{enumerate}
			\item $\int_{\R^n}f(x) dx$ ist gleich dem Riemann-Integral, falls $f$ Riemann integrierbar ist.
			\item Für eine Folge $(f_k)_{k\in\N}$ Borel-Messbarer Funktionen mit $f_{k+1}(x)\geq f_k(x)$ für alle $x$ (monotone Folge) gilt
			\[\int_{\R^n}\lim_{k\to\infty}f_k(x)~dx=\lim_{k\to\infty}\int_{\R^n}f_n(x)~dx\]
		\end{enumerate}
	\end{bem}

%VL 02.05.2017
	\begin{definition}
		Die Abbildung $\la:\Bor(\R^n)\to[0,\infty], A\mapsto\int_{\R^n}\cha_A(x)~dx=\la(A)$ heißt \textbf{Lebesgue-Maß}
	\end{definition}
	
	\begin{exm}\label{0128exm}
			Sei $\varrho:\R^n\to[0,\infty)$ Borel-messbar und $\int\varrho(x)~dx=1$.\\
			Dann ist die Abbildung $\Prb_p\varrho:\Bor(\R^n)\to[0,1],A\mapsto\int_A\varrho(x)~dx=\int\cha_A(y)\varrho(x)~dx$ ein Wahrscheinlichkeitsmaß.
	\end{exm}

	\begin{definition}
		Sei $\varrho:\R^n\to[0,\infty)$ Borel-messbar und $\int\varrho(x)~dx=1$, (=\ref{0128exm}) dann heißt $\varrho$ \textbf{Dichte} von $\Prb_p\varrho$.
	\end{definition}

	\begin{exm}
		Sei $\varrho$ eine Dichte, $x\in\R$. Dann hat $\Prb:=\frac{1}{3}\Prb_p\varrho+\frac{2}{3}\delta_x$ keine Dichte (siehe \ref{0121exm})
	\end{exm}

	\begin{bem*}
		Wenn $\varrho$ Dichte ist schreibt man auch $\Prb_p\varrho\equiv\varrho(x)~dx$
	\end{bem*}

	\begin{definition}\label{0131def}
		Sei $\Omega\in\Bor(\R^n)$ mit $\la(\Omega)<\infty$. Das Wahrscheinlichkeitsmaß auf $\Omega$ mit Dichte $\varrho(y)=\frac{1}{\la(\Omega)}$ heißt \textbf{Gleichverteilung} auf $\Omega$.\\
		Man fasst dann $\tilde\varrho(x)=\frac{1}{\la(\Omega)}\cha_A\Omega(x)$ als Einbettung in den $\R^n$ auf.
	\end{definition}

	\begin{rem*}[Zufallsvariable]
		Der Begriff \enquote{Zufallsvariable} ist historisch gewachsen. (Keine Variable einer Funktion).
		\begin{description}
			\item[Problemstellung] Von einem komplizierten Zufälligen Geschehen will man nur gewisse Aspekte betrachten.
		\end{description}
	\end{rem*}

	\begin{exmlist}
		\begin{exm}[2 mal Würfeln, Würfelsumme]\label{0132aexm}
			Sei $\Omega:\{1,2,3,4,5,6\}\times\{1,2,4,5,6\}$. $\omega=(\omega_1,\omega_2)$. Der zu betrachtende Aspekt: $S(\omega_1,\omega_2)=\omega_1+\omega_2\in\{2,3,...,12\}\neq \Omega$.
		\end{exm}
		\begin{exm}
			Sei $\Omega=\Omega_N$(siehe \ref{0111aexm}, einfache Irrfahrt).\\
			$\omega=(\omega_1,\omega_2,...\omega_N)\in(\Z^d)^N$.\begin{description}
				\item[Aspekt 1] Position nach $N$ Schritten.\\
				Modell: $X_N\big((\omega_1,...,\omega_N)\big)=\omega_N\in\Z^d\neq \Omega$
				\item[Aspekt 2] Maximaler Abstand vom Ursprung bis zum Schritt $N$.\\
				Modell $M_N\big((\omega_1,...,\omega_N)\big)=\max\{|\omega_j|,j\le N\}$.
			\end{description} 
		\end{exm}
		\begin{exm}
			Sei $\Omega=[0,1]$, $\scF\in\Bor([0,1])$, $\omega=x\in[0,1]$.
			\begin{description}
				\item[Aspekt 1] Erste Ziffer nach dem Komma?\\
				Modell: $y_1(x)=\left\lfloor10x\right\rfloor$
				\item[Aspekt 2] Fläche des Quadrates mit Kantenlänge $x$\\
				Modell: $Q(x)=x^2\in[0,1]$.
			\end{description}
		\end{exm}
	Fazit: Modellierung durch Abbildungen.
	\end{exmlist}

	\begin{definition}
		Seine $(\Omega,\scF)$ und $(\Omega',\scF')$ Ereignisräume.\\
		eine Abbildung $X:\Omega\to\Omega'$ heißt \textbf{Zufallsvariable} (ZV) [oder messbare Abbildung, zufälliges Element von $\Omega'$], falls gilt:\\
		$\forall A'\in\scF'$ ist $X^{-1}(A')\in\scF$.\\
		Hierbei ist $X^{-1}$ das Urbild von $A'$ unter $X$.
	\end{definition}

	\begin{bem}
		Die Urbild-Abbildung bilte Mengen in $\scF'$ (d.h.erlaube Ja-Nein-Fragen) auf Mengen  in $\Potset(\Omega)$ (d.h. Ja-Nein-Fragen) ab.
	\end{bem}

	\begin{exm*}
		In 1.32.1 \ref{0132a} ist $S^{-1}(\{4\})=\{(1,3),(2,2),(3,1)\}$.\\
		In 1.32.1 \ref{0132a} ist $Y_1^{-1}(\{3,7\})=[0,3,0,4)\cup[0,7,0,8)$
	\end{exm*}

	\begin{bem*}
		Die Bedingung (*) bedeutet, dass für alle durch $A\in\scF'$ erzeugte erlaubten Ja-Nein-Fragen auch die Frage \enquote{Liegt $X(\omega)$ in $A'$?} erlaubt ist.
	\end{bem*}
	\begin{bem*}
		Oft nimmar  man nur $\Omega$ und $(\Omega',\scF')$ als gegeben.\\
		Dann ist $X^{-1}(\scF'):=\{X^{-1}(A')\mid A'\in \scF'\}$ die von $X$ erzeugt $\sigma$-Algebra.
	\end{bem*}
	\begin{bem*}
		Falls $\scF=\Potset(\Omega)$, dann ist jede Abbildung eine Zufallsvariable.
	\end{bem*}

	\begin{lem}\label{0135lem}
		Seine $(\Omega,\scF)$ und $(\Omega',\scF')$ Ereignisräume, $X:\Omega\to\Omega'$ und sei $\mathscr G'$ ein Mengensystem mit $\scF'=\sigma(G')$. Dann ist $X$ genau dann Zufallsvariable, wenn $X^{-1}(A')\in\scF\forall A'\in \mathscr G'$.
	\end{lem}
	\begin{proof}
		\begin{description}
			\item[\enquote{$\Rightarrow$}] ist klar, da $\mathscr G'\subset \scF'$
			\item[\enquote{$\Leftarrow$}] Sei $\mathscr A':=\{A'\in\Omega'\mid X^{-1}(A')\in\scF\}$ ist eine $\sigma$-Algebra und $\mathscr A'\supset\mathscr G'$ nach Annahme.\\
			Daher ist $\scF'=\sigma(\mathscr G')\subset\mathscr A'$, sodass $X^{-1}(A')\in\scF\forall A\in\scF$.
		\end{description}
	\end{proof}

	\begin{exmlist}
		\begin{exm}
			Sei $(\Omega',\scF')=(\R,\R)$. Nach \ref{0135lem} gilt:\\
			$X:\Omega\to\Omega'$ ist genau dann Zufallsvariable, wenn $X^{-1}\big((-\infty,c)\big)\in\scF\forall c\in\R$
			Für $\Omega'=\R$ heißt $X$ \textbf{reelle Zufallsvariable}
		\end{exm}
		\begin{exm}
			Es ist $\ol{\R}:=[-\infty,\infty]$ mit $\sigma$-Algebra $\Bor(\ol{\R})=\sigma\big(\{[-\infty-c]:c\in\ol{\R}\}\big)$.\\
			Die Abbildung $X:\Omega\to\ol{\R}$ ist genau dann Zufallsvariable, wenn $X^{-1}\big([-\infty,c]\big)\in\scF\forall c$.\\
			Dann heißt $X$ \textbf{numerische Zufallsvariable}
		\end{exm}
	\end{exmlist}

	\begin{theorem}
		Sei $(\Omega,\scF,\Prb)$ ein Wahrscheinlichkeitsraum, $(\Omega',\scF')$ ein Ereignisraum, $X:\Omega\to\Omega'$ eine Zufallsvariable.\\
		Dann ist die Abbildung
		\[\Prb':\scF'\to[0,1],\quad A'\mapsto\Prb'(A'):=\Prb(X^{-1}('A))\] 
		ein Wahrscheinlichkeitsmaß auf $(\Omega',\scF')$.  
	\end{theorem}
	%TODO COUNTER
	\addtocounter{subsection}{-1}
	\begin{definition}
		$\Prb'$ heißt \textbf{Bildmaß von $\Prb$ unter $X$} oder \textbf{Verteilung von $X$ unter $\Prb$}.\\
		Man schreibt $\Prb'=\Prb\circ X^{-1}$ oder $\Prb'=\Prb_X$
	\end{definition}
	\begin{proof}
		Da $X$ eine Zufallsvariable ist, ist $X^{-1}(A')\in\scF\forall A'\in\scF'$, daher im Definitionsbereich von $\Prb$.\\
		Also ist $\Prb'$ wohldefiniert. Prüfe Definition \ref{0120def}.
		\begin{enumerate}[label=\alph*)]
			\item $\Prb'(\Omega')=\Prb(X^{-1}(\Omega'))=\Prb(\Omega)=1$
			\item $A_1',A_2',...\in \scF'$ seien paarweise disjunkt. Dann sind $X^{-1}(A_1'),X^{-1}(A_2'),...$ auch paarweise disjunkt.
			\begin{align*}
			\Prb'\left(\bigcup_{i=1}^\infty A_i'\right)&=\Prb\left( X^{-1}\left(\bigcup_{i=1}^\infty A_i'\right)\right)\\
			&=\Prb\left(\bigcup_{i=1}^\infty X^{-1}(A_i')\right)\\
			&=\sum_{i=1}\infty\Prb(X^{-1}(A_i'))\\
			&=\sum_{i=1}\infty\Prb'(A_i')
			\end{align*}
		\end{enumerate}
	\end{proof}

	\begin{definition}
		Seien $(\Omega_1,\scF_1,\Prb_1)$ und $(\Omega_2,\scF_2,\Prb_2)$ Wahrscheinlichkeitsräume, $X_1:\Omega_1\to\Omega'_1$, $X_2:\Omega_2\to\Omega'_2$ Zufallsvariablen.\\
		Falls $\Prb_1(X^{-1}_1(A'))=\Prb_2(X_2^{-1}(A'))\forall A'\in\scF'$, dann heißen $X_1$ und $X_2$ identisch verteilt.
	\end{definition}

	\begin{bem}[Notation]
		Man schreibt oft:
		\begin{itemize}
			\item $\{X\in A'\}$ statt $X^{-1}(A')$
			\item $\Prb(\{X\in A'\})$ oder $\Prb(X\in A')$ statt $\Prb(X^{-1}(A'))$
			\item $\Prb_X$ statt $\Prb\circ X^{-1}$.
		\end{itemize}
	\end{bem}

%VL 08.05.2017
%TODO COUNTER ASYNC
\addtocounter{theorem}{-1}
	\begin{definition}\label{0140def}
		Sei $\Prb$ ein Wahrscheinlichkeitsmaß auf $(\R,\Bor(\R))$. Dann heißt die Abbildung $F=F_\R:\R\to[0,1]$ mit $x\mapsto\Prb\big((-\infty,x]\big)$ \textbf{Verteilungsfunktion} von $\Prb$.\\
		Für eine reelle Zufallsvariable heißt $X$ auf $(\Omega,\scF,\tilde{\Prb})$ heißt $F_X:\R\to[0,1]$ mit $x\mapsto\Prb(X\leq x)$ Verteilungsfunktion von $X$.
	\end{definition}

	\begin{satz}\label{0141satz}
		Sei $X$ eine Reelle Zufallsvariable und $F=F_X$ die zugehörige Verteilungsfunktion. Es gilt
		\begin{enumerate}
			\item $F$ wächst monoton
			\item $\lim\limits_{x\to\infty}F(x)=1$, $\lim\limits_{x\to-\infty}F(x)=0$.
			\item $F$ ist steig von rechts (d.h. $\lim\limits_{x\searrow y})F(x)=F(y)$.\\
			Äquivalent dazu: Sei $x_n$ eine monoton wachsende Folge mit $\lim\limits_{n\to\infty}x_n=y$, dann ist $\lim\limits_{n\to\infty}F(x_n)=F(y)$.
			\item Für alle $y\in\R$ existiert der Grenzwert $F(y-):=\lim\limits_{x\nearrow}F(x)$ und $F(y-)=\Prb(X<y)$.
			\item Für alle $y\in\R$ ist $F(y)-F(y-)=\Prb(X=y)$
		\end{enumerate}
	\end{satz}
	\begin{proof}
		Übung %TODO 1.41 Beweis Übung
	\end{proof}
	
	\begin{exm}
		Sei $X:\Omega\to\R$ mit $\Prb(X=-3/4)$ und $\Prb\big(x\in[a,b]\big)=16\int_{a}^{b}y^2~dy$ falls $0\leq a\leq b\leq 1/2$.\\
		Dann ist $\Prb(X\in\R\setminus\{-3/4\}\cup [0,1/2])=0$.
		%TODO Graph 1.42, Graph F_X
	\end{exm}

	\begin{bem*}[Lesen einer Verteilungsfunktion]
		\begin{enumerate}
			\item Wenn bei $x\in\R$ eine Sprungstelle liegt, so ist $\Prb(X=x)>0$ die \enquote{Höhe der Sprungstelle}.
			\item Wenn $F$ auf $(a,b]$ konstant ist, dann ist $\Prb(x\in(a,b])=0$.
			\item Wenn $F$ streng monton und stetig bei $x$ ist, dann ist $\Prb(X=\{x\})=0$, aber $\Prb(X\in(x-\epsilon,x+\epsilon))>0$ für alle $\epsilon>0$ (und proportional zu $F'(x)^\epsilon$, falls $F'$ existiert.)
		\end{enumerate}
	\end{bem*}

	\begin{satz}\label{0143satz}
		Sei $F$ eine Funktion die \ref{0141satz}a)-c) erfülle, dann existiert eine Zufallsvariable $X$ (und ein Wahrscheinlichkeitsraum $(\Omega,\scF,\Prb)$), mit $X:\Omega\to \R$, sodass $F$ die Verteilungsfunktion von $X$ ist.
	\end{satz}
	\begin{bem*}
		Eine mögliche Wahl ist immer $(\Omega,\scF,\Prb)=((0,1),\Bor\big((0,1)\big),\la)$ und $X(a)=\inf\left\{c\in\R:F(c)\geq a\right\}$ für alle $a\in(0,1)$.\\
		Man nennt dies auch \enquote{Quantiltransformation} oder \enquote{Verallgemeinerte Umkehrfunktion}.
		%TODO Graph 1.43
	\end{bem*}
	\begin{proof}
		\begin{enumerate}
			\item Aus der Eigenschaft \ref{0141satz}b) folgt:\\
			Für alle $u>0$ gibt es ein $c\in\R$, sodass $F(c)<u$ und für alle $u<1$ gibt es ein $c\in\R$, sodass $F(c)>u$.\\
			Also ist $X(u)<\infty$ für alle $u$.
			\item Es gilt nach Definition:
			\[X(u)\leq c\Leftrightarrow \inf\{\tilde c\in\R:F(\tilde c)\geq u\}\leq c\]
			Und da $F$ rechtsstetig ist:
			\[\Leftrightarrow\min\{\tilde c\in\R:F(\tilde c)\geq u\}\leq c\Leftrightarrow F(c)\geq u\]
			Daher ist 
			\[\{u\in(0,1):X\leq c\}=\{u\in(0,1):F(c)\geq u\}=(0,F(c))\cap(0,1)\in\Bor\big(0,1\big)\]
			Also ist $X$ messbar, also Zufallsvariable.\\
			Außerdem ist $\Prb(x\leq c)=\la\big(X^{-1}\big((-\infty,c]\big)\big)=\la\big((0,F(c)]\big)=F(c)$.\\
			Also ist $F$ die Verteilungsfunktion von $X$.
		\end{enumerate}
	\end{proof}

	\begin{definition}\label{0144def}
		Sei $X$ eine Zufallsvariable und $F$ die Verteilungsfunktion von $X$. Und es gelte $F_X(c)=\int_{-\infty}^{c}\rho(z)~dz$ für alle $c\in\R$.\\
		Dann heißt $\rho$ die Verteilungsdichte von $X$.
	\end{definition}
	%TODO COUNTER
	\addtocounter{theorem}{-1}
	\begin{satz}\label{0144satz}
		$\rho$ ist genau dann Verteilungsdichte von $X$, wenn das Bildmaß $\Prb_X$ die Dichte $\rho$ hat.
	\end{satz}
	\begin{proof}
		Übung %TODO 1.44 Beweis Übung
	\end{proof}

	\begin{prop}\label{0145prop}
		In der Situation von \ref{0144satz} gilt $X$ hat genau dann eine stetige Verteilungsdichte $\rho$, wenn  $c\mapsto F_X(c)$ stetig differenzierbar ist.\\
		Dann gilt $\rho(c)=F_X'(c)$ für alle $c\in\R$.
	\end{prop}






\section{Stochastische Standardmodelle}
		In diesem Kapitel werden wir wichtige Wahrscheinlichkeitsmaße (bzw. Zufallsvariable) behandeln, welche die GRundlgene Bausteine für Anwendungen darstellen.
		
	\begin{definition}\label{0201def}
		Sei $\Omega$ endlich, $\scF=\Potset(\Omega)$.\\
		Das Maß $u_\Omega$ mit $u_\Omega(A):=\frac{|A|}{|\Omega|}$ heißt \textbf{Gleichverteilung} auf $\Omega$.
	\end{definition}
	
	\begin{exmlist}
		\begin{exm}[Würfel]
			$\Omega=\{1,...,6\}$, $u_\Omega(k)=\frac{1}{6}$ gleichverteilt
		\end{exm}
		\begin{exm}[Einfach Irrfahrt]
			$\Omega=\Omega_N$, $u_{\Omega_N}(\omega)=(2d)^{-N}$ für Pfade $\omega$ der und Tiefe $d$.
		\end{exm}
		\begin{exm}[Bose-Einstein Verteilung]
			Man betrachte $n$ Bosonen (Elementarteilchen). Jedes Teilchen kann einen (quantisierten) Zustand einnehmen: z.B. einen Impuls $p\in \Z^3\cap[-k,k]^3$.\\
			Die Teilchen sind ununterscheidbar, d.h. $x_1=(0,01)$, $x_2=(1,1,1)$, $x_3=(0,0,1)$ oder $x_1=x_2=(0,0,1)$, $x_3=(1,1,1)$ müssen identifiziert werden.\\
			Wir definieren die Besetzungszahl $k_p$ für $p\in\Lambda=\Z^3\cap[-k,k]^3$. Diiese gibt an wieviel Teilchen sich bei Impuls $p$ befinden.\\
			\underline{Bedingung} Es muss gelten, dass $\sum_{p\in\Lambda}k_p=n$ der Gesamtzahl der Teilchen entspricht.\\
			\underline{Modell} Sei 
			\[\Omega=\{(k_p)_{p\in\Lambda}\subset\N_0,\sum_{p\in\Lambda}k_p=n\}\]
			Dann ist 
			\[|\Omega|=\binom{(n+|\Lambda|-1)}{n}\]
			und die Zustände sind Gleichverteilt.
		\end{exm}
	\end{exmlist}
	
	\begin{definition}
		Die Gleichverteilung im $\R^n$ wurde in \ref{0131def} definiert.
	\end{definition}

	\begin{exmlist}
		\begin{exm}[Richtung eines Glücksrads]
			$\Omega=[0,2\pi)$, $\Prb\big((a,b)\big)=\frac{|b-a|}{2\pi}=\int_{a}^{b}\frac{1}{2\pi}~dx$ für $0\leq a\leq b< 2\pi$.
		\end{exm}
		\begin{exm}[Bertrand-Paradoxon]
			Sei $B_r(0)$ der Kreis mit Radius $r$ um 0.\\
			Sei $S$ eine Zufällig Sehne in$B_r(0)$ (Gleichverteilt).
			\underline{Frage} Wie groß ist $\Prb(A):=\Prb(S\cap B_{r/2}(0)\neq\emptyset)$?
			\underline{Mögliche Modellierungen:}
			\begin{enumerate}
				\item Der Sehnenmittelpunkt ist gleichverteilt in $B_r(0)$:
				\[\Prb(A)=\Prb(x\in B_{r/2})=\frac{\la(B_{r/2}(0))}{\la(B_r(0))}=\frac{1}{4}\]
				
				\item Länge des Kreissegments: gleichverteilt in $[0,2\pi]$
				\[\Prb(A)=\Prb\left(\al\in\left[\frac{2\pi}{3},\frac{4\pi}{4}\right]\right)=\frac{1}{3}\]
				
				\item Abstand der Sehne vom Ursprung (gleichverteilt in $[-r,r]$)
				\[\Prb(A)=\Prb\left(d\in\left[\frac{-r}{2},\frac{r}{2}\right]\right)=\frac{1}{2}\]
			\end{enumerate}
			Fazit: Modellierung in der ursprünglichen Fassung war zu ungenau.
		\end{exm}
		
	\end{exmlist}


%VL 09.05.2017

\subsection{Urnenmodelle mit Reihenfolge, mit Zurücklegen}
	\begin{exmlist}\label{0205aexm}
		\begin{exm}[Einmal zeihen:]
			 Bsp: Tulpenzwiebeln, $N$ Stück: $k_r$ rote, $k_g$ gelbe, $k_o$ orange.\\
			 Dann ist $\Prb(\text{rot})=\frac{k_r}{N}$,...\\
			 Allgemein: Menge der Merkmale $A=(a_1,...,a_m)$ mit $p_i=$Bruchteil der  \enquote{Kugeln} mit Merkmal $a_i$, sodass$\Prb(\{a_i\})=p_i$.\\
			 Dann ist $\Prb$ eine Zähldichte mit $\sum_{i=1}^mp_i=1$.
		\end{exm}
		\begin{exm}[$N$-Mal ziehen]\label{0205bexm}
			Anzahl der Ziehungen $N\in\N$, $\Omega=A^N$, $\Prb\big((a_{j_1},....,a_{j_N})\big):=p_{j_1}\cdot...\cdot p_{j_N}$, für $j_1,..., j_N\in\{1,...,m\}$ mit $p_1,...,p_m$ wie in 1.\\
			Modelliert $N$-mal \enquote{unabhängig} (siehe Kapitel 3) ziehen mit Zurücklegen.
		\end{exm}
	\end{exmlist}
	\begin{definition}
		Spezialfall von \ref{0205aexm}: $A=\{0,1\}$, $p_1=p,p_0=1-p$.\\
		$1\hat{=}$\enquote{Erfolg}, $p_1\hat{=}$\enquote{Erfolgswahrscheinlichkeit}.\\
		Dann kann für $\omega\in A^N$, $\omega=(\omega_1,...,\omega_N)$ schreiben:
		\[\Prb(\{\omega\})=p^{\sum_{i=1}^N\omega_i}(1-p)^{\sum_{i=1}^N(1-\omega_i)}\]
		Man nennt dies die \textbf{Bernoulli-Verteilung} für $N$ Versuche mit Erfolgswahrscheinlichkeit $p$.
	\end{definition}
	\begin{definition}
		Die Zähldichte $\ol p:A\times ...\times A\to [0,1]$, $\ol p(a_1,...,a_n)\mapsto \Prb(\{a_1\})\cdot...\cdot\Prb(\{a_n\})=p_1\cdot...\cdot p_n$ aus \ref{0205Bexm} heißt $N$-fache \textbf{Produktdichte} der Zähldichte $p:A\to[0,1], a_j\mapsto p(a_j)=p_j$.\\
		Das zugehörige Wahrscheinlichkeitsmaß heißt $N$-faches \textbf{Produktmaß} von $\Prb$.
	\end{definition}
	\subsection{Urnenmodelle, ohne Reihenfolge, mit Zurücklegen}
	\begin{exm}
		In Situation \ref{0205bexm} nimmt man an dass man die Anzahl der gezogenen Kugeln pro Merkmal von Interesse ist.
		Das ist ein \enquote{Aspekt} des Experiments, daher eine Zufallsvariable:
		\[X:A^N\mapsto \N_0^m,\quad (a_{j_1},....,a_{j_N})\mapsto\left(\sum_{k=1}^{N}\cha_{\{a_1\}}(a_{j_k}),\sum_{k=1}^{N}\cha_{\{a_2\}}(a_{j_k}),...,\sum_{k=1}^{N}\cha_{\{a_m\}}(a_{j_k})\right)\]
		(Jede Komponenten entspricht jeweils der Anzahl der gezogenen $a_i$). Nach der Definition des Produktmaßes ist
		\begin{align*}
		\Prb\big((n_1,...,n_m)\big)&=\Prb \left(X^{-1}\big(\{(n_1,...,n_m)\}\big)\right)\\
		&=p_1^{n_1}p_2^{n_2}...p_m^{n_m}\cdot\text{\enquote{Anzahl der $N$-Tupel von Elementen aus $A$ mit $n_1$ mal $a_1$ und $n_2$ mal $a_2$ und...}}\\
		&=\prod_{i=1}^{m}p_i^{n_i}\binom{N}{(n_1,...,n_m)}
		\end{align*}
		Dabei ist $\binom{N}{(n_1,...,n_m)}=\frac{N!}{n_1!n_2!...n_m!}$, falls $\sum_{i=1}^{m}n_i=N$ und $=0$ sonst.\\
		Man nennt $X$ auch \enquote{\textbf{Histogramm}}.
		%TODO Bild Histogramm
	\end{exm}
	\begin{definition}\label{0209adef}
		Sei $p$ Zähldichte auf $A=\{a_1,...,a_m\}$. Dann heißt das Wahrscheinlichkeitsmaß $\Mltn_{n,p}$ auf $(\N^m,\Potset(\N^m))$ mit
		\[\Mltn\big(\{k_1,...,k_m\}\big)=\begin{cases}
		0&\text{falls $k_1,...,k_m\neq N$}\\
		\binom{N}{(k_1,...,k_m)}\prod_{i=1}^mp(a_i)^{k_i}&\text{sonst}
		\end{cases}\]
		\textbf{Multinormalverteilung} für $N$ Stichproben mit Ereigniswahrscheinlichkeiten $p(a_1),...,p(a_n)$ aus der Menge $\{a_1,...,a_m\}$.
	\end{definition}
	%TODO Counter
	\addtocounter{theorem}{-1}
	\begin{definition}\label{0209bdef}
		Falls in \ref{0209adef} gilt, dass $A=\{0,1\}$ und $p(1)=p$, $p(0)=1-p$. \\
		In diesem Fall heißt das Wahrscheinlichkeitsmaß $\Bin_{N,p}$ auf $\{0,1,...,N\}$ (oder auf $\N_0$) mit
		\[B_{N,p}(\{k\})=\binom{N}{k}p^k(1-p)^{N-k}\]
		\textbf{Binomialverteilung} für $N$ Vrsuche mit Erfolgswahrscheinlichkeit $p$.
	\end{definition}
	
	\begin{exm}[Einfache Irrfahrt]
		Sei $d=1$, es wird $N$ Schritte gegangen. Nach rechts mit Wahrscheinlichkeit $p$, nach links mit Wahrscheinlichkeit $1-p$.\\
		Dann ist $X_n$ der Ort der Irrfahrt nach $n$ Schritten.\\
		Die Sitation lässt sich Modellieren mit $\Omega=\{0,1\}^N$, 
		\[\Prb\big((\omega_1,...,\omega_n)\big)=p^{\sum_{i=1}^N\omega_i}(1-p)^{\sum_{i=1}^{N}(1-\omega_i)}\]
		Mit Bernoulli-Verteilung:
		\[X_n(\omega)=2\sum_{i=1}^{N}\omega_i-N\]
		Also ist $\frac{1}{2}(X_n+N)$ eine $B_{N,p}$ verteilte Zufallsvariable. Dann ist
		\[\Prb(X_N=2k-N)=\Prb\big(\frac{1}{2}(X_n+N)=k\big)=\binom{N}{k}p^k(1-p)^{N-k}
		\]
		falls $0\leq l\leq N$.\\
		Im Speziell für $p=1/2$ gilt
		\[\Prb(X_n=2k-N)=\binom{N}{k}2^{-N}\]
		Eine Gute Veranschaulichung ist das sogenannte \enquote{Galton-Brett},
	\end{exm}


	\subsection{Urnenmodelle ohne Zurücklegen}
	\begin{exm}
		Hierfür sei auf den Georgii verwiesen.
	\end{exm}

	\begin{definition}\label{0212def}
		Sei $\Omega_0=\{1,...,m\}$ die Menge der \enquote{Merkmale} 
		$k_1,...,k_m\in \N$ mit $\sum_{j=1}^{m}k_j=M\in\N$.\\
		Modelliert man nun eine Urne mit $M$ Kugeln, davon $k_j$ mit Merkmal $j$, dann heißt für $N\in\N_0,N\leq M$ das Wahrscheinlichkeitsmaß auf
		\[\ol{\Omega}=\left\{(n_1,...,n_m),n_i\leq k_i\forall i,\sum_{i=1}^{m}n_i=N\right\}\subseteq\N_0^m\]
		mit der Zähldichte
		\[\Hypg_{N,(k_1,...,k_m)}\big((n_1,...,n_m)\big)=\frac{1}{\binom{M}{N}}\prod_{i=1}^m\binom{k_i}{n_i}\]
		\textbf{Hypergeometrische Verteilung} zu $N$ und $(k_1,...,k_m)$.\\
		Modelliert man $N$-mal Ziehen ohne zurück legen mit $k_j$ Kugeln der Farbe $j$, dann bedeutet das Ereignis $\{n_1,...,n_m\}$, dass $n_i$ Kugeln der Farbe $k_i$ gezogen wurden.\\
		Falls $\Omega_0=\{0,1\}$, dan ist $\ol{\Omega}=\{0,1,...,n\}$ und
		\[\Hypg_{N,k_1,k_0}\big(\{n\}\big)=\frac{\binom{k_1}{n}\binom{k_0}{N-n}}{\binom{k_1+k_0}{N}}\]
		Dabei ist $N$ die Anzahl der Ziehungen, $k_1$ die Anzahl der \enquote{Gewinne}, $k_0$ die Anzahl der \enquote{Nieten} im Topf und $n$ die Anzahl der gezogenen \enquote{Gewinne}.
	\end{definition}

	\begin{exm}[Lotto (6 aus 49)]
		Sei $k_1=6$ die \enquote{angekreuzten Zahlen}, $k_0=43$. Dann gilt bei $6$-maligem Ziehen:
		\[\Prb(\enquote{\text{4 richtige}})=\Hypg_{6,6,49}(4)=\frac{\binom{6}{4}\binom{43}{2}}{\binom{49}{6}}\approx9.686\cdot10^{-4}\]
	\end{exm}

	\begin{definition}\label{0214def}
		Sei $\la>0$. Das Wahrscheinlichkeitsmaß auf $\N_0$ mit Zähldichte $p_\la(k)=e^{-\la}\frac{\la^k}{k!}$ heißt \textbf{Poisson-Verteilung} zum Parameter $\la$. Schreibe $\Poi(\{k\}):=\Prb_{p_k}(\{k\})$.
	\end{definition}


%VL 15.05.2017

	\begin{bem*}[Bedeutung der Poisson-Verteilung]
		Modelliert die Anzahl der Erfolge, wenn $N=$Anzahl der Versuche $\to\infty$, $\Prb(\text{Erfolg pro Versuch})\xrightarrow{N\to\infty}0$, sodass $N\Prb(\text{Erfolg})\xrightarrow{N\to\infty}\la$.
	\end{bem*}

	\begin{definition}
		Seien $(a_n),(b_n)\subseteq\R^+$ Folgen, dann heißen $(a_n),(b_n)$ zueinander \textbf{asymptotisch äquivalent}, falls $\lim\limits_{n\to\infty}\dfrac{a_n}{b_n}=1$.
	\end{definition}
	\begin{lem}
		Sei $\sim$ die Asymptotische Äquivalenz. Dann gilt
		\begin{enumerate}[label=\alph*)]
			\item $\sim$ ist Äquivalenzrelation (reflexiv, symmetrisch, transitiv)
			\item Falls $\lim\limits_{n\to\infty}a_n=a\leq \infty$ und $(a_n)\sim (b_n)$, dann ist $\lim\limits_{n\to\infty}b_n=a$.
		\end{enumerate}
	\end{lem}
	
	%TODO COUNTER
	\addtocounter{theorem}{-3}
	\begin{satz}[Poisson-Approximation]
		Sei $\la>0$ und $(p_n)\subset[0,1]$ eine Folge, mit $\lim\limits_{n\to\infty}p_n=\la$. Sei $B_{n,p_n}$ Binomialverteilung, dann gilt $\forall k\in\N$:
		\[\lim\limits_{n\to\infty}B_{n,p_n}(\{k\})=\Poi_\la(\{k\})\]
	\end{satz}
	\begin{proof}
		Es gilt für den Binomialkoeffizienten:
		\[\binom{n}{k}=\frac{n!}{k!(n-k)!}=\frac{1}{k!}n(n-1)...(n-k+1)\sim\frac{1}{k!}n^k\]
		denn für festes $k\in\N$ ist $(n-k)\sim n$ und $\sim$ ist transitiv. Es folgt, dass
		\begin{align*}
		B_{n,p_n}(\{k\})=\binom{n}{k}p_n^k(a-p_n)^{n-k}&\sim\frac{1}{k!}n^kp_n^k(1-p_n)^{n-k}\\
		&\sim\frac{\la^k}{k!}\left(1-n\frac{1}{n}p_n n\right)^n\underbrace{(1-p_n)^{-k}}_{\sim 1}\\
		&\sim \frac{\la^k}{k!}\big(1-\frac{1}{n}(p_nn)\big)^n\xrightarrow{n\to\infty}\frac{\la^k}{k!}e^{-\la}
		\end{align*}
		denn falls $\lim\limits_{n\to\infty} a_n=a>0$, dann $\lim\limits_{n\to\infty}\left(1-\frac{1}{n}a_n\right)^ne^{-a}$.
	\end{proof}
	%TODO COUNTER
	\addtocounter{theorem}{2}
	
	\begin{bem}
		Sei $\al$ die \enquote{Rate des Eintreffens} eiens Ereignisses $E$, d.h.
		\[\lim\limits_{\delta\to0}\Prb(\text{$E$ tritt in $[0,\delta]$ ein})\frac{1}{\delta}=\al\]
		Sei $[0,t]$ der Beobachtungszeitraum. Teile $[0,t]$ in $n$ Intervalle der Länge $t/n$. Zusätzlich nehmen wir an
		\begin{enumerate}[label=(\roman*)]
			\item Ob $E$ in einem gewissen Teilintervall auftritt beeinflusst die anderen Teilintervalle nicht.
			\item Zwei Ereignisse in einem Teilintervall sind verschwindend unwahrscheinlich.
		\end{enumerate}
		Dann ist 
		\[\Prb(\text{$E$ passiert $k$-mal in $[0,t]$})\overset{(ii)}{\approx}\Prb(\text{$E$ passiert in $k$ Intervallen})\overset{(i)}{\approx}B_{n,p_n}(k)\xrightarrow{n\to\infty}\Poi_{\al t}(k)\]
		Also beschreibt der Parameter $\la$ der Verteilung $\Poi_\la$ beschreibt \enquote{Rate mal Zeit}.
	\end{bem}
	\begin{definition}
		Für $0<p\leq 1$ heißt das Wahrscheinlichkeitsmaß $\scG_p$ auf $\N_0$ mit Zähldichte $g(k)=p(1-p)^k$ die \textbf{geometrische Verteilung} zur Erfolgswahrscheinlichkeit $p$.\\
		$\scG(\{k\})=$\enquote{Wahrscheinlichkeit bei Erfolgswahrscheinlichkeit $p$ pro Versuch genau $k$ Fehlversuche vor dem Ersten Erfolg zu sehen}.\\
		Allgemein: Da Wahrscheinlichkeitsmaß $\ol{\scB}_{r,p}$ auf $\N_0$ mir $r\in\N$ und $0<p\le 1$ mit
		\[\ol{\scB}(\{k\})=\frac{r(r+1)...(r+k-1)}{k!}p^r(1-p)^k\]
		heißt \textbf{negative Binomialverteilung} und modelliert die Wahrscheinlichkeit vor dem $r-$ten Erfolg genau $k$ Misserfolge zu haben.
	\end{definition}

	\begin{definition}
		Das Wahrscheinlichkeitsmaß $\Exp_\al$ auf $(\R_0^+,\Bor(\R_0^+))$ mit Dichte $\rho_\al(x):=\al e^{-\al x}$ heißst \textbf{Exponentialverteilung} zum Parameter $\al>0$
	\end{definition}

	%TODO COUNTER
	\addtocounter{theorem}{-1}
	\begin{definition}
		Das Wahrscheinlichkeitsmaß $\Exp_\al$ auf $(\R_0^+,\Bor(\R_0^+))$ mit $\al>0,r\ge 1$ und Dichte 
		\[\gamma_{\al,r}:=\frac{\al^r}{(r-1)!}x^{r-1}e^{-\al x}=\frac{\al^r}{\Gamma(r)}x^{r-1}e^{-\al x}\]
		heißt \textbf{Gammaverteilung} zu den Parametern $\al,r$.
	\end{definition}
	
	\begin{bem}
		\begin{enumerate}[label=\alph*)]
			\item $\int\gamma_{\al,r}(x)=1$
			\item \begin{align*}
					\Exp_\al([0,t])&=\al\int_{0}^{t}e^{-\al x}~dx=\al\left[-\frac{1}{\al}e^{-\al x}\right]_{0}^t=1-e^{-\al t}=e^{-\al t}\sum_{k=1}^{\infty}\frac{\al^k}{k!}t^k\\
					&=\Poi_{\al,t}(\{\N\})=\Poi_{\al t}(\text{\enquote{Midestens ein Erfolg vor Zeit $t$ bei Rate $\al$}})\\
					&=\Prb(\text{\enquote{Wartezeit auf ersten Erfolg $\leq t$}})
				\end{align*}
			\item Es gilt \[\Prb_{\gamma_{\al,r}}([0,t])=\frac{\al^r}{(r-1)!}\int_{0}^{t}x^{r-1}e^{-\al x}dx=e^{-\al t}\sum_{k=r}^{\infty}\frac{(\al t)^k}{k!}\tag{\star}\]
			denn \begin{align*}
			\frac{d}{dt}(\star)&=-\al e^{-\al t}\sum_{k=r}^{\infty}\frac{(\al t)^k}{k!}+e^{-\al t}\sum_{k=r}^\infty\frac{\al^k}{k!}k t^{k-1}\\
			&=e^{-\al t}\left(-\al\sum_{k=r}^\infty\frac{(\al t)^k}{k!}+\al\sum_{k=r}^\infty\frac{(\al t)^k}{(k-1)!}\right)\\
			&=e^{-\al t}\al \frac{(\al t)^{r-1}}{(r-1)!}=\frac{\al^r}{(r-1)!}\frac{d}{dt}\int_{0}^{t}x^{r-1}e^{-\al x}~dx
			\end{align*}
			Somit ist also
			\[(\star)=\Poi_{\al,t}(\{r,r+1,...,\})=\Prb(\text{Wartezeit bis zum $r$-ten Erfolg ist $\leq t$})\]
			Daher heißt $\Exp_\al\al$ und $\Prb_{\gamma_{\al,r}}$ \textbf{Wartezeitverteilungen}.
		\end{enumerate}
	\end{bem}

	\begin{definition}
		Das Wahrscheinlichkeitsmaß $\Nv$ auf $(\R,\Bor(\R))$ mit Dichte $\phi_{0,1(x)}=\frac{1}{\sqrt{2\pi}}\exp(-\frac{x^2}{2})$ heißst \textbf{Standard-Normalverteilung}. Das Wahrscheinlichkeitsmaß $\Nv_{m,v}$ auf $(\R,\Bor(\R))$ mit Dichte $\phi_{m,v}(x)=\frac{1}{\sqrt{2\pi v}}\exp\left(-\frac{(x-m)^2}{2v}\right)$ heißst \textbf{Normalverteilung} (oder Gaußverteilung/Gaußmaß) mit Mittelwert $m$ und Varianz $v$.
	\end{definition}
	
	\begin{prop}
		$\phi_{m,v}(x)=\frac{1}{\sqrt{2\pi v}}\exp\left(-\frac{(x-m)^2}{2v}\right)$ ist eine Dichte auf $\R$.
	\end{prop}
	\begin{proof}
		Es gilt 
		\begin{align*}
		\frac{1}{\sqrt{2\pi v}}\int_{-\infty}^{\infty}e^{-\frac{(x-m)^2}{2v}}
		&\overset{y=x-m}{=}
		\frac{1}{\sqrt{2\pi v}}\int_{-\infty}^{\infty}e^{-\frac{y^2}{2v}}~dy\\
		&\overset{z=\frac{y}{\sqrt v}}{=}\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{z^2}{z}}~dz
		\end{align*}
		Berechne dann stattdessen
		\begin{align*}
		\left(\int_{-\infty}^{\infty}e^{-\frac{x^2}{2}}~dx\right)&=\left(\int_{-\infty}^{\infty}e^{-\frac{x^2}{2}}~dx\right)\left(\int_{-\infty}^{\infty}e^{-\frac{y^2}{2}}~dy\right)=\int_{\R^2}e^{-\frac{x^2+y^2}{2}}~dx~dy\\
		&= \int_{0}^{\infty}\int_{0}^{2\pi}e^{-\frac{r^2}{2}}r~d\theta~dr\\
		&= 2\pi\int_{0}^{\infty}e^{-\frac{r^2}{2}}r~dr
		\intertext{mit $s=-\frac{r^2}{2}$}
		&= 2\pi\int_{-\infty}^{0}e^{s}~ds\\
		&=2\pi(e^0-e^{-\infty})=2\pi
		\end{align*}
		Es folgt, dass $\int_{-\infty}^{\infty}e^{-\frac{(x-m)^2}{2v}}=\sqrt{2\pi}$.
	\end{proof}

%VL 10.02.2017
	\begin{bem}
		\begin{enumerate}[label=\alph*)]
			\item Bedeutung der Normalverteilung:\\
			\enquote{universeller} Grenzwert unabhängiger Summen in der \enquote{einzig sinnvollen} Skalierung: $\frac{1}{n}\sum_{i=1}^\infty X_i\to m$.\\
			Es folgt, dass $\frac{1}{\sqrt{n}}\sum_{i=1}^{n}(X_i-m)\sim \Nv(0,v)$.
			\item Geometrische Bedeutung: $\Nv_{0,v}$ ist die erste Koordinate der sogenannte \enquote{Gleichverteilung auf $\R^\infty$}.
		\end{enumerate}
	\end{bem}

	\begin{satz}
		Sei $B_N(r):=\left\{x\in\R^N:|x|\le r\right\}$ Kugel mit Radius $r$im $\R^N$, sei $\Prb_N$die Gelichverteilung auf $B_N(r)$ und sei $X_1:B_N(r)\to\R,x=(x_1,....,x_N)\mapsto x_1$ die Zufallsvariable \enquote{Projektion auf die erste Koordinate}.\\
		Dann gilt
		\[\lim\limits_{N\to\infty}\Prb_{N,r_N}(a\le X_1\le b)=\begin{cases}
		0,&\text{falls $\lim\limits_{n\to\infty}\frac{r_N}{\sqrt{N-1}}=\infty$}\\
		\delta_0([a,b]),&\text{falls $\lim\limits_{N\to\infty}\frac{r_N}{\sqrt{N-1}}=0$ und $a,b\neq 0$}\\
		\frac{1}{\sqrt{2\pi v}}\int_{a}^{b}e^{-\frac{x^2}{2v}~dx},&\text{falls $\lim\limits_{N\to\infty}\frac{r_N}{\sqrt{N_1}}=\sqrt{v}>0$}
		\end{cases}\]
	\end{satz}
	\begin{proof}(nur Fall $a,b\neq 0$ im Fall 2)
		Integration einer Kugel durch Zerlegen in \enquote{Kreisscheiben}.
		\begin{align*}
		h_r(a,b):&=\int_{\R^N}\cha_{x_1^2+...+x_N^2\leq r^2}(x)\cha_{\{a\le x_1\le b\}}~dx\\
		&=\int_{a}^{b}dz\int_{\R^{N-1}}\cha_{x_1^2+...+x_N^2\leq r^2-z^2}(x)~dx\\
		&=\int_{a}^b\int_{B_{N-1}(\sqrt{r^2-z^2})}~dx=\int_{a}^{b}(r^2-z^2)^{N-1/2}~dz\underbrace{\int_{B_{N-1}}(1)~dx}_{=V_1(N-1)}\\
		&=r^{N-1}V_1(N_1)\int_{a}^{b}\left(1-\frac{z^2}{r^2}\right)^{\frac{N-1}{2}}~dz
		\intertext{Durch Subtitution $y=\frac{z}{r}\sqrt{N-1}$}
		&=r^{N-1}V_1(N-1)\frac{r}{\sqrt{N-1}}\int_{\frac{\sqrt{N-1}}{r}a}^{\frac{\sqrt{N-1}}{r}b}\overbrace{\left(1-\frac{1}{N-1}y^2\right)}^{=:f_N(y)}~dy
		\end{align*}
		Da $\Prb_{N,r_N}(a\le x_1\le b)=\frac{h_{r_N}(a,b)}{h_{r_N}(-r_n,r_N)}$ ist kürzen sich die Faktoren vor den Integralen.\\
		Außerdem ist $\lim\limits_{N\to\infty}f_N(y)=\exp(-y^2)^{1/2}=e^{-\frac{1}{2}y^2}$ gleichmäßig auf Kompakta. (d.h. $\lim\limits_{N\to\infty}\sup\left\{\left|f_N(y)-e^{-\frac{y^2}{2}}\right|:|y|\leq C\right\}=0$)\\
		Mit dieser Information kann man relativ leicht zeigen, dass für $a_\infty=\lim\limits_{N\to\infty}\frac{\sqrt{N-1}}{r_{N}}a$, $b_\infty=\lim\limits_{N\to\infty}\frac{\sqrt{N-1}}{r_{N}}b$ gilt
		
		\[\lim\limits_{N\to\infty}\int_{\frac{\sqrt{N-1}}{r_N}a}^{\frac{\sqrt{N-1}}{r_N}b}f_N(x)dy=
		\begin{cases}
		\int_{a_\infty}^{b_\infty}e^{-\frac{y^2}{2}}~dy,&\text{falls $-\infty\leq a_\infty b_\infty \leq\infty$}\\
		0,&\text{falls $a_\infty=b_\infty\in\{-\infty,0,\infty\}$}
		\end{cases}\]
		Aus \ref{0214prop} für $h_{r_N}(-r_n,r_N)$ folgt die Behauptung in allen Fällen.
	\end{proof}
	
\section{Bedingte Wahrscheinlichkeit, Unabhängigkeit}
	\begin{definition}
		Sei $(\Omega,\scF,\Prb)$ ein Wahrscheinlichkeitsraum, $A\in\scF$ mit $\Prb(A)>0$. Dann heißt die Abbildung
		\[\Prb_A:\scF\to[0,1],\quad B\mapsto\frac{\Prb(A\cap B)}{\Prb(A)}\]
		\textbf{bedingtes Wahrscheinlichkeitsmaß} unter der Bedingung $A$.\\
		Für festes $B\in\scF$ heißt die Zahl $\Prb(B|A):=\Prb_A(B)=\frac{\Prb(A\cap B)}{\Prb(A)}$ die \textbf{bedingte Wahrscheinlichkeit} von $B$ unter der Bedingung $A$.
	\end{definition}

	\begin{prop}\label{0302prop}
		$\Prb_A$ ist das eindeutige Wahrscheinlichkeitsmaß auf $(\Omega,\scF)$ mit den Eigenschaften 
		\begin{enumerate}[label=(\roman*)]
			\item $\Prb_A(A)=1$
			\item $\exists c>0$ mit $\Prb_A(B)=c\Prb(B)\forall B\in\scF$ mit $B\subseteq A$.
		\end{enumerate}
	\end{prop}
	\begin{proof}
		\begin{enumerate}[label=\alph*)]
			\item $\Prb_A$ ist ein Wahrscheinlichkeitsmaß und erfüllt (i),(ii).
			\item $\Prb$ ist eindeutig:\\
			Sei $\tilde{\Prb}_A$ ein Wahrscheinlichkeitsmaß mit den Eigenschaften (i),(ii).\\
			Dann gilt $\tilde{\Prb}_A(B)=\tilde{\Prb}_A(A\cap B)+\tilde\Prb_A(B\setminus A=c\Prb(A\cap B))$.\\
			Mit $B=A$ folgt $1=\tilde{\Prb}_A(A)=c\Prb(A)$, also $\frac{1}{\Prb(A)}$. Also ist
			\[\tilde{\Prb}_A(B)=\frac{1}{\Prb(A)}\Prb(A\cap B)=\Prb_A(B)\]
		\end{enumerate}
	\end{proof}

	\begin{bem}
		$\Prb_A$ modelliert die Situation, dass wir wissen, dass $A$ sicher eintritt. $\Prb_A$ beschreibt das Modell welches diese Information berücksichtigt (\ref{0302prop}(i)), aber sonst möglichst wenig ändert (\ref{0302prop}(ii)).
	\end{bem}
	
	\begin{exm*}
		2-Mal Würfeln, Information \enquote*{Summe wird 10 sein}.\\
		(In der Praxis: alle Würfe mit $X_1+X_2\neq 10$ werden ungültig gemacht.)
		\begin{align*}
		\Prb_A(\underbrace{X_1=5}_{B})&=\Prb\big(\underbrace{\{(5,x):1\leq x\leq 6\}}_{B}\cap\underbrace{\{(x,y):x+y=10\}}_{A}\big)/\Prb\big(\underbrace{\{(x,y):x+y=10\}}_{A}\big)\\
		&=\frac{\Prb(\{(5,5)\})}{\Prb(\{(4,6),(5,5),(6,4)\})}=\frac{1}{3}
		\end{align*}
	\end{exm*}

	\begin{satz}\label{0304satz}
		Sei $(\Omega,\scF,\Prb)$ ein Wahrscheinlichkeitsraum, $N\leq \infty$, $(B_i)_{i=1,...,N}$ mit paarweise disjunkten $B_i\in\scF$ und $\bigcap_{i=1}^NB_i=\Omega$ (eine abzählbare Partition von $\Omega$)
		\begin{enumerate}[label=\alph*)]
			\item Fallunterscheidungsformel:\\
			\[\forall A\in\scF\text{ ist }\Prb(A)=\sum_{i=1}^{N}\Prb(B_a)\Prb(A|B_i)\]
			(Konvention: $\Prb(A|B_i)=0$, falls $\Prb(B_i)=0$)
			\item Formal von Bayes: $\forall k\leq N$ ist
			\[\Prb(B_k|A)=\frac{\Prb(B_k)\Prb(A|B_k)}{\sum_{i=1}^{N}\Prb(B_i)\Prb(A|B_i)}\tag{\star}\]
		\end{enumerate}
	\end{satz}
	\begin{proof}
		\begin{enumerate}[label=\alph*)]
			\item $\sum_{i=1}^{N}\Prb(B_i)\Prb(A|B_i)=\sum_{i=1}^{N}\Prb(B_i\cap A)=\Prb\left(\bigcup_{i=1}^NB_i\cap A\right)=\Prb(A)$
			\item $\Prb(B_k|A)=\frac{\Prb(B_k\cap A)}{\Prb(A)}=\frac{\Prb(B_k)\Prb(B_k|A)}{\Prb(A)}=(\star)$
		\end{enumerate}
	\end{proof}

	\begin{exm}[Bayes Formel in der Medizin, False-Positive beim HIV-Test]
		Sei $B_1=\{\text{Menschn mit HIV}\}$, $B_2=\Omega\setminus B_1=\{gesunde Menschen\}$. Empirisch Bekannt $\Prb(B_1)=0.02$ (2\% infizierte), $\Prb(A|B_1)=0.95$ (Sensitivität 95\%), $\Prb(A|B_2)=0.1$ (Spezifität 10\%).\\
		Angenommen \enquote{Test ist Positiv}:
		\[\Prb(B_1|A)=\frac{\Prb(B_1)\Prb(A|B_1)}{\Prb(B_1)\Prb(A|B_1)+\Prb(B_2)\Prb(A|B_2)}=\frac{0.02\cdot0.95}{0.02\cdot0.95+0.98\cdot0.1}\approx\frac{1}{6}\]
		Viel kleiner als die naiv vermuteten 0.9.
	\end{exm}


%VL 22.05.2017
\subsection{Galton-Watson-Prozesse und Wahrscheinlichkeitsbäume}
	\begin{exm}[Mehrstufiges Modell]$ $
		\begin{enumerate}
			\item 1 Lebewesen bekommt $X_{1,1}\in\N_0$ Nachkommen und stirbt danach.\\
			$X_{1,1}$ ist Zufallsvariable.
			\item Die $X_{1,1}$ Nachkommen bekommen jeweils $X_{2,1},X_{2,2},...,X_{2,X_{1,1}}$ Nachkommen und stirbt. Nun leben
			\[Y_2=\sum_{i=1}^{X_{1,1}}X_{2,i}\]
			Lebewesen.
			\item Die $Y_2$ Lebewesen bekommen jeweils $X_{3,1},X_{3,2},...,X_{3,Y_2}$ Nachkommen.
		\end{enumerate}
	\begin{figure}[h]
		\centering
		\begin{tikzcd}[scale=0.5, column sep=3cm,row sep=0.3cm]
		&&(1,3,9)\\
		&&(1,3,5)\\
		&&(1,3,3)\\
		&(1,3)\ar[uuur,"p_3(9)"]\ar[uur,"p_3(5)"]\ar[ur,"p_3(3)"]\ar[r,"p_3(2)"]\ar[dr,"p_3(1)"]\ar[ddr,"p_3(0)"]&(1,3,2)\\
		&&(1,3,1)\\
		&&(1,3,0)\\
		&(1,2)&\\
		(1)\ar[ddddr,"p_1(0)"']\ar[ddr,"p_1(1)"]\ar[ur,"p_1(2)"']\ar[uuuur,"p_1(3)"]
		&&(1,1,3)\\
		&&(1,1,2)\\
		&(1,1)\ar[uur,"p_1(3)"]\ar[ur,"p_1(2)"]\ar[r,"p_1(1)"]\ar[dr,"p_1(0)"]&(1,1,1)\\
		&&(1,1,0)\\
		&(1,0)\ar[r,"p_0(0)=1"]&(1,0,0)
	\end{tikzcd}
	\caption{...}
	\end{figure}
	\end{exm}

	\begin{prop}\label{0307prop}
		Sei $(\Omega,\scF,\Prb)$ein Wahrscheinlichkeitsraum und $A_1,...,A_n\in\scF$. Dann gilt\[\Prb(A_1\cap...\cap A_n)=\Prb(A_1)\cdot\Prb(A_2|A_1)\cdot\Prb(A_3|A_1\cap A_2)\cdot...\cdot\Prb(A_1|A_1\cap...\cap A_{n-1})\]
	\end{prop}
	\begin{proof}
		Falls $\Prb(A_1\cap ...\cap A_n)=0$, dann (Konvention) ist auch $\Prb(A_n|A_1\cap...\Cap A_{n-1})=0$. Sonst
		\begin{align*}
		&\Prb(A_1)\cdot\Prb(A_2|A_1)\cdot\Prb(A_3|A_1\cap A_2)\cdot...\cdot\Prb(A_1|A_1\cap...\cap A_{n-1})\\
		&=\Prb(A_1)\frac{\Prb(A_2\cap A_1)}{\Prb(A_1)}\frac{\Prb(A-1\cap A_2\cap A-3)}{\Prb(A_1\cap A_2)}...\frac{\Prb(A_1\cap....\cap A_n)}{\Prb(A-1\cap ...\cap A_{n-1})}\\
		&=\Prb(A-1\cap...\cap A_n).
		\end{align*}
	\end{proof}

	\begin{satz}\label{0308satz}
		Sei $N\leq\infty$ und seien $(\Omega_i,\scF_i)$ Messräume und die $\Omega_i$ abzählbar.\\
		Sei $\rho_1$ Zähldichte auf $\Omega_1$ und \\
		$\forall k<N,\omega_i\in\Omega_i,i\leq k$ sei $\rho_{k+1|\omega_1,...,\omega_k}$ Zähldichte auf $\Omega_{k+1}$.\\
		Sei $\Omega=\bigtimes\limits_{i=1}^N\Omega_i$ und $X_o:\Omega\to\Omega_i$, $\omega=(\omega_1,...,)\mapsto\omega_i$ die $i$-te Projektion.\\
		\\
		Dann esxistiert genau ein Wahrscheinlichkeitsmaß $\Prb$ auf $(\Omega,\bigotimes_{i=1}^N\scF_i)$ mit den Eigenschaften
		\begin{enumerate}[label=\alph*)]
			\item $\Prb(X_1=\omega_1)=\rho_1(\omega_1)$ für alle $\omega_1\in\Omega_1$.
			\item $\forall k<N,\omega\in\Omega$ und falls $\Prb(X_1=\omega_1,...,X_k=\omega_k)\neq 0$, dann ist
			\[\Prb(X_{k+1}=\omega_{k+1}\mid X_j=\omega_j\forall j\leq k)=\rho_{\omega_{k+1}|\omega_1,...,\omega_k}(\omega_{k+1})\]
		\end{enumerate}
	\end{satz}
	\begin{bem}
		\begin{enumerate}[label=\alph*)]
			\item Insbesondere osz $\Prb(A_1=\omega_1,...,X_{k+1}=\omega_{k+1})=\rho_1(\omega_1)\cdot...\cdot\rho_{k+1|\omega_1,...,\omega_k}(\omega_{k+1})$. (\enquote{Produkt entlang der Äste})
			\item Falls $N<\infty$ dann hat $\Prb$ die Zähldichte $\rho:\omega=(\omega_1,...,\omega_k)\mapsto\rho_1(\omega_1)\cdot...\cdot\rho_{N|\omega_1,...,\omega_{N+1}}(\omega_N)$.
			\item Falls $N=\infty$, dann hat $\Prb$ im Allgemeinen keine Zähldichte
		\end{enumerate}
	\end{bem}
	\begin{proof}
		\begin{enumerate}
			\item Falls $N<\infty$: Nachrechnen
			\item Falls $N=\infty$ Bilde $[0,1]\to\Omega$ mittels $x\mapsto(\omega_1(x),...)$ mit $\omega_i(x)$= dasjenige $\omega_i\in\Omega_i$, sodass $x$ im zu $\omega_i$ gehörigen Intervall liegt, in Stufe $i$.\\
			Zeige dann $X:x\mapsto\omega(x)$ ist Zufallsvariable von $([0,1],\Bor([0,1]))$ nach $(\Omega,\scF)$. $\Prb$ ist dann das Bildmaß .
		\end{enumerate}
	\end{proof}
	
	\begin{exmlist}
		\begin{exm}[unendlich oft wiederholter Münzwurf]
			Sei $\Omega=\{-1,1\}^N$ und $\scF=\bigotimes_{i=1}^\infty\Potset({-1,1})$.\\
			($\scF$ wird also von den Mengen 
			\[\big\{\{\omega\in\Omega:\omega_1=k_1,...,\omega_n=k_n\}:K_i\in\{-1,1\}\forall i\leq nnn\in\N\big\}\]
			erzeugt. und $\Prb(\omega_1=k_1,...,\omega_n=k_n)=2^{-n}$.\\
			Im Fall von Satz \ref{0308satz} bedeutet das $\rho_1(\omega_1)=\frac{1}{2},\rho_{2,\omega_1}(\omega_2)=\frac{1}{2}$.
		\end{exm}
		\begin{exm}[Unendlich oft wiederholtes würfeln, 10-Seitiger Würfel]
			Sei $\Omega=\{0,1,...,9\}$. Die Abbildung $X$ aus \ref{0308satz} ist hier:
			\[X:[0,1)\mapsto\Omega=\bigtimes_{i=1}^\infty\Omega_i, x\mapsto\omega(x)\]
		\end{exm}
	\end{exmlist}

	\begin{definition}
		Sei $(\Omega,\scF,\Prb)$ sei ein Wahrscheinlichkeitsraum, $A,B\in\scF$.\\
		$A,B$ heißen \textbf{unabhängig} (oder \textbf{unabhängige Ereignisse}), falls $\Prb(A\cap B)=\Prb(A)\Prb(B)$. Man schreibt $A\indep B$.
	\end{definition}
	\begin{bem*}
		\begin{enumerate}[label=\alph*)]
			\item Falls $A\indep B$, dann ist
			\[\Prb(A|B)=\frac{\Prb(A\cap B)}{\Prb(B)}=\Prb(A)\]
			\item Beachte:\\
			$\Prb(A\cup B)=\Prb(A)+\Prb(B)$ (\enquote{additiv}), falls $A,B$\\
			$\Prb(A\cap B)=\Prb(A)\Prb(B)$ (\enquote{multiplikativ}), falls $A,B$ unabhängig (per Definition).
			\item Aussage b) gilt zwar für $A_1,A_1,A_3\scF$ falls $\Prb(A_i\cup A_j)=\Prb(A_i)+\Prb(A_j)$ für $i,j\in\{1,2,3\}$und $i\neq j$. Dann gilt auch
			\[\Prb(A_1\cup A_2\cup A_3)=\Prb(A_1)+\Prb(A_2)\Prb(A_3)\]
			Aber: gilt nicht für Unabhängigkeit.
			\\
			Unabhängigkeit ist eine algebraische Eigenschafte von Mengen und Wahrscheinlichkeitsmaßen.\\
			Die Interpretation \enquote{$A$ beeinflusst $B$ nicht} ist nicht immer richtig.
			\item Unabhängigkeit trotz Kausalität: $\Omega=\{1,2,3,4,5,6\}^2$, $\Prb(A\cap B)\Prb(\{6,1\})=\frac{1}{36}=\Prb(A)\Prb(B)$. Geht nicht wenn man $7$ durch $8$ ersetzt!
			\item $\Prb(A)\in\{0,1\}$, es folgt $A\indep A$
			\item Im wichtigen Fall der Produktmaße sind jedoch c) bis e) nicht relevant
		\end{enumerate}
	\end{bem*}

	\begin{definition}
		Sei $(\Omega,\scF,\Prb)$ ein Wahrscheinlichkeitsraum, $I\neq 0$ eine Indexmenge. $(A_i)_{i\in I}$ heißt \textbf{unabhängige} Familie von Mengen, wenn $\forall J\subseteq$, mit $|J|<\infty$
		\[\Prb\left(\bigcap_{j\in J}A_j\right)\prod_{j\in J}\Prb(A_j)\]
		(d.h. Multiplikativ für alle Kombinationen von endlich vielen Mengen) gilt.
	\end{definition}

%VL 23.05.2017
	\begin{definition}\label{0313def}
		Sei $(\Omega,\scF,\Prb)$ Wahrscheinlichkeitsraum, $I\neq 0$ sei Indexmenge und $\forall i\in I$ seien $(\Omega_i,\scF_i)$ Ereignisräume. Seien $Y_i:\Omega\to\Omega_i$ Zufallsvariablen.\\
		Die Familie $(Y_i)_{i\in I}$ heißt \textbf{unabhängig} (oder \textbf{unabhängige Familie von Zufallsvariablen}), wenn  $\forall J\subseteq$, mit $|J|<\infty$ und für alle $(B_j)_{j\in J}$ mit $B_j\in\scF_j$
		\[\Prb\left(\bigcap_{j\in J}Y^{-1}_j(B_j)\right)=\prod_{j\in J}\Prb\left(Y^{-1}_j(B_j)\right)\]
		gilt.
	\end{definition}
	%TODO COUNTER
	\addtocounter{theorem}{-1}
	\begin{definition} %3.13a)
		Eien Familie $(\scA_i)_{i\in I}$ von \textbf{Megensystemen} heißst \textbf{unabhängig} (oder auch unabhänguge Famile von Mengensystemen), wenn  $\forall J\subseteq$, mit $|J|<\infty$, für alle $(A_j)_{j\in J}$ mit $A_j\in\scA_j$ sind die Mengen $(A_j)_{j\in J}$ unabhängig.\\
		(Dabei darf aus jedem $\scA_j$ höchstens ein $A_j$ gewählt werden.)\\
		Daher $X\indep Y\Leftrightarrow\sigma(A)\indep\sigma(Y)$.
	\end{definition}
	
	\begin{satz}\label{0314satz}
		In \ref{0313def} sei $\scG_i$ ein Schnitt-stabiler Erzeuger von $\scF_i$. Dann
		\[\text{\ref{0301} gilt $\forall B_i\in\scG_i$}\Leftrightarrow\text{\ref{0301} gilt $\forall B_i\in\scF_i$}\]
	\end{satz}
	\begin{proof}
		\begin{description}
			\item[\enquote{$\Leftarrow$}] klar, da $\scG_i\subseteq \scF_i$
			\item[\enquote{$\Rightarrow$}] Durch Induktion nach $n:=|\{i\in J:B_i\in\scF-i\setminus\scG_i\}|$:
			\begin{description}
				\item[$n=0$] bedeutet $B_i\in\scG_i\forall i\in J$.
				\item[$n\to n+1$] Seien $(B_i)_{i\in J}$ mit $B_i\in\scF_i\forall i\in J$ und $B_i\in\scF_i\setminus\scG_i$ (n+1)-mal. Wähle $j\in J$  und setze $J'=J\setminus\{j\}$, dann folgt aus der Induktionsannahme, dass für
				\[A:\bigcap_{i\in J'}Y^{-1}_i(B_i)\]
				\[\Prb(A)=\prod_{i\in J'}\Prb(Y^{-1}_i(B_i))\tag{\star}\]
				gilt.\\
				Falls $\Prb(A)=0$, dann ist auch $\Prb(A\cap Y^{-1}(B_j))=0$.\\
				Falls $\Prb(A)>0$, dann definiere die Wahrscheinlichkeitsmaße
				\begin{align*}
				\Prb_i:\scF_j&\to[0,1],\\ \tilde{B_i}&\mapsto\Prb(Y^{-1}_j(\tilde{B_j})|A)=\frac{\Prb(Y^{-1}_j(\tilde{B_j})\cap A)}{\Prb(A)}\tag{\star\star}\\
				\Prb_2:\scF:i&\to[0,1],\\
				\tilde{B_j}&\mapsto\Prb(Y^{-1}_j(\tilde{B}_j))
				\end{align*}
				Da \ref{0301} für $n$ gilt ist $\forall
				\tilde{B}_j\in\tilde{\scG}_j$, also
				\[\Prb_1(\tilde{B}_j)=\frac{\Prb\big(Y^{-1}_j(\tilde{B}_j)\big)\cdot\prod_{i\in J'}\Prb\big(Y^{-1}_i(B_i)\big)}{\Prb(A)}\overset{(\star)}{=}\Prb\big(Y^{-1}_j(\tilde{B}_j)\big)=\Prb_2(\tilde{B}_j)\]
				Es folgt, dass $\Prb_1(\tilde{B})=\Prb_2(\tilde{B})\forall \tilde{B}\in\scG_j$, sodass aus \ref{0124satz} folgt, dass $\Prb_1=\Prb_2$. Dann
				\begin{align*}
				\prod_{i\in J}\Prb\big(Y^{-1}_i(B_i)\big)&\overset{(\star)}{=}\Prb\big(Y^{-1}_j(b_i)\big)\Prb(A)=\Prb_2(B_j)\Prb(A)\\
				&=\Prb_1(B_j)\Prb(A)=\Prb\big(Y^{-1}_j(B_j)\cap A\big)=\Prb\left(\bigcap_{i\in J}Y^{-1}_i(B_i)\right)
				\end{align*}
				für alle $B_i\in\scF_i$. Es folgt die Behauptung.
			\end{description}
		\end{description}
	\end{proof}

	\begin{kor}
		Seien $(Y_i)_{i=1,...,n}$ Zufallsvariablen auf dem Wahrscheinlichkeitsraum $(\Omega,\scF,\Prb)$, dann gilt
		\begin{enumerate}[label=\alph*)]
			\item Falls $Y_i:\Omega\to E_i$ mit $E_i$ abzählbar, dann
			\[\text{$(Y_i)$ ist unabhängig}\Leftrightarrow\Prb(Y_1=e_1,...,Y_n=e_n)=\prod_{i=1}^n\Prb(Y_i=e_i)\]
			für alle $e_1\in E_1,...,e_n\in E_n$.
			\item Falls $Y_i:\Omega\to\R$, dann ist
			\[\text{$(Y_i)$ ist unabhängig}\Leftrightarrow\Prb(Y_1\leq c_1,...,Y_n\leq c_n)=\prod_{i=1}^n\Prb(Y_i\leq c_i)\]
			für alle $c_1,...,c_n\in\R$.
		\end{enumerate}
	\end{kor}

	\begin{kor}
		Sei $(\Omega,\scF\Prb)$ ein Wahrscheinlichkeitsraum, $I$ eine Menge und für alle $i\in I$ sei $A_i\in\scF$. Dann gilt
		\begin{enumerate}
			\item $(A_i)_{i\in I}$ sind unabhängige Mengen $\Leftrightarrow$ $(\cha_{A_i})_{i\in I}$ unabhängige Zufallsvariablen sind.
			\item Insbesondere: Falls $(A_i)_{i\in I}$ unabhängig und $C_i\in\{\emptyset,\Omega,A_i,A_i^C\}$ für alle $i\in I$, dann sind auch $(C_i)_{i\in I}$ unabhängig.
		\end{enumerate}
	\end{kor}
	\begin{proof}
		Für $a\in\scF$ ist $\cha_A:\Omega\to\{0,1\}, \omega\mapsto\cha_A(\omega)$ eine Zufallsvariable und das einelementige Mengensystem $\big\{\big\}$ ist ein Schnitt-stabiler Erzeuger von $\Potset(\{0,1\})$ und $\cha_a^{-1}(\{1\})=A$.\\
		Daher gilt, dass $(A_i)$ unabhängig $\Leftrightarrow$ \ref{0301}  gilt falls $B_i=\{1\}$ $\overset{\ref{0314satz}}{\Leftrightarrow}$ $\cha_{A_i}$ ist unabhängig. Also gilt 1.\\
		Für 2. Setze
		\[B_i=\begin{cases}
		\{1\}&\text{falls $c_i=A$}\\
		\{0\}&\text{falls $c_i=A^c$}\\
		\emptyset&\text{falls $c_i=\emptyset$}\\
		\{0,1\}&\text{falls $c_i=\Omega$}
		\end{cases}\]
		Dann folgt 2 aus der Unabhängigkeit der $\cha_{A_i}$ und aus \ref{0301} mit den so gewählten $B_i$.
	\end{proof}

	\begin{exm}
		Punkt auf Kreisscheibe: Winkel und Radius unabhängig %TODO Bsp 3.17: 23.05.2017
	\end{exm}
	
	\begin{figure}[h]
		\centering
		\begin{tikzcd}
			\Omega\ar[r,"Y_1"]\ar[rd,"Y_2"]\ar[rr, bend left=40, red,"Z_1"]\ar[drr, bend right=60, red,"Z_2"']&\Omega_1\ar[r,"\phi_1"]&E_1\\
			&\Omega_2\ar[r,"\phi_2"']&E_2
		\end{tikzcd}
		\caption{...}
	\end{figure}
	\begin{satz}\label{0318satz}
		Seien $(Y_i)_{i=1}^\infty$ unabhängige Zufallsvariable, mit $Y_i:(\Omega,\scF)\to(\Omega_i\scF_i)$. Dann gilt
		
		\begin{enumerate}
			\item Falls $\phi_i:(\Omega_i\scF_i)\to(E_i\scE_i)$ messbar und $Z_i:\Omega\to E_i,\omega\mapsto\phi_i(Y_i(\omega))$, dann sind auch $(Z_i)_{i\in\N}$ unabhängig. (\enquote{Stabilität unter einsetzen in Funktionen})
			\item Seien $J_1,J_2,...,$ paarweise disjunkte Teilmengen von $\N$. Für jedes $k\in\N$ definiere
			\[W_k(\omega)=(X_i(\omega))_{i\in J_k}\in \bigtimes_{i\in J_k}\Omega_i\]
			Dann sind die Zufallsvariablen $(W_k)_{k\in\N}$ unabhängig:\\
			(\enquote{Stabilität gegenüber Zusammenfassen in disjunkte Blöcke})
		\end{enumerate}
	\end{satz}
	\begin{proof}
		\begin{enumerate}[label=\alph*)]
			\item Sei $I\subset\N$ endlich. Dann ist (für $A_i\in\scE_i$)
			\[\Prb\left(\bigcap_{i\in I}Z^{-1}_i(A_i)\right)=\Prb\left(\bigcap_{i\in I}Y^{-1}_i(A_i)\right)\overset{\text{$Y_i$ unahängig}}{=}\prod\Prb(Y^{-1}_i(\phi^{-1}(A_i)))\]
			\item Es gilt $\bigtimes_{i=1}^nA_i)=\{(\omega_1,...,\omega_n):\omega_1\in A_1,...,\omega_n\in A_n\}\subset\bigtimes_{i=1}^n\Omega_i$.\\
			Für $k\in \N$ setze
			\[\scG_k:=\left\{\bigtimes_{i\in J_k}A_i^{(k)}:A^{(k)}\in\scF_i\forall i\in J_k\,A_i^{(k)}\neq \Omega_i\text{ nur endlich oft}\right\}\]
			$\scG_k$ ist Schnitt-stabiler Erzeuger von $\bigotimes_{i\in J_k}\scF_i$, also miss \ref{0301} nur auf $(G_k)_{k\in\N}$ geprüft werden.\\
			Seien also $B_{k_1}\in\scG_{k_1},...,B_{k_n}\in\scG_{k_n}$. Da $W_{k_j}\in B_{k_j}$ genau dann wenn $X_i\in A_i^{kj}$ ist
			\begin{align*}
			\Prb(W_{k_1}\in B_{k_1},...,W_{k_n\in P_{k_n}})&=\Prb(X_i\in A_i^{(k)}\forall i\in J_{k_1,....,}X_i\in A_{i}^{(k)}\forall
			i\in J_k)\\
			&=\prod_{l=1}^n\prod_{i\in J_{k_l}}\Prb(X_i\in A_i^{(k_l)})=\prod_{l=1}^n\Prb(W_{k_l}\in B_{k_l})
			\end{align*}
		\end{enumerate}
	\end{proof}

%VL 29.05.2017
	\begin{bem*}
		Unabhängigkeit ist eng verwandt mit dem Produktmaß:
	\end{bem*}

	%TODO COUNTER Async
	\setcounter{theorem}{20}
	\begin{definition}\label{0321def}
		Sei $(\Omega,\scF_i,\Prb_i)$ ein Wahrscheinlichkeitsraum für $i\in\N$.\\
		Jedes Maß $\Prb$ auf $(\Omega,\scF):=\left(\bigtimes_{i=1}^\infty \Omega_i,\bigotimes_{i=1}^\infty\scF_i\right)$\\
		($\bigotimes_{i=1}^\infty\scF_i$ erzeugt durch Zylindermengen)\\
		mit der Eigenschaft
		\[\Prb\left(\left\{\omega=(\omega_i)_{i\in \N}\in\Omega:\omega_{j_i}\in A
		_{j_i},...,\omega_{j_m}\in A_{j_m}\right\}\right)=\prod_{k=1}^m\Prb_{jk}(A_{jk})\tag{3.2}\]
		für alle $j_1<j_2<...<j_m$, mit $j_i\in\N$ und $A_{ji}\in\scF_{ji}$ heißt \textbf{Produktmaß} der Maße $\Prb_i$. Schreibweise $\Prb=\bigotimes_{i=1}^\infty\Prb_i$
	\end{definition}

	\begin{bem}
		\begin{enumerate}[label=\alph*)]
			\item In \ref{0321def}existiert immer genau ein Produktmaß.\\
			(Eindeutigkeit nach Satz \ref{0124satz}, Existenz aus dem Maßfortsetzungssatz)
			\item Wichtiger Spezialfall:
			\begin{enumerate}[label=(\roman*)]
				\item Sei $\Omega_i=\{0,1\}$, $\Prb_i(0)=\Prb_i(1)=\frac{1}{2}$ für alle $i$ (unendlich oft wiederholter Münzwurf)
				\item Sei $\Omega_i=\R$, $\scF_i\Bor(\R)$, $\Prb_i=\rho(x)~dx$ für alle $i$.\\
				(Zufällige Folge mit Gliedern die gemäß $\rho(x)~dx$ verteilt sind.)
			\end{enumerate}
		\end{enumerate}
	\end{bem}

	\begin{prop}\label{0324prop}
		\begin{enumerate}[label=\alph*)]
			\item Seien $(X_i)_{i\in\N}$Zufallsvariablen, $X_i:(\Omega,\scF)\to(\Omega_i\scF_i)$, dann gilt
			\[\text{$(X_i)$ ist unabhängig }\Leftrightarrow\text{ Das Bildmaß der Zufallsvariablen }X:\Omega\to\bigtimes_{i=1}^\infty\Omega_i,\omega\mapsto(X_i(\omega))_{i\in\N}\text{ ist ein Produktmaß}\]
			\item Seien $(\Omega_i\scF_i)_{i\in \N}$ seien Maßräume, $\Omega=\bigtimes_{i=1}^\infty,\scF=\bigotimes_{i=1}^\infty\scF_i$.\\
			$\Prb$ sei ein beliebiges Wahrscheinlichkeitsmaß auf $(\Omega,\scF)$. Dann gilt:
			\[\text{$\Prb$ ist Produktmaß}\Leftrightarrow\text{Die Projektionenn $X_i:\Omega\to\Omega_i$, $\omega=(\omega_1,\omega_2,...)\mapsto\omega_i$ sin unabhängige Zufallsvariablen unter $\Prb$}\]
		\end{enumerate}
	\end{prop}

	\begin{bem}
		In \ref{0324prop}a) heißt der Wahrscheinlichkeitsraum $(\tilde\Omega,\tilde{\scF},\tilde\Prb):=(\bigtimes_{i=1}^\infty\Omega_i,\bigotimes_{i=1}^\infty,\Prb_X)$ mit $X=(X_i)_{i\in N}$ der kanonische Wahrscheinlichkeitsraum der Zufallsvariablen $X_i$. Die $i$-te Koordinatenabbildung $Y_i:\tilde{\Omega}\to\Omega_i$ entspricht dann jeweis $X_i$, d.h. $(Y_{i_1},....,Y_{i_n})$ hat die gleiche Verteilung wie $(X_{i_1},....,X_{i_n})$.
	\end{bem}

	\begin{definition}
		Seien $\Pr_1,...,\Prb_n$ Wahrscheinlichkeitsmaße auf $\R$ mit der Dichte $\rho_1,...,\rho_n$. Dann heißt 
		\[\rho:\R^n\to\R_0^+, (x_1,...,x_n)\mapsto \rho(x_1,...,x_n)=\rho_1(x_1)...\rho_n(x_n)\]
		die \textbf{Produktdichte}.
	\end{definition}
	%TODO COUNTER
	\addtocounter{theorem}{-1}
	\begin{satz}
		Die Produktdichte ist die Dichte des Produktmaßes.
	\end{satz}
	\begin{proof}
		Es gilt\begin{align*}
		\Prb\big((-\infty,c_1]\times(-\infty,x_2]\times...\times(-\infty,c_1]\big)&=\Prb(X_1\leq c_1,...,X_n\leq c_n)\\
		&\overset{\ref{0323}b)}{=}\Prb(X_1\leq c)...\Prb(X_n\le n)\\
		&=\int_{-\infty}^{c_1}\rho_1(x_1)~dx_1\cdot...\cdot\int_{-\infty}^{c_n}\rho_n(x_n)~dx_n\\
		&=\overset{\substack{\text{Satz von}\\\text{Fubini}}}{=}\int_{(-\infty,c_1]\times...\times(-\infty,c_n]}\rho_1(x_1)...\rho_n(x_n)~dx_1...~dx_n\\
		&=\int_{(-\infty,c_1]\times(-\infty,x_2]\times...\times(-\infty,c_1]}\rho(x)~dx
		\end{align*}
		Dann folgt mit \ref{0124satz}, dass $\Prb$ die Dichte $\rho$ hat.
	\end{proof}
	
	\begin{bem*}
		Für unabhängige Zufallsvariablen $X,Y$ kann man die Dichte $X+Y$ mittels Faltungsprodukt berechnen.
	\end{bem*}
	
	\begin{satz}\label{0326satz}
		Seien $X,Y:\Omega\to\Omega'$ unabhängige Zufallsvariablen
		\begin{enumerate}[label=\alph*)]
			\item Falls $\Omega'=\Z$ und falls $\Prb_X$ (bzw $\Prb_Y$) Zähldichte $\rho_1$ (bzw. $\rho_2$), dann hat $\Prb_{X+Y}$ die Zähldichte 
			\[\rho_1*\rho_2:\Z\to[0,1],z\mapsto\rho_1*\rho_2(z)=\sum_{k\in Z}\rho_1(k)\rho_2(z-k)\]
			\item Falls $\Omega'=\R$ und falls $\Prb_X$ (bzw $\Prb_Y$) die Dichte $\rho_1$ (bzw. $\rho_2$), dann hat $\Prb_{X+Y}$ die Dichte 
			\[\rho_1*\rho_2:\R\to\R_0^+,x\mapsto\rho_1*\rho_2(x)=\int_{\R}\rho_1(y)\rho_2(x-y)~dy\]
		\end{enumerate}
	\end{satz}
	\begin{proof}
		\begin{enumerate}[label=\alph*)]
			\item Da nur $\Prb_X,\Prb_Y,\Prb_{X+Y}$ untersucht werden kann man$\Omega=\Z\times\Z$, $X\big((z_1,z_2)\big)=z_1$ und $Y\big((z_1,z_2)\big)=z_2$ wählen und $\Prb_{(X,Y)}$ hat die Produkt-Zähldichte $(z_1,z_2)\mapsto\rho_1(z_1)\rho_2(z_2)$.\\
			Dann gilt $X+Y=w\in\Z$,genau dann wenn $z_1+z_2=w$, bzw. $z=1=w-z_2$.\\
			Also $(X+y)^{-1}(\{w\})=\{(z_1,z_2):z_1=w-z_2\}$, es folgt
			\[\Prb_{X+Y}(\{w\})=\Prb_{(X,Y)}\big((X+Y)^{-1}(\{w\})\big)=\sum_{\substack{z_1\in\Z\\z_2\in\Z\\z_1+z_2=w}}\rho(z_1)\rho(z_2)=\sum_{z\in Z}\rho_1(z)\rho_2(w-z)\]
			\item Setze $\Omega=\R^2$, $X\big((x_1,x_2)\big)=x_1$, $Y\big((x_1,x_2)\big)=x_2$, $\Prb\big([a,b]\times [c,d]\big)=\int_{[a,b]\times[b,c]}\rho_1(x_1)\rho_2(x_2)~dx_1~dx_2$.
			Dann ist 
			\begin{align*}
			\Prb(X+Y\leq c&)=\Prb\big(\{(x,y)\in\Omega\}:x+y\leq c\big)\\
			&=\int_{-\infty}^{\infty}\rho_1\int_{-\infty}^{c-x}\rho _2(y)~dy~dx\\
			&=\int_{-\infty}^{\infty}\rho_1\int_{-\infty}^{c}\rho _2(\tilde y-x)~d\tilde y~dx\\
			&=\int_{-\infty}^{c}\underbrace{\int_{-\infty}^{\infty}\rho_1(x)\rho_2(\tilde y-x)~dx}_{=\rho_1*\rho_2(\tilde y)}~d\tilde y
			\end{align*}
			Dann folgt mit \ref{0124satz}, dass $\Prb_{X+Y}$ die Dichte $\rho_1*\rho_2$ hat.
		\end{enumerate}
	\end{proof}
	

	\begin{exmlist}
		\begin{exm}
			Sei $X\sim\Nv_{\mu,\nu}$ und $Y\sim\Nv_{\mu',\nu'}$, dann ist $X+Y\sim\Nv_{\mu+\mu',\nu+\nu'}$
			%TODO Bsp 3.27: 29.05.2017
		\end{exm}
	\end{exmlist}
	
	%TODO COUNTER
	
	\begin{exm}
		Sei $\Omega=\R^n$, $\scF=\otimes_{x=1}^\infty\Bor(\R)$, $X_i(\omega)=\omega_i$ Projektionen. Sei 
		\begin{align*}
		A_1&=\left\{\omega\in\Omega:\lim\limits_{n\to\infty} X_n(\omega)=\lim\limits_{n\to\infty}\omega_n\text{ exitsiert}\right\}\\
		A_2&=\left\{\omega\in\Omega:\lim\limits_{n\to\infty} \frac{1}{n}\sum_{i=1}^nX_n(\omega)=0\right\}\\
		A_3&=\left\{\omega\in\Omega:\lim\limits_{n\to\infty} \frac{1}{\sqrt{n}}\sum_{i=1}^nX_n(\omega)=\lim\limits_{n\to\infty}\frac{1}{\sqrt{n}}\sum_{i=1}^n\omega_n\text{ exitsiert}\right\}
		\end{align*}
		Für $A_1,A_2,A_3$ gilt: Sei $\omega\in A_j$. Wenn $\tilde\omega_i=\omega_i$ für alle außer endlich viele $i\in\N$ gilt, dann ist $\tilde\omega\in A_j$. Ebenso wenn $\omega\in A_j^C$, dann $\omega\in A_j^C$.
	\end{exm}

%VL 30.05.2017
	\begin{definition}\label{0329def}
		Sei $(\Omega,\scF,\Prb)$ ein Wahrscheinlichkeitsraum und seien $X_i:(\Omega,\scF)\to(\Omega_i,\scF_i)$ seien Zufallsvariablen. Setze \[\scF_{\{i\}}=\sigma(X_i)=\{X^{-1}_i(B):B\in\scF_i\} \tag{\enquote{von $X_i$ erzeugte $\sigma$-Algebra}}\]
		Setze für $K\subseteq\N$
		\[\scF_K:=\sigma(\{X_i:i\in K\})=\sigma(\scF_{\{i\}}:i\in K) \tag{\enquote{von $(X_i)_{i\in K}$ erzeugte $\sigma$-Algebra}}\]
		Schreibe für $n\in\N$
		\[\scF_n:=\scF_{\{1,...,n\}} \tag{\enquote{$\sigma$-Algebra bis zur \enquote{Zeit} $n$}}\]
		\[\tau_n:=\scF_{n+1,n+2,...} \tag{\enquote{$\sigma$-Algebra nach der Zeit $n$}}\]
		\[\tau:=\bigcap_{n\in \N}\tau_n \tag{\enquote{terminale/asymptotische $\sigma$-Algebra}}\]
	\end{definition}

	\begin{exm}
		Für $\Omega=\R^N$, $\scF=\Bor(\R^N)$, $X_i$ ist $i$-te Projektion. Dann ist
		\[\scF_{\{1,....,n\}}=\left\{\left\{x\in\R^N:(x_1,...,x_n)\in B,\text{$x_{n+1}$ beliebig},\text{$x_{n+1}$ beliebig},...\right\}:B\in\Bor(\R^n)\right\}\]
		sind Zylindermengen.\\
		Für $|K|=\infty$ (insbesondere für $\tau_n$) kann man $\scF_K$ nicht explizit angeben. Stattdessen: $\tau_n=$kleinste $\sigma$-Algebra, die $\scF_{n+1,n+2,...,m}$ enthält für alle $m>n$.
	\end{exm}

	\begin{definition}\label{0332def}
		Sei $(\Omega,\scF)$ Messraum, $A_i\in \scF$ für alle $\in\N$. Definiere
		\begin{align*}
		\limsup_{n\to\infty}A_n&:=\bigcap_{m=1}^\infty\bigcup_{k\geq m}A_k=\{\omega,\in\Omega:\text{$\omega\in A_i$ für unendlich viele $i$}\}\\
		\liminf_{n\to\infty}A_n&:=\bigcup_{m=1}^\infty\bigcap_{k\geq m}A_k=\{\omega,\in\Omega:\text{$\omega\in A_i$ für alle außer endlich viele viele $i$}\}
		\end{align*}
	\end{definition}
	
	\begin{prop}
		\begin{enumerate}[label=\alph*)]
			\item Die Gleichheiten in \ref{0332def} gelten
			\item $\bigcap_{n=1}^\infty A_n\subseteq\liminf_{n\to\infty}A_n\subseteq\limsup_{n\to\infty}A-n\subseteq\bigcap_{n=1}^\infty A_n$
			\item Für alle $\omega\in\Omega$ gilt 
			\begin{align*}
				\liminf_{n\to\infty}\cha_{A_i}(\omega)&=\cha_{\liminf_{n\to\infty}A_i}(\omega)\\
				\limsup_{n\to\infty}\cha_{A_i}(\omega)&=\cha_{\limsup_{n\to\infty}A_i}(\omega)
			\end{align*}
			\item Falls $X_i:(\Omega,\scF)\to(\Omega_i\scG_i)$ Zufallsvariablen sind uns $A_i=X^{-1}(B_i)$ mit $B_i\scG_i$, dann sind $\limsup_{n\to\infty} A_n\in\tau$ und $\liminf_{n\to\infty}A_n\in \tau$.\\
			(Nicht aber $\bigcap_{n=1}^\infty A_n$ oder $\bigcup_{n=1}^\infty$!)
		\end{enumerate}
	\end{prop}
	
	\begin{exm}
		In der Situation von \ref{0329def} mit $\Omega_i=\R$ ist
		\[A=\left\{\lim\limits_{N\to\infty}\frac{1}{N}\sum_{i=1}^{N}X_i\text{ exitsiert und liegt in $[a,b]$}\right\}\in\tau\]
		für alle $a\leq b$.
	\end{exm}
	

	\begin{satz}[Kolmogorov]
		%TODO Satz 3.34: 30.05.2017
	\end{satz}

	\begin{bem*}
		In der Sitation von \ref{0333exm} können also nur Wahrscheinlichkeiten $\{0,1\}$ auftreten, wenn die $X_i$ unabhängig sind. Welche der beiden Möglichkeiten trifft zu?\\
		Diese frage is i.A. schwierig zu beantworten, aber im Fall $\limsup_{n\to\infty}A_n$ gibt es folgenden Satz:
	\end{bem*}

	\begin{satz}[Lemma von Borel-Cantelli]\label{0335satzBorCant}
		Sei $(\Omega,\scF,\Prb)$ ein Wahrscheinlichkeitsraum und $A_j\in\scF$, für alle $j\in\N$. Dann
		\begin{enumerate}[label=\alph*)]
			\item Falls $\sum_{k=1}^{\infty}\Prb(A_k)<\infty$, dann ist $\Prb(\limsup_{n\to\infty}A_n)=0$
			\item Falls $(A_k)_{k\in \N}$ unabhängig und $\sum_{k=1}^{\infty}\Prb(A_k)=\infty$, dann ist $\Prb(\limsup_{n\to\infty} A_n)=1$.
		\end{enumerate}
	\end{satz}
	\begin{proof}
		\begin{enumerate}[label=\alph*)]
			\item Für alle $m$ ist $A\subseteq \bigcap_{k=1}^\infty A_k$. Also ist $\Prb(A)\leq\sum_{k=m}^{\infty}\Prb(A_k)$.\\
			Da $\sum_{k=1}^{\infty}\Prb(A_k)<\infty$ gilt, muss $\lim\limits_{n\to\infty}\sum_{k=m}^{\infty}\Prb(A_k)=0$ gelten. Da die Mengenfolge $\left(\bigcup_{k=m}^{\infty}\right)_{m\in\N}$ absteigend ist gilt 
			\[\Prb\left(\bigcap_{m=1}^\infty\bigcup_{k=m}^\infty A_k\right)=\lim\limits_{n\to\infty}\Prb\left(\bigcup_{k=m}^\infty A_k\right)\leq\lim\limits_{m\to\infty}\sum_{k=m}^\infty\Prb(A_k)=0\]
			\item Es gilt für das Komplement:
			\[A^C=\bigcup_{m=1}^\infty\bigcap_{k=m}^\infty\]
			Also ist
			\begin{align*}
			\Prb(A^C)&\leq\sum_{m=1}^{\infty}\Prb\left(\bigcap_{k=m}^\infty A_K^C\right)\overset{\substack{\text{Satz \ref{0122satz} c)}\\\text{absteigende}\\\text{Mengefolge}}}{=}\sum_{m=1}^{\infty}\lim\limits_{m\to\infty}\Prb\left(\bigcap_{k=m}^\infty A_K^C\right)\\
			&\overset{\text{unabh.}}{=}\sum_{m=1}^{\infty}\lim\limits_{n\to\infty}\prod_{k=m}^n(1-\Prb(A_k))\\
			&=\sum_{m=1}^{\infty}\lim\limits_{n\to\infty}e^{-\sum_{k=m}^{\Prb(A_k)}}\\
			&=\sum_{m=1}^{\infty}0=0
			\end{align*}
		\end{enumerate}
	\end{proof}

% 06.06.2017
	\section{Erwartungswert und Varianz}
	\begin{bem}[Motivation]
		Sei $(\Omega,\scF,\Prb)$ ein Wahrscheinlichkeitsraum und $(X_i)_{i\in\N}$ mit $X_i:\Omega\to\Z$, $\Prb(X_i=k)=p_k$ seien unabhängige Zufallsvariablen.\\
		(d.h. $(p_k)$ ist zähldichte für jedes $X_i$)\\
		Wenn man nun $N$-mal mit $(X_i)$ \enquote{würfelt}, erhält man $N$ zufällige ganze Zahlen $X_1(\omega),...,X_n(\omega)$.\\
		(\enquote{Realisierung von $(X_1,....,X_n)$})\\
		\begin{description}
			\item[Frage] Wie groß ist der (zufällige) Mittelwert für $\frac{1}{N}\sum_{i=1}^{n}X_i$ typischerweise wenn $n$ groß (unendlich groß) wird?
			\item[Beispiel] $\Prb(X_i=k)=\frac{1}{6}$, falls $k\in \{1,...,6\}$. Dann ist der Mittelwert $\approx3.5$. Es ist aber auch $(1,1,1,...,1)$ möglich.
			\item[Überlegung] $p_k$ modelliert die \enquote{relative Häufigkeit} mit der der Wert $k$ auftritt. \\
			D.h. für große $n$ sollte man cs $p_k\cdot n$-mal den Wert $k$ erhalten.\\
			Wähle z.B. $(-4,2,1,2,-4,0,1,3,2,-3,1,0)$:
			\begin{align*}
			\frac{1}{n}\sum_{i=1}^{n}X_i(\omega)&=\frac{1}{n}\left(0|\{i\leq n:X_i(\omega=0)\}|+1|\{i\leq n:X_i(\omega)=1\}|+(-1)|\{i\leq n:X_i(\omega=-1)\}|+...\right)\\
			&=\sum_{k\in\Z}\frac{k}{n}|\{i\leq n:X_i(\omega)=k\}|\\
			\overset{?}{\approx}\sum_{k\in\Z}kp_k\tag\star
			\end{align*}
			\item[Beobachtung] Das letzte $\approx$ stimmt sicher nicht immer, sollte aber \enquote{meistens} richtig sein.\\
			Siehe dazu \ref{gdgz} (Gesetze der großen Zahlen)
		\end{description}
		Betrachte nun die rechte Seite von $(\star)$:
	\end{bem}

	\begin{definition}
		Eine reelle Zufallsvariable $X:\Omega\to\R$ heißt \textbf{diskret} wenn die Menge $X(\Omega)$ höchstens abzählbar ist
	\end{definition}

	\begin{definition}
		Sei $X$ eine diskrete Zufallsvariable. $X$ heißt \textbf{integrierbar} (bzw \enquote{\textbf{Erwartungswert existiert}}), falls
		\[\sum_{y\in X(\Omega)}|y|\Prb(X=y)<\infty \label{eq:integrierbar}\]
		Man schreibt $X\in\scL^1$ bzw. $X\in\scL^1(\Prb)$.
	\end{definition}
	
	%TODO COUNTER
	\addtocounter{theorem}{-1}
	\begin{definition}
		Für $X\in\scL^1$ heißt
		\[\Epv(X)=\Epv_\Prb(X):=\sum_{y\in X(\Omega)}\Prb(X=y)\in\R\]
		der \textbf{Erwartungswert} von $X$.
	\end{definition}

	\begin{exmlist}
		\begin{exm}
			Sei $X:\Omega\to\{a,b\}$, $a,b\in\R$ und $\Prb(X=a)=p=1-\Prb(X=b)$.\\
			Dann ist $\Epv(X=p\cdot 0+(1-p)b=b+p(a-b)$.
			\begin{description}
				\item[$a=1$, $b=0$] Dann ist $\Epv(X=p$ (Münzwurf)
				\item[$a=1$, $b=-1$] Dann ist $\Epv(X=1-2p$ (Schrittweise Irrfahrt)
			\end{description}
		\end{exm}
		\begin{exm}
			Sei $T$ geometrisch verteilt, also $\Prb(T=k)=p(1-p)^{k-1}$, dann ist
			\[\Epv(X=\sum_{k=1}^{\infty}p{\overbrace{(1-p)}^{=:q}}^{k-1}=p\underbrace{\sum_{k=1}^{\infty}kq^{k-1}}_{=:f(q)}=(\star)\]
			Für die Funktion $f(q)$ gilt 
			\begin{align*}
			f(q)&=\frac{d}{dq}\sum_{k=0}^{\infty}q^k
			=\frac{d}{dy}\frac{1}{1-q}\\
			=\frac{1}{(1-q)^2}=\frac{1}{p^2}
			\end{align*}
			Also ist $(\star)=\frac{p}{p^2}=p$.
		\end{exm}
	\end{exmlist}

	\begin{satz}\label{0405satz}
		Seien $X,Y\in\scL^1$ diskrete Zufallsvariablen.
		\begin{enumerate}
			\item Falls $X\geq Y$, dann ist $\Epv(X)\geq \Epv(Y)$
			\item \begin{enumerate}
				\item Sei $c\in\R$, dann ist auch $cX\in\scL^1$ und $\Epv(cX)=c\Epv(X)$.
				\item Es ist auch $X+Y\in\scL^1$ und $\Epv(X+Y)=\Epv(X)+E(Y)$
			\end{enumerate}
		\end{enumerate}
	\end{satz}
	\begin{proof}
		Aus der Maßtheoreie, da $\Epv(X)$ das (elementare) Integral ist.
	\end{proof}

	\begin{definition}\label{0406def}
		Sei $X$ reelle Zufallsvariable. Dann heißt $X_{(n)}$ für alle $n\in\N$
		\[X_{(n)}(\omega:=\frac{1}{n})\lfloor nX(\omega)\rfloor\]
		die $\frac{1}{n}$-Approximation.\\
		(Dabei ist $\lfloor\cdot\rfloor$ die Abrunden-Funktion.)
	\end{definition}

	\begin{satz}\label{0407satz}
		Sei $X$ reelle Zufallsvariable, $X_{(n)}$ aus \ref{0406def}.
		\begin{enumerate}
			\item Für alle $n\in \N$ ist $X_{(n)}\leq X\leq X_{(n)}+\frac{1}{n}$
			\item Falls $X_(N_0)\in\scL^1$ für ein $n_0\in\N$, dann ist $X_{(n)}\in\scL^1$ für alle $n$.
			\item Falls b) gilt, so ist $\big(\Epv(X_{(n)})\big)_{n\in \N}$ eine Cauchyfolge.
		\end{enumerate}
	\end{satz}
	\begin{proof}$~$
		\begin{enumerate}
			\item ist klar.
			\item Aus a) folgt, dass $|X_{(n)}(\omega)|\leq |X_{(m)}|+\max\{\frac{1}{n},\frac{1}{m}\}$.\\
			Da insbesondere $1\in\scL^1(\Prb)$ folgt die Behauptung.
			\item Aus b) folgt, dass $|\Epv(X_{(n)})-\Epv(X_{(m)})|\leq\max\{\frac{1}{n},\frac{1}{m}\}\xrightarrow{n,m\to\infty}0$. Ist Cauchyfolge.
		\end{enumerate}
	\end{proof}

	\begin{satz}\label{0409satz}
		Die Aussagen von Satz \ref{0405satz} (Linearität und Monotonie) gelten auch für Reelle Zufallsvariablen.
	\end{satz}
	\begin{proof}
		Durch Grenzwertsätze der Maßtheorie:
	\end{proof}

%TODO COUNTER
\stepcounter{theorem}

	\begin{satz}[Monotonen Konvergenz]\label{0410monokonv}
		Seien $(X_n)_{n\in\N},X$ relle Zufallsvariablen mit punktweise $X_{n}(\omega)\leq X_{n+1}(\omega)$ für alle $\omega\in\Omega$ und $\lim\limits_{n\to\infty}X_n(\omega)=\sup_{n\in\N}X_n(\omega)=X(\omega)$.\\
		Dann gilt
		\[\lim\limits_{n\to\infty}\Epv(X_n)=\Epv(X)\]
	\end{satz}

	\begin{satz}[Majorisierte Konvergenz]\label{0411majkonv}
		Seien $(X_n)_{n\in\N},X$ relle Zufallsvariablen und es existiert $Y\in\scL^1$, mit $|X_n(\omega)|\leq |Y(\omega)|$ für alle $\omega\in\Omega$ und $\lim\limits_{n\to\infty}X_n(\omega)=X(n)$ für alle $\omega\in\Omega\setminus\Omega_0$, mit $\Omega_0$ Nullmenge. Dann ist
		\[\lim\liminf_{n\to\infty}\Epv(X_n)=E(X)\]
	\end{satz}

	\begin{definition}
		Sei $A:=\{\omega\in\Omega\mid\text{$\omega$ hat Eigenschaft $E$}\}\in\scF$.\\
		Falls $\Prb(A^c)=0$, dann sagen wir \enquote{die Eiegnschaft $E$ gilt $\Prb$-\textbf{fast sicher}}.
	\end{definition}

	\begin{exmlist}
		\begin{exm}
			Sei $\Prb=U[0,1]=\la_{[0,1]}$, $X(\omega)=\omega$.. Dann ist $X\notin\Q$ $\Prb$ fast sicher, denn $\la(\Q\cap[0,1])=0$
		\end{exm}
		\begin{exm}
			Sei $\Prb=U[-1,1]$, $X_n(y)=\sin\big(\frac{1}{|y|+\frac{1}{n}}\big)$, dann konvergiert $X_n(y)$ $\Prb$ fast sicher.
		\end{exm}
	\end{exmlist}

	\begin{bem}
		Falls $X\geq 0$, dann erlauben wir auch $\Epv(X)=\infty$. Wir definieren dann $\Epv(X)=\lim\limits_{n\to\infty}\Epv(\min\{X,n\})$.\\
		Satz \ref{0409satz}a),b) und \ref{0410monokonv} gilt weiterhin, falls $X,Y\geq 0$ und $c\geq 0$.
	\end{bem}

	\begin{satz}\label{0415fatou}
		Seien $(X_n)$ reele Zufallsvariable und $X_n\geq 0$, dann ist
		\[\Epv(\liminf_{n\to\infty} X_n)\leq \liminf_{n\to\infty} \Epv(X_n)\]
	\end{satz}

%VL 12.06.2017
	\begin{exm}
		Sei $(\Omega,\scF,\Prb)=\big((0,1),\Bor\big((0,1)\big),\la_{(0,1)}\big)$ und $X_n=n\cha_{0,\frac{1}{n}}$.\\
		Dann ist $\Epv(X_n)=1\forall n$, aber $\liminf_{n\to\infty}X_n(\omega)=0$.\\
		Also kann die Ungleichung in Satz \ref{0415fatou} kann also Strikt sein.
	\end{exm}

	\begin{satz}[Integration bezüglich des Bildmaßes]\label{0417satz}
		Sei $X:\Omega\to\Omega'$ Zufallsvariable, $f:\Omega'\to\R$ messbar.\\
		Es gelte entweder $f(\omega')\geq 0\forall\omega'\in\Omega'$ oder
		\[\Epv\left(|f(X)|\right)=\int_\Omega|f\big(X(\omega)\big)|\Prb(d\omega)<\infty\]
		Dann gilt:
		\[\Epv(f(X))=\int_{\Omega'}f(\omega')\Prb_X(d\omega')=\Epv_{\Prb_X}(f)=\int_{\Omega'}f(\omega')\Prb\circ X^{-1}(d\omega')\]
	\end{satz}
	\begin{proof}
		\begin{enumerate}[label=(\roman*)]
			\item Sei $f(\omega')=\cha_B(\omega')$ mit $B\in\scF'$.\\
			Dann ist $\Epv(f(X))=\Epv(\cha_B(X))=\Prb(X\in B)=\Prb(X^{-1(B)})=\Prb_X(B)$ nach Definition von $\Prb_X$. Dann gilt die Gleichung aus \ref{0417satz}.
			\item Sei nun $f(\omega')=\sum_{n=1}^{m}c_n\cha_{B_n}(\omega')$, mit $C_n\in\R$, $B_n\in\scF'$.
			Dann ist
			\[\Epv(f(X))\overset{\ref{0409satz}}{=}\sum_{n=1}^{m}c_n\Epv(\cha_{B_n}(X))\overset{(i)}{=}\sum_{n=1}^{m}c_n\Epv_{\Prb_X}(\cha_{B_n})\overset{\ref{0409satz}b)}{=}\Epv_{\Prb_X}\left(\sum_{n=1}^{m}c_n\cha_{B_n}\right)=\Epv_{\Prb_X}(f)\]
			\item Sei nun $f(\omega')\geq 0\forall\omega\in\Omega$. Setze 
			\[f_n(\omega'):=\sup\left\{\frac{1}{n}\lfloor f(\omega')\rfloor,n\right\}\]
			(Abgeschnittene $\frac{1}{n}$ Approximation.)\\
			Dann folgt aus (ii), dass $\Epv(f_n(X))=\Epv_{\Prb_X}(f_n)$. Dann folgt aus der monotonen Konvergenz, dass im Grenzwert $n\to\infty$ gilt: $\Epv(f(X))=\Epv_{\Prb_X}(f)$. Dann gilt die Gleichung aus \ref{0417satz}.
			\item Sei $\Epv(|f(X)|)<\infty$. Da $|f(X(\omega))|=f_+(X(\omega))+f_-(\omega')$ ist auch $\Epv(f_-(X))<\infty$ und $\Epv(f_+(X))<\infty$.\\
			Da $f(\omega')=f_+(\omega')-f_-(\omega')$ gilt ist 
			\[\Epv(f(X))=\Epv(f_+(X))-\Epv(f_-(X))\overset{(iii)}{=}\Epv_{\Prb_X}(f_+)-\Epv_{\Prb_X}(f_-)=\Epv_{\Prb_X}(f)\]
		\end{enumerate}
	\end{proof}

	\begin{bem}
	 	Sei $H:[a,b]\to\R$ streng monoton wachsend und $h\in C^1$. Sei $\Omega=[a,b],\scF\Bor([a,b]),\mu(dx)=h'(x)~dx$.\\
		Dann ist $\mu([a,b])=\int_{a}^{b}h'(x)~dx=h(b)-h(a)$ das Bildmaß von $\mu$ unter $h$.
		\begin{align*}
		\mu\circ h^{-1}\big((-\infty,x]\big)&=\mu\big(\{y\in[a,b]:-\infty\leq h(y)\leq x\}\big)=\mu\big([a,b]\cap(-\infty,h^{-1}(x))\big)\\
		&=\int_{a}^{h^{-1(x)}}h'(y)~dy=\left[h(y)\right]_a^{h^{-1}(x)}=h(h^{-1}(x))-h(a)=x-h(a)
		\end{align*}
		falls $h(a)\leq x\leq h(b)$. Dann ist
		\[\mu\circ h^{-1}=\la_{[h(a),h[b]]}\]
		Dann folgt aus \ref{0417satz}
		\[\int_{a}^{b}f(h(x))h'(x)~dx=\Epv_\mu(f(h))=\Epv_{\Prb_n}(f)=\int_{h(a)}^{h(b)}f(y)~dy\]
	\end{bem}

	\begin{kor}
		Seien $X:\Omega_1\to\Omega'$, $Y:\Omega_2\to\Omega'$ Zufallsvariablen mit $\Prb_X=\Prb_Y$. Dann gilt $\Epv(f(x))=\Epv(f(Y))$ für alle $f:\Omega'\to\R$ mit $\Epv(|f(x)|)<\infty$ oder $f\geq 0$.\\
		Umgekehrt: sei $\Epv(f(X))=\Epv(f(Y))$ für alle $f$ messbar, beschränkt, dann gilt $\Prb_X=\Prb_Y$.
	\end{kor}
	\begin{proof}
		\begin{description}
			\item[\enquote{$\Rightarrow$}] Folgt aus \ref{0417satz}.
			\item[\enquote{$\Leftarrow$}] $\Prb_X(A)=\Epv(\cha_A(X))=\Epv(\cha_A(X))=\Prb_Y(A)$.
		\end{description}
	\end{proof}

	\begin{exmlist}
		\begin{exm}
			Sei $X\sim\Exp(\al)$, d.h. $\Prb_X(dx)=\al e^{-\al x}~dx$. Dann ist
			\[\Epv(X^k)=\al \int_{0}^{\infty}x^ke^{-\al x}~dx=\frac{k!}{\al^k}\]
		\end{exm}
		\begin{exm}
			Sei $X\sim \Nv(m,v)$, dann ist $\Epv(X)=m$ und $\Epv\big((X-m)^2\big)=v$
		\end{exm}
	\end{exmlist}

	\begin{kor}\label{0421kor}
		\begin{enumerate}
			\item Sei $X$ reelle Zufallsvariable. $\Prb_X$ habe dichte $\rho$ bezüglich des Lebesgue-Maß. Dann ist
			\[\Epv(f(X))=\int_{-\infty}^{\infty}\rho(x)f(x)~dx\]
			(falls $\int\rho(x)|f(x)|~dx<\infty$). Inesbesondere
			\[\Epv(X)=\int_{-\infty}^{\infty}x\rho(x)~dx\]
			\item Sei $\Omega=\R$, $\Prb(dy)=\rho(y)~dy$ ($\rho$ ist Dichte von $\Prb$).\\
			Sei $X$ reelle Zufallsvariable . Dann gilt:
			\[\Epv(f(X))=\int_{-\infty}^{\infty}f(X(y))\rho(y)~dy\]
			Insbesondere $\Epv(X=\int_{-\infty}^{\infty}X(y)\rho(y)~dy$
		\end{enumerate}
	\end{kor}

	\begin{definition}
		Sei $X$ reelle Zufallsvariable
		\begin{enumerate}
			\item Falls $|X^n|\in\scL^1$ für $n\in\N$, so heißst $\Epv(X^n)$ das \textbf{$n$-te Moment von $X$}.
			\item Falls $|X^p|\in\scL^1$, für $p>0$, so schreibt man $X\in\scL^p$.
			\item Die Zahl $\Var(X):=\Epv\left((X-\Epv(X))^2\right)$ heißt die \textbf{Varianz} von $X$ (bezüglich $\Prb$).\\
			$\sqrt{\Var(X)}$ heißt \textbf{Standardabweichung} von $X$.
		\end{enumerate}
	\end{definition}

	\begin{bem}
		\begin{enumerate}
			\item Für die Varianz gilt \[\Var(X)=\Epv(X^2-E\Epv(X)X+\Epv(X)^2)=\Epv(X^2)-2\Epv(X)\Epv(X)+\Epv(X)^2=\Epv(X^2)-\Epv(X)^2\]
			\item Da $|X|^p\leq 1+|X|^q$, falls $p\leq q$ ist
			\[\scL^q\leq \scL^p\]
			falls $p\leq q$.
		\end{enumerate}
	\end{bem}

%VL 13.06.2017
	\begin{definition}
		Sei $X,Y\in\scL^2$, dann heißt \[\Cov(X,Y):=\Epv\big(X-\Epv(X)\big)\big(Y-\Epv(Y)\big)=\Epv(XY)-\Epv(X)\Epv(Y)\]
		die \textbf{Kovarianz} von $X$ und $Y$.\\
		Man nennt $X$ und $Y$
		\begin{description}
			\item[positiv korreliert] falls $\Cov(X,Y)>0$
			\item[unkorreliert] falls $\Cov(X,Y)=0$
			\item[negtaiv korreliert] falls $\Cov(X,Y)<0$
		\end{description}
	\end{definition}

	\begin{exm}\label{0425exm}
		Sei $\Omega=[0,1]$ mit Lebesgue-Maß, sei $X(\omega)=\omega$,$Y(\omega)=\omega^2$ und $Z(\omega)=\frac{1}{2}\sin(\pi\omega)$.
		Dann ist
		\begin{align*}
		\Epv(X)&=\int_{0}^{1}d~dx=\frac{1}{2}\\
		\Epv(Y)&=\int_{0}^{1}x^2~dx=\frac{1}{3}\\
		\Epv(Z)&=\int_{0}^{1}\frac{\sin(\pi\omega)}{2}~dx=\frac{1}{2\pi}
		\end{align*}
		Dann folgt für die Kovarianz:
		\begin{align*}
		\Cov(X,Y)&=\Epv\big(\left(X-\frac{1}{2}\right)\left(Y-\frac{1}{3}\right)\big)=\frac{1}{12}\\
		\Cov(X,Z)&=\int_{0}^{1}\left(x-\frac{1}{2}\right)\left(\sin(\pi x)-\frac{1}{2\pi}\right)~dx=0
		\text{Zum Vergleich: }\Cov(X,X)&=\Var(X)=\frac{1}{12}
		\end{align*}
		Daher eigenen sich die Korrelationskoeffizienten besser al Maß
	\end{exm}

	\begin{definition}
		Für $X,Y\in\scL^2$ heißt\[\Kor(X,Y)=\frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}\]
		\textbf{Korrelationskoeffizient} von $X$ und $Y$.
	\end{definition}

	\begin{exm}
		In Beispiel\ref{0425exm} gilt
		\begin{align*}
		\Kor(X,Y)&=\frac{\sqrt{15}}{4}\approx0.968\\
		\Kor(X,X)&=1
		\end{align*}
	\end{exm}

	\begin{satz}[Ungleichungen für Integrale]\label{0428intUngl}
		\begin{enumerate}
			\item \textbf{Jensen-Ungleichung} Sei $X$ reelle Zufallsvariable, $\phi(\R\to\R)$ sei konvex.\\
			Sei zusätzlich noch $\Epv(|X|)<\infty$ und $\Epv(|\phi(X)|)<\infty$.\\
			Dann gilt
			\[\Epv(\phi(X))\geq \phi(\Epv(X))\]
			
			%TODO COUNTER
			\setcounter{enumi}{3}
			\item \label{chebyshev-markov}
			\textbf{Chebyshev-Markov-Ungleichung} Sei $X$ reelle Zufallsvariable, $f:[0,\infty)\to[0,\infty)$ sei monoton wachsend. Dann gilt $\forall c>0$ mit $f(c)>0$, dass\[\Prb(|X|\geq c)\leq\frac{\Epv(f(|X|))}{f(c)}\]
			Insbesondere gilt für $f(x)=x^2$
			\[\Prb(|X-\Epv(X)|\geq c)\leq\frac{\Var(X)}{c^2}\]
			
			%TODO COUNTER
			\setcounter{enumi}{1}
			\item\textbf{Hölder Ungleichung} Seine $X,Y$ reelle Zufallsvariablen. Dann gilt für alle $p,q\in[1,\infty]$ mit $\frac{1}{p}+\frac{1}{q}=1$, dass
			\[\Epv(|XY|)\leq\norm{X}_p\norm{Y}_q\]
			dabei ist 
			\[\norm{A}_p:=\begin{cases}
			\Epv(|X|^p)^\frac{1}{p}&p\leq \infty\\
			\sup\{|X(\omega)|:\omega\in\Omega\}&p=\infty
			\end{cases}\]
			\item\textbf{Cauchy-Schwarz-Ungleichung} Seien $X,Y\in\scL^2$. Dann ist $XY\in\scL^2$ und\[\Epv(|XY|)^2\leq\Epv(|X|^2)\Epv(|Y|^2)\]
			
		\end{enumerate}
	\end{satz}
	\begin{proof}
		\begin{enumerate}
			\item Da $\phi$ konvex ist, ist per Definition $A:=\{(x,y)\in\R^2:y\geq\phi(x)\}$ eine konvexe Menge.\\
			$\Leftrightarrow$ $\forall x\in\R$ gibt es eine Gerade durch $(x,\phi(X))$, die das Innere $A^0$ von $A$ nicht schneidet.\\
			$\Leftrightarrow $ $\forall x\in\R\exists a\in\R:\ell(y):=\phi(x)+a(y-x)\leq \phi(y)$.\\
			Wähle nun $x=\Epv(X)$. Dann ist 
			\begin{align*}
			\phi\big(X(\omega)\big)\geq \ell\big(X(\omega)\big)=\phi\big(\Epv(X)\big)+a\big(X(\omega)-\Epv(X)\big)
			\end{align*}
			Da der Erwartungswert monoton ist erhält anwenden des Erwartungswerts auf beide Seiten die Ungleichung:
			\[\Epv\big(\phi(X)\big)\geq \Epv\big(\phi(\Epv(X))+a(X-\Epv(X))\big)
			=\Epv\big(\phi(\Epv(X))\big)+a\Epv\big(X-\Epv(X)\big)=\phi\big(\Epv(X)\big)\]
			
			%TODO COUNTER
			\setcounter{enumi}{3}
			\item Es gilt
			\begin{align*}
			\Prb(|X|\ge c)&=\Prb(f(|X|)\geq f(c))\\
			&=\Prb\left(\frac{f(|X|)}{f(c)}\geq 1\right)=\Epv\left(\cha_{\left\{\frac{f(|X|)}{f(c)}\geq 1\right\}}\right)\\
			&\leq\Epv\left(\cha_{\left\{\frac{f(|X|)}{f(c)}\geq 1\right\}\frac{f(|X|)}{f(c)}}\right)\\
			&\leq\Epv\left(\frac{f(|X|)}{f(c)}\right)=\frac{1}{f(c)}\Epv(f(|X|))
			\end{align*}
			
			%TODO COUNTER
			\setcounter{enumi}{1}
			\item Für $p=\infty$ gilt  $\Epv(\abs{XY})\leq\Epv(\norm{X}_\infty \abs{Y})=\norm
			X_\infty\norm{Y}_1$.\\
			Für $p<\infty$: Falls $\norm{X}_p=0$, dann folgt für $c>0$ aus d), dass $\Prb(|X|>c)\leq\norm{X}_p^{p}(c^p)=0$. Somit its
			\[\Prb(\abs{X}>0)=\Prb\left(\bigcap_{n\in\N}\{\omega:X(\omega)>\frac{1}{n}\}\right)\overset{\ref{0122satz}e)}{=}\lim\limits_{n\to\infty}\Prb(X\geq 1/n)=0\]\\
			Es folgt, dass $\Epv(|XY|)=\Epv(|XY|\cha_{\{\abs{X}>0\}})=0$, sodass die Ungleichung gilt (als Gleichung).\\
			Sei $\norm{X}_p>0$. Da in der Behauptung nur Beträge vorkommen können wir o.b.d.A. annehmen, dass $X>0$ und $Y\geq 0$ ist. Sei nun das Wahrscheinlichkeitsmaß $\tilde P$ auf $\Omega$ definiert als
			\[\tilde{P}(A):=\Prb\left(\cha_A\frac{X^p}{\Epv(X^p)}\right).\]
			Sei außerdem $Z:=Y/X^{p-1}$. Dann ist
			\[\Epv(XY)=\Epv(X^pZ)=\tilde\Epv\big(Z\Epv(X^p)\big)=\tilde{\Epv}(Z)\Epv(X^p)\tag{\star}\]
			Anwenden der Jensen Ungleichung a) mit der Funktion $\phi(x)=x^q\cha_{\{q>0\}}$ ergibt
			\begin{align*}
			(\star)&\leq \tilde{\Epv}(Z^q)^\frac{1}{q}\Epv(X^p)=\Epv\left(\frac{Y^q}{X^{q(p-1)}\frac{X^p}{\Epv(X^p)}}\right)\Epv(X^p)\\
			&=\Epv(Y^qX^{p-q(p-1)})^{\frac{1}{q}}\Epv(X^p)^{1-\frac{1}{q}}=\Epv(Y^q)^\frac{1}{q}\Epv(X^p)^\frac{1}{p}
			\end{align*}
			(Da $p,q$ konjugierte Exponenten sind ist $p-pq+q=0$.)
			\item Spezialfall von b) mit $p=q=2$
		\end{enumerate}
	\end{proof}

	\begin{bem}
		\begin{itemize}
			\item Sei $X,Y\in\scL^2$, dann gilt mit \ref{0428intUngl} c), dass auch$X+\al Y\in\scL^2$. Also ist $\scL^2$ ein Vektorraum und $\norm{X}_2=\Epv(|X|^2)^\frac{a}{2}$ ist eine Halbnorm.
			Dann ist $\Var(X)=\Epv\big((X-\Epv(X))^2\big)$ die \enquote{Länge des optimal Verschobenen Vektors}:
			\[\Var(X)=\min\{\Epv\big((X-a)^2\big):r\in\R\}\]
			und
			\[\Cov(X,Y)=\Epv\left(\big(X-\Epv(X)\big)\big(Y-\Epv(Y)\big)\right)\]
			ist ein Skalarprodukt der beiden optimal verschobenen Vektoren.
			\[\Kor(X,Y)=\frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}\]
			gibt den Winkel zwischen optimal verschobenen Vektoren an.
		\end{itemize}
	\end{bem}

	\begin{satz}\label{0430rechenregeln}
		\begin{enumerate}
			\item $\Cov(X+a,Y+b)=\Cov(X,Y)$ für alle $a,b\in\R$.
			\item $\Cov(aX,bY)=ab\Cov(X,Y)$
			\item $\Var(aX)=a^2\Var(X)$
			\item Es ist $\sum_{i=1}^{n}X_i\in\scL^2$ und
			\[\Var(\sum_{i=1}^{n}X_i)=\sum_{i=1}^{n}\Var(X_i)+\sum_{\substack{i,j=0\\i\neq j}}^{n}\Cov(X_i,X_j)\tag{\text{Binomische Formel}}\]
			\item Falls $(X_i)$ paarweise unkeorreliert sind, dann
			\[\Var\left(\sum_{i=1}^{\infty}X_i\right)=\sum_{i=1}^\infty\Var(X_i)\tag{\text{\enquote{Gleichheit von Bienarme}/\enquote{Pythagoras}}}\]
		\end{enumerate}
	\end{satz}
	
	\begin{satz}
		Seien $X,Y\in\scL^1$ und $X\indep Y$, dann gilt
		\begin{enumerate}
			\item $XY\in\scL^1$ und $\Epv(XY)=\Epv(X)\Epv(Y)$
			\item $\Cov(X,Y)=0$
		\end{enumerate}
	\end{satz}
%VL 19.06.2017
	\begin{proof}
		\begin{enumerate}
			\item Da $X\indep Y$ ist folgt mit \ref{0323}a), dass das Bildmaß von $\Prb$ unter $(X,Y):\Omega\to\R^2$ ein Produktmaß, also $\Prb_X\otimes\Prb_Y$ ist.\\
			Nach \ref[a)]{0417satz}ist
			\begin{align*}
			\Epv\big(|X||Y|\big)&=\int|x||y|\Prb_{(XY)}~(dx~dy)=\int_{\R^2}|x||y|\Prb_X(dx)\Prb_Y(dy)
			\intertext{mit $h:\R^2\to R,(x,y)\mapsto |x|y|$. Dann folgt mit dem Satz von Fubini}
			&=\int_{\R}|x|\Prb_X(dx)\int_\R|y|\Prb_Y(dy)=\Epv(|X|)\Epv(|Y|)
			\end{align*}
			Dann folgt dass $XY\in\scL^1$ ist. Also darf die gleiche Rechnung auch ohne $|\cdot|$ durchgeführt werden.
			\item Es gilt
			\[\Cov(X,Y)=\Epv\left(\big(X-\Epv(X)\big)\big(Y-\Epv(Y)\big)\right)\overset{a)}{=}\Epv\big(X-\Epv(X)\big)\Epv\big(Y-\Epv(Y)\big)=0\]
		\end{enumerate}
	\end{proof}

	\begin{bem}
		\begin{enumerate}
			\item \ref[e)]{0430rechenregeln} gilt also auch wenn $X_i\indep X_j$ für alle $i\neq j$.
			\item Es kann aber $\Epv(XY)=\Epv(X)\Epv(Y)$ gelten, ohne dass $X,Y$ unabhängig sind.
		\end{enumerate}
	\end{bem}

	\begin{exm*}
		Sei $\Omega=[0,1]$, $\Prb=\frac{1}{2}\lambda_{[-1,1]}$, $X(x)=y$, $Y(x)=x^2$. Dann ist
		\[\Epv(XY)=\frac{1}{2}\int_{-1}^{1}x^3~dx=0=\Epv(X)=\Epv(X)\Epv(Y)\]
		\[\Epv(X^2Y)=\frac{1}{2}\int_{-1}^{1}x^4~dx=\frac{1}{5}\quad\text{aber}\quad\Epv(\Epv(X^2))\Epv(Y)=\left(\frac{1}{3}\right)^3=\frac{1}{9}\]
		Wären $X\indep Y$, dann müsste auch $X^2\indep Y$ sein, also muss $X\not\indep Y$.
	\end{exm*}
	\begin{bem*}
		Orthogonalität in $\scL^2$ ist viel schwächer (und weniger stabil) als Unabhängigkeit.
	\end{bem*}
	
	
	\subsection{Erzeugende Funktionen}
	Für viele Wahrscheinlichkeitsmaße auf $\N_0$ besteht die Zähldichte aus den Taylor Koeffizienten einer Funktion entwickelt bei $x=0$.
	\begin{exm}
		Poisson Verteilung: $e^{-\la}\frac{\la^k}{k!}$. Dann ist
		\[e^{\la u-\la}=e^{-\la}\sum_{k=0}^{\infty}\frac{\la^ku^k}{k!}\]
		Geometrische Verteilung: $(1-p)^{k-1}p$. Dann ist
		\[\sum_{k=0}^{\infty}(1-p)^{k-1}pu^k=\frac{p}{1-p}\sum_{k=1}^{\infty}\big((1-p)\big)^k=\frac{p}{1-p}\frac{1}{1-(1-p)u}\]
	\end{exm}

	\begin{definition}
		Sei $\Prb$ ein Wahrscheinlichkeitsmaß auf $\N_0$ mit Zähldichte $(p_k)_{k\geq 0}$. Die Funktion
		\[\varphi_{\Prb}:[0,1]\to\R\quad \varphi_{\Prb}\Prb(u)=\sum_{k=0}^{\infty}p_ku^k\]
		heißt \textbf{Erzeugende Funktion} von $\Prb$ (oder der Folge $(p_k)$).
	\end{definition}

	\begin{bem}
		\begin{enumerate}
			\item Da $\sum_{k=0}^{\infty}p_k=1$ ist der Konvergenzradius von $\phi$ größer gleich als 1.
			\item Es gilt 
			\[\varphi_{\scB_{n,p}}(u)=\sum_{k=0}^{n}p^k(1-p)^{n-k}\binom{n}{k}u^k=\big(pu+(1-p)\big)^n\]
		\end{enumerate}
	\end{bem}

	\begin{definition}
		Sei $X$ Zufallsvariable mit Werten in $N_0$. Dann heißt\[\varphi_X(u)=\Epv(u^X)=\sum_{k=0}^{\infty}\Prb(X=k)u^k\]
		\textbf{erzeugende Funktion von $X$}.
	\end{definition}

	\begin{satz}\label{0436satz}
		Sei $X$ eine $N_0$-wertige Zufallsvariable. Dann
		\begin{enumerate}
			\item $\Prb(X=k)=\frac{1}{k!}\left.\frac{d^k}{du^k}\varphi_X(u)\right|_{u=0}$  für alle $k\in\N$.
			\item $\Epv(X)$ existiert $\Leftrightarrow$ $\lim\limits_{u\uparrow 1}\phi'(u)$ existiert. Dann ist $\Epv(X)=\varphi'_X(1)$.
			\item Für alle $p\in \N$ ist $X\in\scL^p$ $\Leftrightarrow$ $\lim\limits_{u\uparrow 1}\frac{d^p}{du^p}\varphi(u)$ existiert. Dann ist
			\[\frac{d^p}{du^p}\varphi_X(1)=\Epv\big(X(X-1)\cdot...\cdot(X-p+1)\big)\]
		\end{enumerate}
	\end{satz}
	\begin{proof}
		\begin{enumerate}
			\item Satz von Taylor: Für alle $u\leq 1$(Konvergenzradius)
			\[\partial_u^k\varphi(u)=\sum_{n=0}^{\infty}p_n(\partial^nd^k)=\sum_{n=k}^{\infty}p_n\frac{n!}{(n-k)!}u^{k-n}\]
			Also gilt bei Auswertung in $0$
			\[\frac{1}{k!}\left.\partial_n^k\varphi(n)\right|_{u=0}=p_k=\Prb(X=k)\]
			\item Für $u<1$ konvergiert die Reihe absolut
			\begin{align*}
			\sum_{k=0}^{\infty}ku^k\Prb(X=k)&=u\sum_{k=0}^{\infty}(\partial_uu^k)\Prb(X=k)
			\intertext{also darf man gliedweise Differenzieren}
			&=u\partial u\sum_{k=0}^{\infty}u^k\Prb(X=k)=u\varphi'_X(u)
			\end{align*}
			Beide Seiten wachsen Monoton, wenn $u\uparrow 1$:
			\[\text{Linke Seite}\operatornamewithlimits{\uparrow}_{u\uparrow 1}\Epv(X)\quad\text{Rechte Seite}\operatornamewithlimits{\uparrow}_{u\uparrow 1}\varphi'(1)\]
			\item Anwenden des gleichen Arguments mit
			\[\sum_{k\geq p}k(k-1)...(k-p+1)u^k\Prb(X=k)=u^p\sum_{k=0}^{\infty}\partial_u^p(u^k)\Prb(X=k)\]
			gegen $\Epv\big(X(X-1)...(X-p+1)\big)$ bzw. $1^p\partial_u^p\varphi_X(1)$.
		\end{enumerate}
	\end{proof}

	\begin{exm}
		Sei $X\sim\Poi_\la$: Dann ist
		\[\Epv(X)=\left.\partial_u\left(e^{\la u-\la}\right)\right|_{u=1}=\la e^0=\la\]
		\[\Epv\big(X(X-1)\big)=\partial^2_u\left(e^{\la u-\la}\right)=\la^2\]
		Also gilt für die Varianz:
		\[\Var(X)=\Epv(X^2)-\Epv(X)^2=\Epv\big(X(X-1)+\Epv(X)-\Epv(X)^2\big)=\la\]
	\end{exm}

	\begin{satz}\label{0438satz}
		Seien $X,Y$ unabhängige $\N_0$-verteilte Zufallsvariablen. Dann gilt
		\[\varphi_{X+Y}(u)=\varphi_X(u)\varphi_Y(u)\]
	\end{satz}
	\begin{proof}
		Sei $X\sim\Poi_\la$, $Y\sim\Poi_\mu$, $Y\indep X$. Dann ist\[\varphi_{X+Y}(u)=\varphi_X(u)\varphi_Y(u)=e^{\la u-\la}e^{\mu u-\mu}=e^{(\mu+\la)u-(\mu+\la)}\]
		Also $X+Y\sim\Poi_{\la+\mu}$.
	\end{proof}

	\begin{bem}[Ausblick]
		\begin{enumerate}
			\item Die rechte Seite in \ref{0436satz}c) ist etwas unhandlich. Daher definiere für $t>0$
			\[L_X(t):=\varphi_X(e^{-t})=\Epv\big(\left(e^{-t}\right)^X\big)=\Epv\left(e^{-t}\right)\tag{\enquote{Laplace Transformierte von $\Prb_X$}}\]
			Dann ist
			\[\Epv(X^p)=\left.\frac{d^p}{dt^p}L_X\right|_{t=0}\]
			falls $\Epv(X^p)<\infty$.
			\item Das geht auch für reelle Zufallsvariablen. Sei $X:\Omega\to\R$. Es sei $\Epv\left(e^{-tX}\right)<\infty$ für ein $c>0$. Dann ist
			\[L_X(t)=\Epv(e^{-tX})=\int_{-\infty}^{\infty}e^{-t}\Prb_X(dy)\]
			für alle $t\in\{\tilde t\in\R\mid \Epv(e^{-\tilde t X})<\infty\}$.\\
			Wieder gilt
			\[\Epv(X^p)=(-1)^p\left.\partial_t^pL_X(t)\right|_{t=0}\]
			und es gilt auch $L_X$ bestimmt $P_X$ eindeutig.\\
			Satz \ref{0438satz} gilt mit $L_{X+Y}=L_XL_Y$. Außerdem ist $L_{\al X}(t)=\Epv\left(e^{-t\al X}\right)=L_X(\al t)$, falls alles $<\infty$.
			\item Für unabhängige reelle Zufallsvariablen $X_i$ mit $\Prb_{X_i}=\Prb_{X_j}$ gilt
			\[(**)=L_{\frac{1}{\sqrt{n}}\sum_{i=1}^n X_j}(t)=\prod_{i=1}^{n}L_{\frac{X_i}{\sqrt n}}(t)=\prod_{i=1}^nL_{X_i}\left(\frac{t}{\sqrt{ n}}\right)=\left(L_{X_1}\left(\frac{t}{\sqrt{n}}\right)\right)^n\]
			Es gilt immer $L_X(0)=1$ und $L'_X(0)=\Epv(X)$.\\
			Angenommen $\Epv(X)=0$ und $L''_X(0)=\Epv(X^2)<\infty$, dann gilt die Taylor-Gleichung:
			\[\ln(L_X(s))=\ln\left(L_X(0)+sL'_X(0)+\scO(s^3)\right)=\frac{1}{2}s^2L''_X(0)+\scO(s^3)\]
		\end{enumerate}
	Es folgt, dass 
	\[(**)=e^{n\ln L_X\left(\frac{t}{\sqrt{n}}\right)}=e^{n\left(\frac{1}{2}\frac{t^2}{n}L''_X(0)+\scO\left(\frac{t^3}{n^3}\right)\right)}\xrightarrow{n\to\infty}e^{\frac{t^2}{2}\Epv(X^2)}\]
	\end{bem}

%VL 20.06.2017

\section{Grenzwertsätze}
	Wir haben $\Epv(X)=\sum_{k\in\Z}k\Prb(X=k)$ über die relative Häufigkeit motiviert: 
	\[\Epv(X)\approx\frac{1}{N}\sum_{i=1}^NX_i(\omega)\approx\sum_{k\in\Z}\frac{|\{i:X_i(\omega=k)\}|}{N}k\]
	Wir werden nu begründen, dass dies Intuition richtig ist.
	
	\begin{definition}
		Eine Folge $(X_n)$ von Zufallsvariablen heißt \textbf{unabhängig identisch Verteilt} (\textbf{uiv}), wenn
		\begin{enumerate}[label=(\roman*)]
			\item die $X_n$ sind paarweise unabhängig
			\item $\Prb_{X_i}=\Prb_{X_j}$ für alle $i,j\in\N$.
		\end{enumerate}
	\end{definition}

	\begin{satz}[Schwache Gesetz der großen Zahlen (GGZ), Basisversion]\label{0502satzGGZschwach}
		Seien $(X_i)$ uiv, reelle Zufallsvariable mit $\Var(X_i)<\infty$.\\
		Dann ist für alle $\epsilon>0$
		\[\lim\limits_{n\to\infty}\Prb\left(\abs{\frac{1}{n}\sum_{i=1}^{n}X_i-\Epv(X_1)}\geq \epsilon\right)=0 \autotag\label{eq:0501}\]
	\end{satz}
	\begin{proof}
		Setze $Y_n=\frac{1}{n}\sum_{i=1}^{n}X_i$, dann ist 
		\[\Epv(Y_n)=\Epv\left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)=\frac{1}{n}\sum_{i=1}^{n}\Epv(X_i)=\Epv(X_1)\]
		\[\Var(Y_n)=\Var\left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)\overset{\ref{0430rechenregeln}}{=}\frac{1}{n^2}\sum_{i=1}^{n}\Var(X_i)=\frac{1}{n}\Var(X_1)\]
		Mit \ref{0428intUngl}d) (Chebyshev-Markov-Ungleichung) folgt
		\[\Prb\left(\abs{\frac{1}{n}\sum_{i=1}^{n}X_i-\Epv(X_1)}>\epsilon\right)=\Prb\left(\abs{Y_n-\Epv(Y_n)}>\epsilon\right)\leq\frac{1}{\epsilon^2}\Var(Y_n)=\Var(X_1)\frac{1}{n\epsilon^2}\xrightarrow{n\to\infty}0\]
	\end{proof}

	\begin{satz}[Schwache Gesetz der großen Zahlen, $\scL^2$-Version]\label{0503satzGGZL^2}
		Seien $(X_i)$ (nicht notwendigerweise uiv) reelle Zufallsvariablen für die gilt:
		\begin{enumerate}[label=(\roman*)]
			\item \textbf{Paarweise unkorreliert} $\Cov(X_i,X_j)=0$ für alle $i\neq j$.
			\item \textbf{Beschränkte Varianzen} $v:=\sup\left\{\Var(X_i),i\in\N\right\}<\infty$
		\end{enumerate}
		Dann gilt für alle $n\in \N$ und alle $\epsilon_n>0$
		\[\Prb\left(\abs{\frac{1}{n}\sum_{i=1}^{n}\big(X_i-\Epv(X_i)\big)}\geq\epsilon_n\right)\leq\frac{v}{n\epsilon_n^2}\]
	\end{satz}
	\begin{proof}
		Analog, Übung %TODO Beweis Übung
	\end{proof}
	
	\begin{satz}[Schwache Gesetz der großen Zahlen, $\scL^1$-Version]\label{0504satzGGZL^1}
		Seien $(X_i)$ paarweise undabhängige, identisch Verteilte Zufallsvariablen und $\Epv(|X_1|)<\infty$.\\
		Dann gilt für alle $\epsilon>0$
		\[\lim\limits_{n\to\infty}\Prb\left(\abs{\frac{1}{n}\sum_{i=1}^{n}X_i-\Epv(X_1)}\geq \epsilon\right)=0\]
	\end{satz}
	\begin{proof}
		Zerlege 
		\[X_i(\omega)=\underbrace{X_i(\omega)\cha_{\{|X_i(\omega)|<\sqrt[4]{i}\}}(\omega)}_{Y_i(\omega)}
		+\underbrace{X_i(\omega)\cha_{\{|X_i(\omega)|\geq\sqrt[4]{i}\}}(\omega)}_{Z_i(\omega)}\]
		Dann gilt
		\begin{enumerate}
			\item $\Epv(X_i)=\Epv(Y_i)+\Epv(Z_i)$
			\item mit der Dreiecksungleichung
			\begin{align*}
			\abs{\frac{1}{n}\sum_{i=1}^{n}X_i(\omega)-\Epv(X_1)}&=\frac{1}{n}\abs{\sum_{i=1}^{n}\left(Y_i(\omega)-\Epv(Y_i)+Z_i(\omega)-\Epv(Z_i)\right)}\\
			&\leq \frac{1}{n}\abs{\sum_{i=1}^{n}\left(Y_i(\omega)-\Epv(Y_i)\right)}
			 +\frac{1}{n}\abs{\sum_{i=1}^{n}\left(Z_i(\omega)-\Epv(Z_i)\right)}
			\end{align*}
		\end{enumerate}
	Daher ist
	\begin{align*}
	\underbrace{\left\{\omega\in\Omega:\abs{\frac{1}{n}\sum_{i=1}^{n}X_i(\omega)-\Epv(X_i)}\geq\epsilon\right\}}_{A_n}\subset&
	\underbrace{\left\{\omega\in\Omega:\abs{\frac{1}{n}\sum_{i=1}^{n}Y_i(\omega)-\Epv(Y_i)}\geq\frac{\epsilon}{2}\right\}}_{B_n}\\&\bigcup
	\underbrace{\left\{\omega\in\Omega:\abs{\frac{1}{n}\sum_{i=1}^{n}Z_i(\omega)-\Epv(Z_i)}\geq\frac{\epsilon}{2}\right\}}_{C_n}
	\end{align*}
	und damit $\Prb(A_n)\leq\Prb(B_n)+\Prb(C_n)$.\\
	Wegen \ref{0318satz}a) sind auch die $Y_i$ paarweise unabhängig. Sei nun $\ol{Y}_n:=\frac{1}{n}\sum_{i=1}^{n}Y_i$, dann ist
	\begin{align*}
	\Prb(B_n)&=\Prb\left(\abs{\ol{Y}_n-\Epv(\ol{Y}_n)}\geq \frac{\epsilon}{2}\right)\overset{\ref{0428intUngl}d)}{\leq}\frac{4}{\epsilon^2}\Var(\ol{Y}_n)=\frac{4}{\epsilon^2}\frac{1}{n^2}\sum_{i=1}^{n}\Var(Y_i)\\
	&\leq\frac{4}{n^2\epsilon^2}\sum_{i=1}^{n}\Epv(Y_i^2)\leq\frac{4}{n^2\epsilon^2}\sum_{i=1}^{n}\sqrt{i}\leq\frac{4}{\epsilon^2n^2}n\sqrt{n}=4\frac{\epsilon^2}{\sqrt{n}}\xrightarrow{n\to\infty}0
	\end{align*}
	Analog für $C_n$ mit Sei $\ol{Z_n}=\frac{1}{n}\sum_{i=1}^{n}Z_i$. Dann ist
	\begin{align*}
	\Prb(C_n)&=\Prb\left(\abs{\ol{Z_n}-\Epv(\ol{Z_n})}\geq\frac{\epsilon}{2}\right)\overset{\substack{\ref{0428intUngl}d)\\f(x)=\abs{x}}}{\leq}\frac{2}{\epsilon}\Epv\left(\abs{\ol{Z}_n-\Epv(\ol Z_n)}\right)=\frac{2}{n\epsilon}\Epv\left(\abs{\sum_{i=1}^{n}Z_i-\Epv(Z_i)}\right)\\
	&\leq\frac{2}{n\epsilon}\sum_{i=1}^{n}\Epv\left(\abs{Z_i-\Epv(Z_i)}\right)\leq\frac{2}{n\epsilon}\sum_{i=1}^{n}2\Epv(Z_i)
	\end{align*}
	Da aber falls $a_i\xrightarrow{i\to\infty}0$ geht auch $\frac{1}{n}\sum_{i=1}^{n}a_i\xrightarrow{in\to\infty}$. Also
	\[\Prb(C_n)\leq \frac{2}{n\epsilon}\sum_{i=1}^{n}2\Epv(Z_i)\xrightarrow{n\to\infty}0\]
	\end{proof}

	\begin{exm}[Satz von Weierstraß]
		Sei $f:[0,1]\to\R$ stetig.\\
		Zu $p\in[0,1]$ seien $(X_i)_{i\in\N}$ uiv Zufallsvariablen mit $\Prb(X_i=1)=p=1-\Prb(X_i=0)$. Dann ist $\sum_{i=1}^{n}X_i\sim\scB_{n,p}$ und
		\[f_n(p):=\Epv_p\left(f\left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)\right)=\sum_{k=0}^{n}f\left(\frac{k}{n}\right)\Prb_p\left(\sum_{i=1}^{n}X_i=k\right)=\sum_{k=0}^{n}f\left(\frac{k}{n}\right)\binom{n}{k}p^k(1-p)^{n-k}\]
		ist Polynom $n$-ten Grades in $p$ (\enquote{Bernstein-Polynome}).\\
		$f$ ist gleichmäßig stetig in $[0,1]$, also $\forall\epsilon>0\exists\delta>0$, sodass für alle $|x-y|<\delta$ immer $|f(x)-f(y)|<\epsilon$ gilt.\\
		Außerdem ist
		\[f_n(p)-f(p)=\Epv_p\left(f\left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)-f(p)\right)\]
		Also kann man abschätzen:
		\begin{align*}
		|f_n(p)-f(p)|&\leq\abs{\Epv_p\left(f\left(\frac{1}{n}\sum_{i=1}^{\infty}\right)-f(p)\right)}
		\leq \Epv_p\left(\abs{f\left(\frac{1}{n}\sum_{i=1}^{\infty}\right)-f_p}\right)\\
		&\leq \epsilon+ 2\norm{f}_\infty\Prb\left(\abs{\frac{1}{n}\sum_{i=1}^{n}X_i-p}\geq \delta\right)
		\overset{\ref{0503satzGGZL^2}}{\leq} \epsilon+2\norm{f}_\infty\frac{\Var(X_i)}{n\delta^2}
		\end{align*}
		Also ist $\limsup_{n\to\infty}\abs{f_n(p)-f(p)}\leq\epsilon$  für alle $\epsilon>0$.\\
		Also ist $\lim\limits_{n\to\infty}\abs{f_n(p)-f(p)}=0$.
	\end{exm}
	
	\begin{bem*}
		Bisher haben wir immer $\Prb(\abs{\ol S_n-m}>\epsilon)\xrightarrow{n\to\infty}0$ gezeigt (Für $\ol S_n=\frac{1}{n}\sum_{j=1}^{n}X_j$ und $m=\Epv(X_j)$). Wir schreiben nun $X_N\xrightarrow{\Prb}Y$ genau dann wenn frü alle $\epsilon>0$ $\lim\limits_{n\to\infty}\Prb(\abs{X_1-Y}>\epsilon)=0$. Man spricht von \textbf{Stochastischer Konvergenz} oder \textbf{Konvergenz in der Wahrscheinlichkeit}.\\
		Dan bedeutet, zu jedem \enquote{Zeitpunkt} $n\in\N$ hat $B_n:=\{\omega\in\Omega:\abs{\ol S_n-m}>\epsilon\}$ Maß welches mit $n\to\infty$ verschwindet.\\
		\\
		In der Praxis (z.B. Glücksspiel) verfolgen wir aber oft die ZEitliche entwocklung für \underline{ein} $\omega\in\Omega$.\\
		Es ist möglich, dass $\lim\limits_{n\to\infty}\Prb(B_n)>0$ ist aber troztdem $\limsup_{n\to\infty}\abs{\ol S_n(\omega)-m}=1$ für alle $\omega\in\Omega$:
		
	\end{bem*}

%VL 26.07.2017
	\begin{exm}[Wandernder Dorn]
		Sei $\Omega=[0,1)$ mit dem Lebesgue-Maß, Sei $X_i(\omega)=\cha_{I_i}(\omega)$. für $i=1,...,15$ mit
		\begin{align*}
		I_1&=\big[0,\frac{1}{2}\big)&&&I_1&=\big[\frac{1}{2},1\big)\\
		I_3&=\big[0,\frac{1}{4}\big)&\cdots&&I_6&=\big[\frac{3}{4},1\big)\\
		I_7&=\big[0,\frac{1}{8}\big)&\cdots&&I_{15}&=\big[\frac{7}{8},1\big)
		\end{align*}
		Dann ist 
		\[\Prb(|X_n-0|>\epsilon)=\begin{cases}
		\frac{1}{2}&n=1,2\\
		\frac{1}{4}&n=3,4,5,6\\
		\frac{1}{8}&n=7,...,15
		\end{cases}\quad\leq\frac{2}{n}\xrightarrow{n\to\infty}0\]
		Aber $\limsup_{n\to\infty}|X_n(\omega)-0|=1$ $\forall\omega\in\Omega$.\\
		Also $X_n(\omega)\not\xrightarrow{n\to\infty}0$.
	\end{exm}

	\begin{definition}\label{0507def}
		Seien $(S_n)$ Zufallsvariablen. Man sagt für $c\in\R$, dass $S_n$ \underline{konvergiert $\Prb$ fast sicher} ($S_n\xrightarrow{n\to\infty}c$), wenn \[\Prb\left(\lim\limits_{n\to\infty} S_n=c\right)=1\autotag\label{eq:0502}\]
	\end{definition}
	
	\begin{bem}\label{0508bem}
		Setze $\Omega_m:=\{\omega\in\Omega:\limsup_{n\to\infty}\abs{S-n-c}<\frac{1}{m}\}$.\\
		Da $\Omega_{m+1}\subseteq\Omega_m$, also 
		\[\Omega_\infty:=\left\{\omega\in\Omega:\limsup_{n\to\infty}S_n=c\right\}\]
		mit \ref{0122satz}e). Dann gilt
		\[\Prb\left(\limsup_{n\to\infty} S_n=c\right)=1\Leftrightarrow \forall\epsilon>=:\Prb\left(\limsup_{n\to\infty}\abs{S_n-c}<\epsilon\right)=1\tag{\star\star\star}\label{0507defEq}\]\\
		\\
		Daher besteht der Unterschied zwischen \ref{eq:0501} und \ref{eq:0502} in der Platzierung der Grenzwerts: $\forall\epsilon,c>0$
		\begin{itemize}
			\item \ref{eq:0501} besagt $\lim\limits_{n\to\infty}\Prb\left(\abs{S_n-c}<\epsilon\right)=1$
			\item \ref{eq:0502} besagt 
			$\Prb\left(\limsup_{n\to\infty}\abs{S_n-c}<\epsilon\right)=1$
		\end{itemize}
		Außerdem gilt das Lemma von Fatou \ref{0415fatou}: $\Epv\left(\liminf_{n\to\infty} X_n\right)\leq\liminf_{n\to\infty}\Epv(X_n)$, falls $X_n\geq 0$.\\
		Setze nun $X_n=\cha_{\{\abs{S_n-c}<\epsilon\}}$. Dann ist
		\begin{align*}
		\liminf_{n\to\infty}(X_n\omega)=1&\Leftrightarrow \exists m\in\N:\forall n>m\cha_{\{\abs{S_n-c}<\epsilon\}}(\omega)=1\\
		&\Leftrightarrow\abs{S_n-c}>\epsilon\text{ unendlich oft}\Leftrightarrow\limsup_{n\to\infty}\abs{S_n-c}<\epsilon
		\end{align*}
		Dann folgt (Fatou)
		\[\liminf_{n\to\infty}\Prb(\abs{S_n-c}<\epsilon)=\liminf_{n\to\infty}\Epv(X_n)\geq \Epv\left(\liminf_{n\to\infty}X_n\right)=\Epv\left(\limsup_{n\to\infty}\abs{S_n-c}<\epsilon\right)\]
		Daher folgt aus $\Prb\left(\lim\limits_{n\to\infty}\abs{S_n-c}=0\right)$ mit \ref{0507defEq}, dass $\forall\epsilon>0.\Prb\left(\limsup_{n\to\infty}\abs{S_n-c}<\epsilon\right)$.
		Dann folgt mit Fatou: $\forall\epsilon>0\lim\limits_{n\to\infty}\Prb\left(\abs{ S_n-c}<\epsilon\right)=1$.\\
		Also ist \ref{eq:0502} stärker als \ref{eq:0502}.
	\end{bem}
	
	\begin{satz}[Starkes Gesetz der großen Zahlen, $\scL^4$-Version]\label{0509satzGGZL^4}
		Seien $X_n$ Zufallsvariablen mit $\Epv(|X_n|^4)<\infty$, dann ist
		\[\Prb\left(\lim\limits_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}X_i=\Epv(X_1)\right)=1\]
	\end{satz}
	\begin{proof}
		Wegen $\frac{1}{n}\sum_{i=1}^{n}X_i-\Epv(X_1)=\frac{1}{n}\sum_{i=1}^{n}\left(X_i-\Epv(X_i)\right)$ kann man $\Epv(X_1)=0$ annehmen.
		\begin{enumerate}[label=(\roman*)]
			\item Sei 
			\[A_n=\left\{\omega\in\Omega\mid \abs{\frac{1}{n}\sum_{i=1}^{n}X_i(\omega)}\geq n^{-\frac{1}{8}}\right\}\]
			Es gilt
			\begin{align*}
			\Prb(A_n)&=\Prb\left(\abs{\frac{1}{n}\sum_{i=1}^{n}X_i}\geq n^{-\frac{1}{8}}\right)\\
			\substack{\ref{0428intUngl}\\\text{\enquote{Chebychev $f(x)=x^4$}}}&\leq\frac{\Epv\big(\left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)^4\big)}{\left(n^{-\frac{1}{8}}\right)^{4}}=n^{\frac{1}{2}}n^{-4}\Epv\left(\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{k=1}^{n}\sum_{l=1}^{n}X_iX_jX_kX_l\right)\\
			&=n^{\frac{1}{2}-4}\sum_{i,j,k,l=1}^{n}\Epv\big(X_iX_jX_kX_l\big)
			\intertext{$\Epv(...)\neq 0$, nur wenn je zwei oder alle Indizes gleich sind.}
			&=n^{\frac{1}{2}-4}\left(\sum_{i=1}^{n}\Epv(X_i^4)+\sum_{i=1}^{n}\sum_{\substack{i=1\\i\neq j}}^{n}\Epv(X_i^2X_j^2)\right)\\
			&=n^{\frac{1}{2}-4}\left(n\Epv(X_1^4)+n(n-1)\Epv(X_1^2)^2\right)\leq c n^{-\frac{3}{2}}
			\end{align*}
			Also ist $\sum_{i=1}^{\infty}\Prb(A_n)\leq \infty$.
			\item Dann folgt mit dem Lemma von Borel-Cantelli (\ref{0335satzBorCant}):
			\begin{align*}
			0=\Prb\left(\limsup_{n\to\infty} A_n\right)&=\Prb\left(\abs{\frac{1}{n}\sum_{i=1}^nX_i}\geq n^{-\frac{1}{8}}\text{ unendlich oft}\right)\\
			&=1-\underbrace{\Prb\left(\abs{\frac{1}{n}\sum_{i=1}^nX_i}\geq n^{-\frac{1}{8}}\text{ nur endlich oft oft}\right)}_{\geq\Prb\left(\lim\limits_{n\to\infty}\abs{\frac{1}{n}\sum_{i=1}^{n}X_i}=0\right)}\\
			&\geq1-\Prb\left(\lim\limits_{n\to\infty}\abs{\frac{1}{n}\sum_{i=1}^{n}X_i}=0\right)
			\end{align*}
			Dann ist also $\Prb\left(\lim\liminf_{n\to\infty}\abs{\frac{1}{n}\sum_{i=1}^{n}X_i}=0\right)\geq 1$, also $=1$.
		\end{enumerate}
	\end{proof}

	\begin{bem}
		Die aussage von \ref{0509satzGGZL^4} gilt auch falls $X\in\scL^1$.
	\end{bem}

	\begin{bem}
		Rückblick auf \ref{0503satzGGZL^2}:\\
		Falls $X_i$ uiv, dann ist
		\[\Prb\left(\abs{\frac{1}{n}\sum_{i=1}^{n}X_i-\Epv(X_1)}\geq \epsilon_n\right)\leq\frac{V}{n\epsilon_n^2}\]
		Also gilt $\Prb(...)\xrightarrow{n\to\infty}0$, falls $\epsilon_n\gg\frac{1}{\sqrt{n}}$ (Also $\limsup_{n\to\infty}\sqrt{n}\epsilon_n=\infty$).\\
		Das Bedeutet1: Der Abstand zwischen $\frac{1}{n}\sum_{i=1}^{n}X_i$ und $\Epv(X_1)$ schrumpft \enquote{fast} wie $\frac{1}{\sqrt{n}}$.\\
		\\
		Frage: Was ist $\Prb\left(\abs{\frac{1}{n}\sum_{i=1}^{n}\big(X_i-\Epv(X_i)\big)}\geq\frac{1}{\sqrt{n}}\right)$?\\
		Für $\al>\frac{1}{2}$ gilt
		\[\Var\left(n^{\al-1}\sum_{i=1}^{n}\big(X_i-\Epv(X_i)\big)\right)=n^{2\al-2}\sum_{i=1}^{n}\Var(X_1)\]
		Sei $v=n^{2\al-1}$, dann $v\xrightarrow{n\to\infty}$, falls $\al>\frac{1}{2}$ und $n>0$.
		Also ist zu erwarten, dass für $\al>\frac{1}{2}$ gilt
		\[\Prb\left(\abs{\frac{1}{n}\sum_{i=1}^{n}X_i-\Epv(X_i)}\leq n^{-\al}\right)\not\to 0\]
		Tatsächlich gilt
	\end{bem}

	\begin{satz}[zentraler Grenzwertsatz]\label{0512satzZentrGrenzw}
		Sei $X_n$ eine Folge von unabhängigen, identisch verteilten Zufallsvariablen, $\Epv(X_i)=m$, $\Var(X_i)=v\leq\infty$ für alle $i\in\N$.\\
		Dann gilt
		\[\lim\limits_{n\to\infty}\Prb\left(\frac{1}{\sqrt{n}}\sum_{i=1}^{n}(X_i-m)\leq c\right)=\int_{-\infty}^{c}\frac{1}{\sqrt{2\pi v}}e^{-\frac{x^2}{2v}}~dx\]
	\end{satz}

	\begin{bem}
		Die Gleichung aus \ref{0512satzZentrGrenzw} bedeutet, dass die Folge der Bildmaße $\Prb_{\ol S_n(\omega)}$ von $\Prb$ unter $\omega\mapsto\ol{S_n(\omega)}:=\frac{1}{\sqrt{n}}\sum_{i=1}^{n}(X_i(\omega)-m)$
		konvergiert gegen $\Nv_{0,v}$.
		
%VL 27.06.2017
		Im Sinne der Maßkonvergenz
		\[\Prb_{\ol S_n}\big((-\infty,c]\big)\xrightarrow{n\to\infty}\Prb_{\Nv_{0,v}}\big((-\infty,c]\big)\forall c\]
	\end{bem}

	\begin{definition}
		Seine Folge $\mu_n$ von endlichen Maßen auf $\R$ heißt \textbf{schwach konvergent} gegen ein Maß $\mu$, falls $\forall f\in C_0(\R)$
		\[\lim\limits_{n\to\infty}\Epv_{\mu_n}(f)=\Epv_{\mu}(f)\quad\text{bzw.}\quad\lim\limits_{n\to\infty}\int f~d\mu_n=\int f~d\mu\]
	\end{definition}
	
	\begin{bem*}
		In \ref{0512satzZentrGrenzw} verwenden wir $f(x)=\cha_{(-\infty,c]}(x)$. Da $f$ unstetig ist, gilt z.b. mit $\mu_n=\delta_{\frac{1}{n}}$, $g=\cha_{(-\infty,0]}$, $\mu=\delta_0$. Dann ist
		\[\int f~d\mu_n=f\left(\frac{1}{n}\right)\xrightarrow{n\to\infty}f(0)\]
		falls $f$ stetig differenzierbar bei $0$, aber $\int g~d\mu_n=0$, $\int g~d\mu=1$.
	\end{bem*}

	\begin{prop}\label{0515prop}
		Seien $\mu_n$ Wahrscheinlichkeitsmaße auf $\R$, $\mu$ Wahrscheinlichkeitsmaß auf $\R$ und $\mu$ habe Lebesgue-Dichte $\rho$ und sei für alle $k\in\N$
		\[\lim\limits_{n\to\infty}\Epv_{\mu_n}(f)=\Epv_{\Prb_\mu}(f)\forall f\in C_b^k\]
		Dann ist
		\[\lim\limits_{n\to\infty}\Prb_{\mu_n}\big((-\infty,c]\big)=\Prb_\mu\big((-\infty,c]\big)\forall c\in\R\]
	\end{prop}
	\begin{proof}
		\begin{enumerate}[label=(\roman*)]
			\item Zu zeigen: $\forall x\in\R\forall\epsilon>0\exists\delta>0$
			\[\limsup_{n\to\infty}\Prb_{\mu_n}\big([x-\delta,x+\delta]\big)<\epsilon\]
			Wähle dazu $f\in C_b^k$ mit $0\leq f\leq 1$, $f(y)=1\forall y\in[x-\delta,x+\delta]$ und $f(y)=0\forall y\notin[x-2\delta,x+2\delta]$.\\
			Dann gilt
			\begin{align*}
			\limsup_{n\to\infty} \Prb_{\mu_n}\big([x-\delta,x+\delta]\big)&\leq \limsup_{n\to\infty} \Epv_{\mu_n}(f)=\Epv_{\Prb_X}\mu(f)\\
			&=\int_{x-2\delta}^{x+2\delta}f(y)\rho(y)~dy&\leq \int_{x-2\delta}^{x+2\delta}\rho(y)~dy\xrightarrow{\delta\to 0}0
			\end{align*}
			mit \ref{0411majkonv}, da $\rho\in\scL^1$.
			\item Zu $\epsilon>0$, $c\in\R$ wähle $f(x)=1\forall x\leq c$.\\
			Dann ist
			
			\[\Prb_{\mu_n}\big((-\infty,c]\big)-\Prb_\mu\big((-\infty,c]\big)=\Epv_{\mu_n}\left(\cha_{(-\infty,c]-f}\right)\tag{\star}+\Epv_{\mu_n}(f)-\Epv_{\mu}(f)-\Epv_\mu\underbrace{\left(\cha_{(-\infty,c]}-f\right)}_{|\cdot|\leq\cha_{(-\infty,c]}}\]
			Mit (i) folgt, $\delta$ existiert, sodass $|(\star)|\leq 2\epsilon+\abs{\Epv_{\mu_n}(f)-\Epv_{\Prb_X}\mu(f)}$.
			Also ist $\limsup_{n\to\infty}\abs{(\star)}\leq 2\epsilon\forall\epsilon>0$.
		\end{enumerate}
	\end{proof}

	\begin{proof}[Beweis von \ref{0512satzZentrGrenzw}]
		Setze $\tilde X_i=\frac{1}{\sqrt{v}}\left(X_i-m\right)$. Dann ist $\Epv(\tilde X_i)=0$ und $\Var(\tilde X_i)=1$ und
		\[\Prb\left(\frac{1}{\sqrt{n}}\sum_{i=1}^{n}(X_i-m)\geq c\right)=\Prb\left(\frac{1}{\sqrt{n}}\sum_{i=1}^{n}\tilde X_i\geq c\sqrt{v}\right)\]
		Also kann oBdA angenommen werden, dass bereits $\Epv(X-i)=0$ und $\Var(X_i)=1$ gilt.\\
		Sei nun $\ol{S}_n=\frac{1}{\sqrt{n}}\sum_{i=1}^{n}X_i$.\\
		Mit \ref{0515prop} folgt, dass es bereits ausreicht, dass zu prüfen, ob (mit $k=3$) $\lim\limits_{n\to\infty}\Epv\big(f(\ol{S}_n)\big)=\Epv_{\Nv_{0,1}}(f)$ für alle $f\in C^3_b(\R)$ gilt.\\
		\\
		Sei dazu $(Y_n)$ eine weiter Folge von unabhängig identisch verteilen Zufallsvariablen mit
		\begin{enumerate}
			\item $(Y_n)$ unabhängig von $(X_n)$
			\item $(Y_n\sim\Nv_{0,1})\forall n$
		\end{enumerate}
		Dann gilt für $\ol T_n:=\frac{1}{\sqrt{n}}\sum_{i=1}^{n}Y_i\sim\Nv_{0,1}$, dass
		\[\Epv\big(f(\ol T_n)\big)=\Epv_{\Nv_{0,1}}(f)\]
		Also gilt es zu Ziegen, dass
		\[\forall f\in\C^3_b\quad\Epv\big(f(\ol S_n)-f(\ol T_n)\big)\xrightarrow{n\to\infty}0\]
		Dabei gilt
		\begin{align*}
		&-f\big(\ol S_n(\omega)\big)-f\big(\ol T_n\big)\\
		=&\sum_{i=1}^{n}\left[f\left (\frac{1}{\sqrt{n}}\left(\sum_{j=1}^{i}X_j(\omega)+\sum_{j=i+1}^{n}Y_j(\omega)\right)\right)
		-f\left(\frac{1}{\sqrt{n}}\left(\sum_{j=1}^{i-1}X_j(\omega)+\sum_{j=i}^{Y_j(\omega)}\right)\right)\right]\\
		=&\sum_{i=1}^{n}f %TODO Ende VL 27.06.2017
		\end{align*}
	\end{proof}

%VL 03.07.2017
\section{Grundzüge der Statistik}
	\subsection{Fragestellung und Modellbildung}
	\begin{bem}
		\begin{enumerate}
			\item \underline{Ausgangspunkt:} Daten $x=(x_1,...,x_n)\in\R$ oder allgemein $x\in\Omega$. Diese Daten sind fest vorgegeben, nicht zufällig und nicht verhandelbar (z.B. Messwerte).
			\item \underline{Modellannahme:} $x_1,...,x_n$ sind das Ergebnis eines (unbekannten) zufälligen Geschehens. Also gibt es einen Wahrscheinlichkeitsraum $(\Omega,\scF,\Prb)$ mit $x\in\Omega$, sodass $x$ das Ergebnis von \enquote{einmal würfeln mit $\Prb$} ist.
			\item \underline{Ziel:} Finde heraus welches $\Prb$ (äquivalent: welche Zufallsvariable $X$) die Daten $x$ erzeugt haben könnte.
			\item \underline{Häufige zusätzliche Annahme:} Es kommt in b) nur eine Familie $(P_\theta)_{\theta\in\Theta}$ mit $\Prb_\theta=\Nv(\mu,\sigma^2)$ in Frage.\\
			$\theta$ heißt Parameter. Daher spricht man von \enquote{parametrische Statistik}.
		\end{enumerate}
	\end{bem}
	\begin{exm}[Feuerwerk]\label{0602exm}
		Großpackung $10^6$ Raketen. Davon werden 100 getestet und 5 davon sind defekt.\\
		Wie viele der $10^6$ Raketen sind voraussichtlich Defekt (Ausschussquote)?
		\begin{enumerate}
			\item Daten: $x=5$
			\item Modell (Approximativ) ziehen mit zurücklegen (eigentlich: ohne) mit Erfolgswahrscheinlichkeit $\theta=\frac{|\{\text{defekte}\}|}{10^6}$.
			\stepcounter{enumi}
			\item $\Theta=[0,1]$, $\Prb_\theta=\scB_{N,\theta}$ binomialverteilt $N=100$, Erfolgswahrscheinlichkeit $\theta$.
			\addtocounter{enumi}{-2}
			\item Welches $\theta$ hat die Daten erzeugt.
		\end{enumerate}
	\end{exm}
	\begin{exm}[\enquote{\ref{0602exm}} abstrahiert]
		Ein Ratespiel. Der Spielleiter hat für jedes $\theta\in\Theta$ einen Würfel mit der Verteilung $\Prb_\theta$. Er nimmt einen davon, würfelt einmal und sagt das Ergebnis.\\
		Nun muss man möglichst gut Raten welcher Würfel genommen wurde.
	\end{exm}

	\begin{definition}
		Ein \textbf{statistisches Modell} (sM) ist ein Tripel $(\Omega,\scF,(\Prb_\theta)_{\theta\in\Theta})$, wobei $(\Omega,\scF)$ ein Messraum ist und $\Theta$ die sog. \enquote{\textbf{Parametermenge}} und $\Prb_\theta$ ein Wahrscheinlichkeitsmaß auf $(\Omega,\scF)$ für alle $\theta\in\Theta$ ist.
	\end{definition}

	\begin{definition}
		Ein statistisches Modell heißt \textbf{Produktmodell}, falls $n\in\N$ existiert, sodass für $(\tilde{\Omega},\tilde{\scF})$ Messraum, $\tilde{\Prb_\theta}$ Familie von Wahrscheinlichkeitsmaßen auf $\tilde\Omega$
		\[\big(\Omega,\scF,(\Prb_\theta)_{\theta\in\Theta}\big)=\big(\tilde{\Omega}^n,\tilde{\scF}^n,\big(\tilde{\Prb}_\theta^{\otimes n}\big)_{\theta\in\Theta}\big)\]
		ein $n$-faches unabhängiges Würfeln Modelliert.
	\end{definition}

	\begin{exm}
		In Beispiel \ref{0602exm} alternativ:
		\[\Omega=\{0,1\}^{100},\quad\scF=\Potset(\{0,1\})^{\otimes 100},\quad\Prb_{\theta}=\big(\Ber_{\{0,1\},\theta}\big)\]
		Typische Daten: $x=(0,0,1,0,...,1)$. Interssant ist die Anzahl der Einsen.
	\end{exm}

	\begin{definition}\label{0607def}
		$\big(\Omega,\scF,(\Prb_\theta)_{\theta\in\Theta}\big)$ sei statistisches Modell. Eine messbare Abbildung $T:\Omega\to\Theta$ heißt \textbf{Schätzer} für den Parameter $\theta\in\Theta$.\\
		Eine messbare Abbildung $\Omega\to\Omega'$ heißt $\Omega'$-wertiger Schätzer.\\
		Bedeutung: Bei Daten $x\in\Omega$ \enquote{schätzt} man dass $\theta=T(x)$. Ein $\Omega'$-wertiger Schätzer ist sinnvoll, falls z.B. $\Theta=\R\times \R^+$, $\theta=(\mu,\sigma^2)$, $\Prb_\theta=\Nv(\mu,\sigma^2)$ man aber nur $\mu$ wissen will. Dann ist $\Omega'=\R$.
	\end{definition}

	\begin{bem*}
		\ref{0607def} verlangt nicht,dass der geschätzte Wert $T(x)$ etwas mit dem wahren $\theta$ zu tun hat. Im falle eines Produktmodells hat man aber folgendes Gütekriterium:
	\end{bem*}

	\begin{definition}\label{0608def}
		Zu jedem $n\in\N$ sei $(\Omega^n,\scF^{\otimes n},\left(\Prb_{(X,Y)}\theta\right)^{\otimes n}_{\theta\in\Theta})$ ein Produktmodell $T_N:\Omega\to\Omega'$ ein Schätzer, $f:\Theta\to\Omega'$ eine Funktion: $(T_n)_{n\in\N}$ heißt \textbf{schwach konsistente Familie von Schätzern}, falls
		\[\forall\theta\in\Theta,\forall\epsilon>0:\lim\limits_{n\to\infty}\Prb_{\theta^{\otimes n}}\left(\abs{T_n-f(\theta)}>\epsilon\right)=0\]
	\end{definition}

	\begin{bem}
		\ref{0608def} bedeutet nicht, dass für gegebene Daten $x$ und $\epsilon>0$ und großem $n$ der Wert $T(x)$ mit hoher Wahrscheinlichkeit höchstesn Abstand $\epsilon$ von $f(\theta)$ hat.\\
		Denn $x$ und $\theta$ sind fest (obwohl $\theta$ unbekannt ist). Hier kommt kein Zufall in Spiel.\\
		Stattdessen: Egal welches $\theta$ das richtige ist liegt $T(x)$ nur dann weit von $f(\theta)$ weg (für große $n$) wenn wir beim Erzeugen der Daten \enquote{Pech hatten}.\\
		Mit anderen Worten: Würden wir die Messung häufig wiederholen und Datensätze $x^{(1)},...,x^{(m)}$ erzeugen, so wäre für die meisten dieser Datensätze die Ungleichung $|T(x^{(k)})-f(\theta)|\leq\epsilon$ wahr.
	\end{bem}

	\begin{exm}
		In \ref{0602exm} wählt man $T(x)=\frac{x}{100}$ bzw in der Form \ref{0606exm}: $T(x)=\frac{1}{100}\sum_{i=1}^{100}x_i$.\\
		Speziell für $x=5$ ist $T(x)=\frac{1}{20}$, d.h. man schätzt $5\%$ Ausschuss.
	\end{exm}

	\begin{definition}
		Sei $(\Omega^n,\scF^{\otimes n},(\Prb_\theta)_\theta^{\otimes n})$ für alle $n$ ein Produktmodell. Sei $\Prb_\theta$  die Verteilung einer reelle Zufallsvariable $X$ mit $\Epv(|X|)<\infty$ und $\Epv(X)=\theta$. Dann heißst
		$T_n:\Omega^n\to\R, x\mapsto\frac{1}{n}\sum_{i=1}^{n}x_i$ heißt \textbf{Mittelwertschätzer}.
	\end{definition}
	%COUNTER
	\addtocounter{theorem}{-1}
	\begin{satz}
		Der Mittelwertschätzer ist schwach konsistent
	\end{satz}
	\begin{proof}
		Durch GGZ.
	\end{proof}

	\begin{definition}
		Sei $T$ Schätzer für eine Größe $f(\theta)$ dann \begin{enumerate}
			\item ist der sog \textbf{Bias} definiert als
			\[\Bias_\theta:=\Epv_\theta(T)-f(\theta)\]
			\item heißt $T$ \textbf{Erwartungstreu}, falls  $\Bias_\theta=0$.
			\item der sog \textbf{mean square error} definiert als
			\[\MSE_\theta(T):=\Epv_{\Prb_\theta^{\otimes n}}\left(\big(T-\Epv_{\Prb_\theta^{\otimes n}}(T)\big)^2\right)=\Var_{\Prb_\theta^{\otimes n}}(T)\]
			\item heißt $T$ \textbf{konsistent im quadratischen Mittel} (im Produktmodell), falls 
			$\MSE_\theta\xrightarrow{n\to\infty}0$.
		\end{enumerate}
	\end{definition}
	
	\begin{lem}
		Sei $T$ erwartungstreu und $\Prb_{(X,Y)}\theta^{\otimes n}(T)$ konsistent im quadratischen Mittel, dann ist $T$ schwach konsistent.
	\end{lem}
	
	\begin{definition}
		Sei $(\Omega^n,\scF^{\otimes n},(\Prb_\theta)_\theta^{\otimes n})$ für alle $n$ ein Produktmodell, sei $\Prb_{(X,Y)}\theta$ die Verteilung einer rellen Zufallsvariable $X$ mit $\Epv(|X|)<\infty$, $\Var_\theta(X)=\theta$.\\
		Dann ist der \textbf{Varianzschätzer} gegeben durch
		\[x\mapsto\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\frac{1}{n}\sum_{i=1}^{n}x_j)^2\tag{\enquote{Empirische Varianz}}\]
	\end{definition}
	\addtocounter{theorem}{-1}
	\begin{kor}
		$T$ ist erwartungstreu
	\end{kor}
	\addtocounter{theorem}{-1}
	\begin{kor}
		Wenn $(X_i)$ Gaußverteilt ist, dann ist $T$ konsistent im quadratischen Mittel.
	\end{kor}

%VL 04.05.2017
	\begin{bem*}
		In der Theorie der Statistik untersucht man \enquote{optimale} Schätzer (z.B. mit minimalem MSE). Sie zu finden ist nicht immer einfach.
	\end{bem*}

	\begin{exm}
		Sei $\Omega=[0,\infty)$, $\Prb_{(X,Y)}\theta=\frac{1}{\theta}\cha_{[0,\theta]}\cdot\la$. (Gleichverteilung auf $[0,\theta]$).\\
		Modell: $(\Omega^n,\scF^{\otimes n},\Prb_\theta^{\otimes n})$. Schätze $\theta$ aus Daten $x_1,...,x_n$.\\
		Mögliche Sätzer
		\begin{enumerate}
			\item $T_1(x)=2\frac{1}{n}\sum_{i=1}^{n}x_i$
			\item $T_2(x)=\frac{n+1}{n}\max\{x_1,...,x_n\}$
			\item $T_3(x)=\max(x_1,...,x_n)+\min\{x_1,...,x_n\}$
		\end{enumerate}
	\end{exm}

	\subsection{Maximum-Likelihood-Schätzer}
	\begin{definition}
		\begin{enumerate}
			\item Auf diskreten Wahrscheinlichkeitsräumen: Sei $(\Omega,\scF,(\Prb_\theta)_{\theta\in\Theta})$ statistisches Modell, $\Omega$ abzählbar, $\rho_{\theta}(x):=\Prb(\{x\})$ sei Zähldichte von $\Prb_\theta$.
			\begin{description}
				\item[Simpled Idee] \begin{enumerate}
					\item Zu den Daten $x$ Berechne $\Prb_\theta(\{x\})$
					\item Rak-Vorschrift: das \enquote{richtige} $\theta$ ist jenes für das $\Prb_\theta(\{x\})$ maximal ist.
				\end{enumerate}
				\item[Formal] Zu $x\in\Omega$ heißt die Abbildung $\theta\mapsto\rho_\theta(x)$ Likelyhood- (oder Plausibilitäts-)Funktion zum Beobachtungswert $x$.\\
				Eine Abbildung $\Omega\to\Theta,x\mapsto\arg\max\rho_x$ heißt \textbf{maximum-Likelyhood Schätzert}.
			\end{description}
		\item Stetige Wahrscheinlichkeitsräume mit Dichte: Sei $(\Omega,\scF,(\Prb_\theta)_{\theta\in\Theta})$ statistisches Modell, $\Omega=\R^d$, $\rho_\theta$ Dichtefunktion zu $\Prb_\theta$.\\
		Dann heißt die Abbildung $\Omega\to\Theta, x\mapsto\arg\max\rho_\theta$ \textbf{ML-Schätzer}.
		\end{enumerate}
	\end{definition}

	\begin{bem}
		Berechnen von ML Schätzern ist meist durch Ableiten von $\theta\mapsto\rho_\theta(x)$ und Nullsetzen möglich. Da Schätzer häufig Produktstruktur haben ist es oft einfacher $\theta\mapsto\ln\rho_\theta(x)$.
	\end{bem}

	\begin{definition}
		Sei $(\Omega,\scF,(\Prb_\theta)_{\theta\in\Theta})$ statistisches Modell, $f:\Theta\to\Omega'$ messbar, $\al>0$.\\
		Eine Abbildung $K:\Omega\to\Potset(\Omega')$ heißt \textbf{Konfidenzbereich für $f$} zum \textbf{Irrtumsniveau} $\al$ (\textbf{Sicherheitsniveau} $1-\al$), wenn gilt
		\begin{enumerate}[label=(\roman*)]
			\item $\{\omega\in\Omega:f(\theta)\in K(\omega)\}\in\scF$
			\item $\Prb_\theta\big(f(\theta\in K(\cdot))\big)\geq 1-\al$
		\end{enumerate}
	\end{definition}

	\begin{bem}
		es wird zu jedem Datensatz $x$ eine Menge $K(x)\subset\Omega'$ bestimmt, sodass $f(\theta)$ nur mit sehr geringer Wahrscheinlichkeit (beim Erzeugern der Daten) nicht in $K(x)$ liegt, falls $\theta$ der wahre Wert war.\\
		Genauer: Macht man das Experiment
		\begin{enumerate}
			\item Würfele $x$ mit $\Prb_\theta$ aus
			\item rate, dass $\theta\in K(x)$
			\item Schaue nach ob richtig geraten wurde
		\end{enumerate}
	Häufiges unabhängiges durchführen: Man hat in mehr als $(1-\al)N$ Fällen recht.
	\end{bem}

	\begin{definition}
		Sei (vgl \ref{0618def}) $(\Omega,\scF,(\Prb_\theta)_{\theta\in\Theta})$ statistisches Modell, $f:\Theta\to\Omega'$ messbar, $\Omega'=\R$ und $K:\Omega\to\Potset(\Omega')$. Dann heißt die Abbildung $x\mapsto K(x)$ \textbf{Konfidenzintervall}.
	\end{definition}

	\begin{satz}[Berechnung von Konfidenzintervallen]
		Sei $(\Omega,\scF,(\Prb_\theta)_{\theta\in\Theta})$ statistisches Modell, $\Omega'=\R$, $f:\Theta\mapsto\R$ Zielgröße, $x\in\Omega$ Daten.\\
		Gesucht: $KI:[R^-,R^+]$.
		\begin{description}
			\item[Einzige Möglichkeit] Sage $R_-,R_+$ aus den Daten vorher. Gesucht sind Schätzer $x\mapsto R_-(x),x\mapsto R_+(x)$ für die Intervallenden.
			\item[Benötigte Eigenschaft]
			\[\forall\theta:\Prb_\theta\big(f(\theta)\in[R_-,R_+)\big)\geq 1-\al\Leftrightarrow\Prb_\theta\big(R_->f(\theta)\big)\text{ oder }\Prb_\theta\big(R_+<f(\theta)\big)<\al\]
			\item[Hinreichend] (und in der Praxis auch immer Benutzt)
			\[\Prb_\theta\big(R_->f(\theta)\big)\leq\al/2\quad\text{ und }\quad \Prb_\theta\big(R_+<f(\theta)\big)\leq\al/2\]
			Finde also $\forall\theta\in\Theta$  eine Funktion $\omega\mapsto R_{-,\theta}(\omega)$, sodass 
			\[\Prb_\theta\big(R_{-,\theta}>f(\theta)\big)\leq \al/2\Leftrightarrow\Prb_\theta\circ R^{-1}_{-,\theta}\big([f(\theta,\infty))\big)\leq \al/2\]
			Da man $\theta$ nicht kennt muss man annehmen $R_-(x)=\inf\{R_{-,\theta(x)}|\theta\in\Theta\}$ und $R_+(x)=\sup\{R_{+,\theta(x)}|\theta\in\Theta\}$.
		\end{description}
	\end{satz}

	\begin{exm}\label{0622exm}
		Sei $\Prb_\theta=\Nv_{\theta,\sigma^2}$, $\sigma^2$ bekannt und fest.\\
		Finde $f(\theta)=\theta$, d.h. schätze den Erwartungswert.\\
		$(\Omega^n,\scF^{\otimes n},(\Prb^{\otimes n}_\theta)_{\theta\in\Theta})$
		Wähle dann
		\[R_-(x)=\frac{1}{n}\sum_{i=1}^{n}x_i-s\quad R_+=\frac{1}{n}\sum_{i=1}^{n}x_i+s\]
		Dann ist , da$X_i\sim\v_{\theta,\sigma^2}$, $Y_i\sim\Nv_{0,\sigma}$, $Z\sim\Nv_{0,\sigma^2}$, $W\sim\Nv_{0,1}$.
		\begin{align*}
		\Prb^{\otimes n}(R_-\geq \theta)&=\Prb\left(\frac{1}{n}\sum_{i=1}^{n}(X_i-\theta)\geq s\right)\\
		&=\Prb\left(\frac{1}{\sqrt n}\left(\frac{1}{\sqrt{n}}\sum_{i=1}^{n}Y_i\right)\geq s\right)\\
		&=\Prb\left(\frac{1}{\sqrt{n}}Z\geq s\right)\\
		&=\Prb(Z\geq s\sqrt{n})=\Prb\left(W\geq \frac{s}{\sigma}\sqrt{ n}\right)
		\end{align*}
		Also muss $s$ (mindestens) als Lösung der Gleichung
		\[\int_{\frac{s}{\sigma}\sqrt{n}}^\infty\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}~dx=\al/2\]
		wählen.
	\end{exm}
	\begin{definition}\label{0623def}
		Sei $\Prb$ Wahrscheinlichkeitsmaß auf $\R$, $0<\al<1$.\\
		Eine Zahl $q_a\in\R$ heißt \textbf{$\al$-quantil} von $\Prb$, falls $\Prb\left((-\infty,q_\al)\right)\leq \al$ und $\Prb\left((-\infty,q_\al]\right)\geq\al$.
	\end{definition}

	\begin{lem}\label{0624lem}
		In \ref{0623def} sei $F$ die Verteilungsfunktion von $\Prb$. Dann gilt
		\begin{enumerate}
			\item $\lim\limits_{x\uparrow q_\al}\leq \al$ und $F(q_\al)\geq\al$ für jedes $\al$-Quantil $q_\al$.
			\item Jede zahl im Intervall $\left(\sup\left\{y\in\R:F(y)\leq \al\right\},\inf\left\{y\in\R:F(y)\geq\al\right\}\right)$ ist ein $\al$-Quantil
			\item Falls $F$ stetig und streng monoton ist, dann ist $q_\al$ die eindeutige Zahl mit $F(q\al)=\al$.
		\end{enumerate}
	\end{lem}
	
%VL 10.07.2017
	\begin{prop}\label{0625prop}
		Sei $\big(\Omega^n,\scF^{\otimes n},(\Prb_\theta^{\otimes n})_{\theta\in\R}\big)$ statistisches Modell mit $\Prb_\theta=\Nv(\theta,\sigma^2)$, wobei $\sigma^2$ bekannt und fest. Sei $q_{1-\frac{\al}{2}}$ das $1-\frac{\al}{2}$-Quantil von $\Nv(0,1)$. Dann ist die Abbildung
		\[K:\Omega^n\to\Potset(\R):x\mapsto
		\left[M(x)-\frac{\sigma}{\sqrt{n}}q_{1-\frac{\al}{2}},M(x)+\frac{\sigma}{\sqrt{n}}q_{1-\frac{\al}{2}}\right]
		=\left[M(x)+\frac{\sigma}{\sqrt{n}}q_{\al/2},M(x)+\frac{\sigma}{\sqrt{n}}q_{1-\al/2}\right]\]
		(Dabei ist $M(x):=\frac{1}{n}\sum_{i=1}^{n}x_i$ der Mittelwert) ein Konfindenzintervall zum Irrtumniveau $\al$.
	\end{prop}
	\begin{proof}
		Wähle $s=\frac{\sigma}{\sqrt n}q_{1-\al}$ in\ref{0622exm}
	\end{proof}

	\begin{exm}
		Sei $\Theta=\R\times\R^+$, $\theta=(\mu,\sigma^2)$, $\Prb_\theta=\Nv(\mu,\sigma^2)$ und $\big(\Omega^n,\scF^{\otimes n},(\Prb_\theta^{\otimes n})$ statistisches Modell.\\
		Gesucht wird nun das Konfidezintervall für die Varianz $\sigma^2$.\\
		Der einzige vernünftige Schätzer ist dann
		\[S^2:\Omega^n\to\R^+,\,S^2(x)=\frac{1}{n-1}\sum_{i=1}^{n}\big(x_i-M(x)\big)^2\]
		Für $X_i\sim\Nv(\mu,\sigma^2)$ ist dann
		\begin{align*}
		S^2\big((X_1,...,X_n)\big)&=\frac{1}{n}\sum_{i=1}^{n}\left[(X_i-\mu)-\frac{1}{n}\sum_{j=1}^{n}(X_j-\mu)\right]^2\\
		&=\frac{\sigma^2}{n-1}\sum_{i=1}^{n}\left[\frac{X_1-\mu}{\sigma}-\frac{1}{n}\sum_{j=1}^{n}\frac{X_i-\mu}{\sigma}\right]^2
		\intertext{Dabei ist $\frac{X_i-\mu}{\sigma}\sim\Nv(0,1)$, also mit $Y_1,...,Y_n\sim\Nv(0,1)$ uiv}
		&\sim\sigma^2S^2\big((Y_1,...,Y_n)\big)
		\end{align*}
		Da Bildmaß von $\frac{S^2(x)}{\sigma^2}$ ist unabhängig von $\theta$! Also folgt
		\begin{align*}
		\Prb_\theta(S^2>a\sigma^2)&=\Prb_\theta\circ\left(\frac{S^2}{\sigma^2}\right)^{-1}\big((a,\infty]\big)=:\nu\big(a,\infty\big)\\
		\Prb_\theta(S^2<b\sigma^2)&=\Prb_\theta\circ\left(\frac{S^2}{\sigma^2}\right)^{-1}\big((-\infty,b)\big)=:\nu\big(-\infty,b\big)
		\end{align*}
		Falls man $a$ als $(1-\frac{\al}{2})$-Quantil von $\nu$ und $b$ als $\al/2$-Quantil von $\mu$ wählt, dann gilt
		\[\Prb_\theta\big(\sigma^2\in[\dfrac{1}{a}S^2,\dfrac{1}{b}S^2]\big)\geq 1-\al\]
	\end{exm}

	\begin{definition}
		%TODO Def 6.27: VL 10.07.2017
	\end{definition}
	%TODO COUNTER
	\addtocounter{theorem}{-1}
	\begin{satz}
		Seien $X-1,...,X_n$ uiv und $X_1\sim\Nv(\mu,\sigma)$. Dann ist $M(x)=\dfrac{1}{n}\sum_{i=1}^{n}X_i(\omega)$, $S^2(\omega)=\dfrac{1}{1-n}\sum_{i=1}^{n}\big(X_i(\omega)-M(\omega)\big)$.
		%TODO Satz 6.27: VL 10.07.2017
	\end{satz}

	\begin{bem*}
		$t_{n-1}$ ist ähnlich wie $\Nv(0,1)$ und für große n gilt $\Prb_{t_n}\to\Nv(0,1)$
	\end{bem*}

	\begin{prop}\label{0628prop}
		Sei $\big(\Omega^n,\scF^{\otimes n},(\Prb_\theta^{\otimes n})$ statistisches Modell mit $\Prb_\theta=\Nv(\mu,\sigma^2)$, $f(\theta)=f\big((\mu,\sigma^2)\big)=\sigma^2$.\\
		Seien $q_\al$ das $\al$-Quantil der $\chi^2_{n-1}$-Verteilung. Dann ist die Abbildung 
		\[K:\Omega^n\to\Bor\big([0,\infty]\big),\quad x\mapsto\left[(n-1)\frac{S^2(x)}{q_{1-\al/2}},(n-1)\frac{S^2(x)}{q_{\al/2}}\right]\]
		ein Konfidenzintervall von $f(\theta)$ zum Irrtumsniveau $\al$.
	\end{prop}
	\begin{proof}
		Es gilt
		\begin{align*}
		\Prb_\theta\left(f(\theta<\frac{(n-1)S^2}{q_{1-\al/2}})\right)&=\Prb\left((n-1)S^2>\sigma^2q_{1-\al/2}\right)\\
		&=\Prb_\theta\left(\frac{n-1}{\sigma^2}S^2>q_{1-\al/2}\right)\\
		&=\Prb_{\chi^2_{n-1}}\left(Y>q_{1-\al/2}\right)=\al/2
		\end{align*}
		und $\Prb_\theta\big(f(\theta)>(n-1)^2\frac{S^2}{q_{\al/2}}\big)=...=\Prb_{\cha_{n-1}^2}(Y<q_{\al/2})=\frac{\al}{2}$.
	\end{proof}

	\begin{bem}
		\ref{0628prop} erscheint so also ob das Konfidenzintervall mit wachsendem $n$ größer wird. Kontraintuitiv!\\
		Es gilt
		\[\Prb_{\cha_n^2}(\mathrm{id}\leq c)=\Prb\left(\sum_{j=1}^{n}X_i\leq c\right)\]
		mit $X-i\sim\Gamma_{\frac{1}{2},\frac{1}{2}}$ (siehe \ref{0327???}c).\\
		Dann gilt $\Epv(X_i)=1$, $\Epv(X_i^2)=3$, also $\Var(X_i)=2$. Somit ist
		\begin{align*}
		\Prb_{\cha_{n-1}^2}\left((-\infty,c]\right)&=\Prb\left(\sum_{j=1}^{n}(X_j-1)\leq c-(n-1)\right)\\
		&=\Prb\left(\frac{1}{\sqrt{n-1}}\sum_{j=1}^{n}(X_j)\leq\frac{c}{\sqrt{n-1}}-\sqrt{n-1}\right)
		\end{align*}
		ist nach dem Gesetz der Großen Zahlen $\sim\Nv(0,2)$ für große $n$.
		%TODO Rest der VL 10.07.2017
	\end{bem}

%VL 11.07.2017
	\begin{exm}
		Sei $\Theta=\R\times R^+$, $\theta=(\mu,\sigma^2)$, $\Prb_\theta=\Nv(\mu,\sigma^2)$. Sei $\big(\Omega,\scF,(\Prb_\theta)_{\theta\in\Theta}\big)$ statistisches Modell. Nun sind $\mu$ und $\sigma^2
		$ unbekannt, gesucht ist ein Konfidenzintervall für $\mu$:\\
		Naiver Versuch: Berechne das Konfidenzintervall gemäß \ref{0622exm} für alle $\sigma^2$ und maximiere über $\sigma^2$. Dann egit sich
		\[K_{\sigma^2}(x)=\left[M(x)+\frac{\sigma}{\sqrt{n}}q_{\al/2},M(x)+\frac{\sigma
		}{\sqrt{n}}q_{(1-\al)/2}\right]\]
		Dann ergibt das Maximieren über $\sigma\in\R$ ergibt $K(x)=\R$.
	\end{exm}
	\begin{proof}
		Für alle $\theta\in\Theta$ gilt
		\[\Prb_\theta\left(M-\frac{\sqrt{S^2}}{\sqrt{n}}q_{1-\al/2}\geq \mu\right)=\Prb_\theta\left(M-\mu\geq \frac{\sqrt{S^2}}{\sqrt{n}}q_{1-\al/2}\right)=\Prb_\theta\left(\sqrt{n}\frac{M-\mu}{\sqrt{S^2}}\geq q_{1-\al/2}\right)\]
	\end{proof}
	
	\subsection{Tests}
	\begin{bem}
		Wichtigstes Werkzeug der Statistik: wissenschaftliche Studien, Medikamentenzulassung, Nachweis von Produktmängeln, ect. Dient zum Treffen von Ja-Nein-Entscheidungen anhand \enquote{verrauschter} Daten.
		\\
		Sei $\SM$ statistisches Modell. Wir teilen $\Theta$ in zwei Bereiche: $A_0,A_1\subset\Theta$, mit $A_0\dot{\cup}A_1=\Theta$.
		Dann beschreibt die
		\begin{itemize}
			\item \textbf{Nullhypothese}: $\theta\in A_0$ (Schreibe $H_0$ gilt)
			\item \textbf{Alternative}: $\theta\in A_1$ (Schreibe $H_1$ gilt)
		\end{itemize}
		Testproblem: Kann man aufgrund der Daten $x\in\Omega$ bestimmen, ob $\sigma\in A_0$ oder $\in A_1$ war?\\
		Dabei soll $\theta\in A_1$ nur sehr unwahrscheinlich (bwzüglich der Daten) fälschlicherweise angenommen werden.
		
	\end{bem}
	\begin{definition}
		Eine Abbildung $T:\Omega\to\{0,1\}$ heißt \textbf{Test}. Dabei bedeute
		\begin{description}
			\item[$T(x)=0$] als \enquote{$\theta$ liegt in $A_0$}.\\
			Wir sagen \textbf{Nullhypothese wird nicht abgelehnt}
			\item$T(x)=1$] als \enquote{$\theta$ liegt in $A_1$}.\\
			Wir sagen \textbf{Nullhypothese wird abgelehnt}
		\end{description}
		Falls $\theta\in A_0$  aber $T(x)=1$ so sagen wir die Daten führen zu einem \textbf{Fehler 1. Art}.\\
		Falls $\theta\in A_1$  aber $T(x)=0$ so sagen wir die Daten führen zu einem \textbf{Fehler 2. Art}.
	\end{definition}

	\begin{exmlist}
		\begin{exm}[Feuerwerkskörper]
			Behauptung des Herstellers: höchstens $3\%$ Ausschuss, \\
			$H_1$:$\theta>0.03$, $H_0:\theta\leq 0.03$.\\
			Falls die Daten suggerieren, dass $H_1$ gilt, dann gibt es Ärger für alle Beteiligten (Reklamation, Untersuchung, Prozess). Falls $H_1$ dagegen nicht erkannt wird, sind die Verluste gering.
		\end{exm}
		\begin{exm}[Wissenschaftlicher Durchbruch] 
			$H_0$ entspricht der \enquote{Lehrmeinung}, $H_1$ entspricht \enquote{Skandal, Sensation} (z.B. \enquote{Kalte Kernfusion},\enquote{Stammzellentherapie},...).\\
			Ein Falsches Behaupten von $H_1$ kostet Reputation, Karriere.\\
			Nicht erkennen von $H_1$ ist eine verpasste Chance aber nicht fatal.
		\end{exm}
	\end{exmlist}

	\begin{definition}
		Ein Test $T$ hält das \textbf{Irrtumsniveau} $\al>0$ ein, falls gilt:
		\[\forall\theta\in A_0:\Prb_\theta(T=1)<\al\]
		Zu $\theta\in A_1$ heißt die Zahl $G_T(\theta):=\Prb_\theta(T=1)$ die \textbf{Güte} (oder Schärfe, Machte) des Test bei $\theta$.
	\end{definition}

	\begin{bem}[Konstruktion von Tests] mit Konfidenzintervallen
		\begin{enumerate}
			\item Zweiseitiger Test: angenommen man hat ein Konfidenzintervall $K:\Omega\to\Omega'$ für die Größe $f(\theta)\in\Omega'$ zum Nievau $\al$ gegeben. Dann setzte man
			\[T:\Omega\to\{0,1\},\,T(x)=\begin{cases}
			!&\text{falls }K(x)\cap\{f(\theta:\theta\in A_0)\}=\emptyset\\
			0&\text{sonst}
			\end{cases}\]
			Dann gilt für alle $\theta_o\in A_0$
			\[\Prb_\theta(T=1)\Prb_{\theta_0}\big(\forall\theta\in A_0:f(\theta)\notin K\big)\leq\Prb_{\theta_0}\big(f(\theta_0)\notin K\big)=1-\Prb_{\theta_0}\big(f(\theta_0)\in K\big)\leq \al\]
			Interpretation, falls $T(x)=1$: Die Daten $x$ können nur mit sehr geringer Wahrscheinlichkeit von $\theta_0\in A_0$ erzeugt werden, sodass wir $\theta_0\in A_0$ ausschließen.
			\item einseitige Tests: mit Asymmetrischen Konfidenzintervallen. Oft ist $\Omega'=\R$, $A_0=(-\infty,c]$ oder $A_0=[c,\infty)$. Dann ist es besser statt wie in a) symmetrischer auch asymmetrische Konfidenzintervalle der Form $K(x)=[S(x),\infty)$ oder $K(x)=(-\infty,S(x)]$ zu wählen.
		\end{enumerate}
	\end{bem}

	\begin{exmlist}
		\begin{exm}[Gaußtest, Zweiseitig]
			Inhalt...
		\end{exm}
	\end{exmlist}




%VL 17.07.2017
\section{Markovketten}
	\begin{bem*}[Motivation]
		Sei $E$ eine Menge (=\enquote{Spielfeld}) mit abzählbar vielen Feldern (\enquote{Zuständen}) =Elemente von $E$.\\
		Auf jemden Feld $x\in E$ liegt ein Würfel bereit, der $N_r\leq |E|$ Seiten hat und mit Wahrscheinlichkeit $p(x,y)$ die Seite $y$ würfelt.\\
		Man geht dann auf das Feld $y$. 
	\end{bem*}

	\begin{exmlist}
		\begin{exm}[Irrfahrt]
			$E=\Z$, $p(x,x+1)=p(x,x-1)=\frac{1}{2}$.
		\end{exm}
		\begin{exm}[Galton-Watson-Populationsmodell]
			$E=\N_0$, $p(x,y)=\Prb\left(\sum_{j=1}^{n}Z_j=y\right)$, wobei $Z_j$ eine Zufallsvariable ist, die die Nachkommen des $j$-ten Individuum modelliert. Normalerweise sind die $Z_j$ uiv.
		\end{exm}
		\begin{exm}[Sammelbilder]
			Sei $E=\{1,...,N\}$ (\enquote{Anzahl der noch fehlenden Bilder})und
			\[p(x,y)=\begin{cases}
			x/N&\text{falls }y=x-1\\
			1-\frac{x}{N}&\text{falls }y=x\\
			0&\text{sonst}
			\end{cases}\]
		\end{exm}
	\end{exmlist}

	\begin{bem*}
		Wie hoch ist die Wahrscheinlichkeit in 2 Schriteenvon $x$ nach $z$ zu gehen?
		\begin{align*}
		\Prb(x\xrightarrow{\text{2 Schritte}}z)&=\Prb(x\to y,y\to z)\\
		&=\sum_{y\in E}\Prb(x\to y)\Prb(y\to z)\\
		&=\sum_{y\in E}p(x,y)p(y,z)
		\intertext{Vergleiche: Matrixmultiplkation}
		&=P^2(x,z)
		\end{align*}
		mit $P^2=P\cdot P$, $P=\big(p(x,y)\big)_{x,y\in E}$ (Matrix)\\
		Beachte dabei:
		\[\sum_{y\in E}p(x,y)=\sum_{y\in E}\Prb(x\to y)=\Prb(x\to E)=1\]
		(Zeilensumme =1)
	\end{bem*}

	\begin{definition}
		Eine Matrix $P=\big(p(x,y)\big)_{x,y\in E}$ heißt \textbf{stochastisch}, wenn gilt
		\begin{enumerate}[label=(\roman*)]
			\item $p(x,y)\geq 0$ $\forall x,y\in E$
			\item $\sum_{y\in E}p(x,y)=1$ $\forall x\in E$
		\end{enumerate}
	\end{definition}

	\begin{definition}
		Sei $E$ höchstens abzählbar, $P$ stochastische Matrix.\\
		Eine Folge $(X_n)$ von $E$-wertigen Zufallsvariablen heißt \textbf{Markovkette} (Mk) mit Übergangsmatrix $P$ (ÜM), falls für alle $n\in\N$ und für alle $x_1,...,x_n\in E$ gilt
		\[\Prb\left(X_{n+1}=x_{n+1}\vert X_0=x_0,...,X_n=x_n\right)=p(x_n,x_{n+1})\]
	\end{definition}
	
	\begin{bem*}
		Falls $\Prb(X_0=x)=\mu(x)$ für ein Wahrscheinlichkeitsmaß $\mu$, dann schreibt man auch $\Prb^\mu$ statt $\Prb$.\\ Falls
		\[\Prb(X_0=x)=\delta_y(x)=\begin{cases}
		1&\text{falls }y=x\\
		0&\text{sonst}
		\end{cases}\]
		schreibt man $\Prb^y$ statt $\Prb$.
	\end{bem*}

	\begin{prop}\label{0704prop}
		Sei $(X_n)$ Markovkette mit Übergangsmatrix $P$. Dann gilt für alle $x\in\N$, $x,y\in E$
		\[\Prb^x(X_n=y)=P^n(x,y)\]
		wobei $P^n$ die $n$-te Potenz von $P$ ist. 
	\end{prop}
	\begin{proof}
		Induktiv
	\end{proof}

	\begin{bem*}
		Es besteht eine starke Verbindung zur linearen Algebra. Falls $|E|=\infty$ wird $P$ trotzdem Matrix genannt.
	\end{bem*}

	\begin{prop}\label{0705prop}
		Sei $P$ stochastische Matrix. Dann gilt
		\begin{enumerate}
			\item $P1=1$, wobei $1(x)=1\forall x$.\\
			Das heißt, $1$ ist Rechts-Eigenvektor von $P_y$ zum Eigenwert $1$.
			\item Falls $|E|<\infty$, dann Existiert ein Zeilenvektor $\mu\neq 0$ mit $\mu P=\mu$. D.h. $\mu$ ist Links-Eigenvektor zu $P$,zum Eigenwert $1$.
		\end{enumerate}
	\end{prop}
	\begin{proof}
		\begin{enumerate}
			\item $P1(x)=\sum_{y\in E}p(x,y)1(y)=1$ (Zeilensumme)
			\item $P1=1$ genau dann wenn $\scp{w,P1}=\scp{w,1}$ für alle $w\in\R^E$.\\
			Genau dann wenn $\scp{P^*w,w,1}=0$ für alle $w\in R^E$.\\
			Genau dann wenn $\operatorname{\Img}(P^*w-1)\perp \scp{\{1\}}$.\\
			Dann ist also $\dim(\Kern(P^*-1))>0$.\\
			Aus dem Eigenwert 1 folgt, dass $P$ Zeileneigenwerte zum Eigenwert $1$.
		\end{enumerate}
	\end{proof}

	\begin{definition}
		Sei $(X_i)$ Markovkette auf $E$ mit Übergangsmatrix $P$.\\
		Ein Wahrscheinlichkeitsmaß $\hat{\mu}$ auf $E$ heißt \textbf{invariante Verteilung} von $(X_n)$, falls $\Prb^{\hat{\mu}}(X_1=y)=\hat{\mu}(\{y\})$.
		Dabei ist
		\begin{align*}
		\Prb^{\hat{\mu}}(X_1=y)&=\sum_{y\in E}\hat{\mu}(\{x\}) \Prb^x(X_1=y)\\
		&=\sum_{y\in E}\hat{\mu}(\{x\})p(xy)=(\mu P)(y)
		\end{align*}
		(und $\mu$ die Zähldichte von $\hat{\mu}$).
	\end{definition}

	\begin{bem}
		Invarianz von Verteilungen bedeutet, dass die Zähldichte $\mu$ von $\hat{\mu}$ ein Linkseigenvektor von $P$ ist.\\
		Nach \ref{0705prop} existiert für $|E|<\infty$ immer irgendein Linkseigenvektor.\\
		Ist dies auch die Zähldichte einer Verteilung?\\
		Dazu muss der Linkseigenvektor zusätzlich nur nichtnegative Einträge haben und Zeilensumme 1 haben.\\
		Der Satz von Perron-Frobenius (\enquote{Sei $A$ eine Matrix mit nichtnegativen Einträgen, $\norm{A}=\sup\{|Av|:|v|=1\}$, dann ist $w$ ein Eigenvektor von $A$ zum Eigenwert $\norm{A}$. Dann kann $w$ nicht-negativ gewählt werden.})
	\end{bem}

	\begin{bem*}
		Markovketten sind \enquote{gedächtnislose} Prozesse.\\
		Ist man zur Zeit $x\in X$, dann spielt es für die weitere Entwicklung keine Rolle, wie man dort hin gekommen ist.
	\end{bem*}

	\begin{satz}[Markov-Eigenschaft]\label{0708satzMarkov}
		Sei $X_n$ Markovkette mit Übergangsmatrix $P$. Dann gilt für alle $x,z\in E$, $n\in\N$, $A\in\Potset(E)^{\otimes\N_0}$ und $B\in E^n$, falls $\Prb\big((X_1,...,X_n)\in B,X_n=x\big)>0$, dann ist
		\begin{align*}
		\Prb^z\big((X_n,X_{n+1},X_{n+2},...)\in A\vert (X_0,...,X_{n-1})\in B,X_n=x\big)&=\Prb^z\big((X_n,X_{n+1},...)\in A\vert X_n=x\big)\\
		&=\Prb\big((X_1,X_2,...)\in A\big)
		\end{align*}
	\end{satz}
%TODO Ist das der Beweis zu 7.8?
	\begin{proof}
		Sei $k\in\N$, $y_1,...,y_k\in E$. Dann ist \begin{align*}
		&\Prb\big((x_0,...,x_{n-1})\in B,\,x_n=x,\, x_{n+1}=y_i\, \text{für alle $i\geq k$}\big)\\
		=&\sum_{\omega_1,...,\omega_{n-1}\in B}p(z,\omega_1)p(\omega_1,\omega_2)...p(\omega_{n-2},\omega_{n-1})p(\omega_{n-1},x)p(x_1,y_1)p(y_1,y_2)....p(y_{k-1},y_k)\\
		=&\Prb^k\big((x_1,...,x_{n-1})\in B,\,x_n=x\big)\Prb^x\big((x_1,...,x_k)=(y_1,...,y_k)\big)
		\end{align*}
		Daraus folgt die Behauptung für $A=\{v\in E^{\N_0}:(v_1,...=y_1,...,v_{k-1})\}$.\\
		Nach der Definition der bedingten Wahrscheinlichkeit sind das alle Zylindermengen. Es folgt mit \ref{0112satz} die Behauptung.
	\end{proof}

	\begin{exm}[Ruin des Spielers]
		Sei $X_n=$ Gewinng nach dem $n-$tenn Spiel, $E=\{-a,...,0,...,b\}$, $p(n,n+1)=p$, $p(n,n-1)=1-p$, falls $-a<n<b$.
		\begin{itemize}
		\item $p(-a,-a)=1$ (\enquote{Ruin})
		\item $p(b,b)=1$ (\enquote{Gewinnmitnahme})
		\end{itemize}
	\end{exm}

	%TODO Counter
	\stepcounter{equation}
	\begin{definition}\label{0710def}
		$z\in E$ heißt \textbf{absorbierend} bezüglich $\Prb$, falls $p(z,z)=1$.\\
		Dann heißt
		\[h_z(x):=\Prb^x\left(\exists N\in\N:\forall n\geq N:X_n=z\right)=\Prb\left(\bigcup_{N\geq 0}\bigcap_{n\geq N}\{x_n=z\}\right)\]
		die \textbf{Absorptionswahrscheinlichkeit in $z$ bei Startwert $x$}.
	\end{definition}
	
	\begin{definition}\label{0711def}
		Sei $(X_n)$ eine Markovkette, $z\in E$. Die Zufallsvariable
		\[\tau_z:\Omega\to\N\cup\{\infty\},\,\omega\mapsto\infty\{n\geq 1:X_n(\omega)=z\}\]
		heißt Trefferzeit (oder Eintrittszeit) bei $z$.
	\end{definition}
	
	%VL 18.07.2017
	\begin{lem}\label{0712lem}
		Sei $(X_n)$ Markovkette, $\tau_z$ Trefferzeit. Dann ist
		\[\{\omega\in\Omega:\tau_z=n\}\in\sigma(X_1,...,X_n)\autotag\label{eq:0702}\]
	\end{lem}
	\begin{proof}
		Es gilt\begin{align*}
		\{\omega\in\Omega:\tau_z(\omega)=n\}&=\{\omega\in\Omega:X_1(\omega)\neq z,...,X_{n-1}(\omega)\neq z,X_n(\omega)=z\}\\
		&=\bigcap_{k=1}^{n-1}X_k^{-1}\left(E\setminus\{z\}\right)\cap X_n^{-1}\big(\{z\}\big)\in\sigma(X_1,...,X_n)
		\end{align*}
	\end{proof}

	\begin{bem*}
		Die Messbarkeitseigenschaft \ref{eq:0702} macht $\tau_z$ zu einer sogenannten \textbf{Stoppzeit}. Um zu entscheiden, dass \enquote{gestoppt} wird musss man nciht \enquote{in die Zukunft Blicken}.
	\end{bem*}

	\begin{satz}\label{0713}
		Sei $(X_n)$ Markovkette, $z\in E$ absorbierend, $x\in E$ beliebig. Dann ist
		\[h_z(x)=\Prb^x(\tau_z<\infty)=\Prb^x\left(\lim\limits_{n\to\infty}X_n=z\right)\autotag\label{eq:0703}\]
		und
		\[h_z=\min\left\{h:E\to\R^+\text{ mit }h(z)=1\text{ und }\sum_{y\in E}p(x,y)h(y)=h(x)\right\} \autotag \label{eq:0704}\]
		(wobei $h\leq g\Leftrightarrow h(x)\leq g(x)\forall x\in E$)\\
		Da Minimum ist eindeutig.
	\end{satz}

	\begin{bem*}
		$h_z$ ist also Rechts-Eigenvektor von $P$, mit $h_z(z)=1$. Auch $1$ mit $1(x)=1$ ist Rechts-Eigenvektor, aber $h_z$ kann kleiner sein.
	\end{bem*}
	\begin{proof}
		Zu \ref{eq:0703}: 
	\begin{align*}
		\Prb^z(X_n=z\forall>\geq 1)&=\lim\limits_{k\to\infty}\Prb^z(X_n=z\forall n\leq k)\intertext{Da $\{X_n=z\forall n\leq k\}_k$ eine Folge absteigender Mengen ist}
		&=\lim\limits_{k\to\infty}P^k(z,z)?\lim\limits_{k\to\infty}p(z,z)^k=1
	\end{align*}
	Also gilt für alle $x\in E$
	\begin{align*}
	\Prb^x(X_n=z)&=\Prb^x(X_n=z)\Prb^z(X_i=z\forall i\geq 1)\\
	&\overset{\ref{0708satzMarkov}}{=}\Prb^x(X_n=zX_{n+1}=z\forall i\geq 1)\\
	&\xrightarrow{n\to\infty}\Prb\left(\bigcup_{n=1}^\infty\bigcap_{j=n}^\infty\{X_j=z\}\right)=h_z(x)
	\end{align*}
	Weiter gilt für alle $k\leq n$
	\begin{align*}
	\Prb^x(X_n=z\vert \tau_z=k)&=\Prb^x(X_n=z\vert \tau_z=k,X_k=z)\\
	&\overset{\ref{0708satzMarkov}}{=}\Prb^x(X_n=z\vert X_k=z)=\Prb^z(X_{n-k}=z)=1
	\end{align*}
	Dann folgt mit \ref{0712lem}, dass
	\begin{align*}
	\underbrace{\Prb^x(X_n=z)}_{\xrightarrow{n\to\infty}h_z(x)}&=\sum_{k=1}^n\Prb^x(\tau_z=k)\overbrace{\Prb^x(X-n=z\vert \tau_z=k)}^{=1}\\
	&=\Prb(\tau_n\leq n)\xrightarrow{n\to\infty}\Prb(\tau_z<\infty)
	\end{align*}
	\\
	Zu \ref{eq:0704}:
	\begin{enumerate}
		\item $h_z\in\{h:E\to\R^+,h(z)=1,Ph=h\}$, denn $h_z(x)\geq 0$ als Wahrscheinlichkeit, $h(z)=1$ und
		\begin{align*}
		\sum_{y\in E}p(x,y)h_z(y)=\sum_{y\in E}\Prb^x(X_1=y)\Prb^y(\{\exists N\in\N:\forall n\geq N:X_n=z\})\\
		&=\sum_{y\in E}\Prb^x(X_n=y)\Prb(\exists N\in\N_0:\forall n\geq N:X_{n+1}=z\vert X_1=y)\\
		&\overset{\ref{0304satz}a)}{=}\Prb^x(\exists N\in\N:\forall n\geq N:X_{n+1}=z)=h_z(x)
		\end{align*}
		\item $h_z$ ist maximale. Sei $\tilde h>\geq 0$, $\tilde h(x)=1$ und $P\tilde h=\tilde h$, dann ist
		\[\tilde h(x)=P^n\tilde h(x)=\sum_{x\in E}P^n(x,y)\tilde h(y)\geq ...\] %TODO Bew Gl 7.5: VL 18.07.2017
	\end{enumerate}
	\end{proof}
	
	\begin{bem}
		für $z\in E_0\subset E$ ($\phi$ beliebige Funktion) und $Ph(y)=h(y)\forall y\notin E_0$. 
	\end{bem}
	%TODO Bem 7.14 VL 18.07.2017
	
	\begin{exm}[Galton-Watson-Verzweigungsprozess]
		Sei $n$ die Anzahl der Individuen, jedes davon bekommt $k$ Kinder mit Wahrscheinlichkeit $\rho(k)$ und stirbt dann.
		Dann ist also $E=\N_0$ und
		\begin{align*}
		p(n,m)=\sum_{\substack{k_1,...,k_n=1\\\sum_{i=1}^\infty}k_i=m}^{\infty}\rho(k_1)...\rho(k_n)&=\sum_{k_1=0}^{m}\sum_{k_2=0}^{m-k_1}...\sum_{k_n=0}^{m-\sum_{i=1}^{n-1}k_i}\rho(k_1)...\rho(k_n)
		\end{align*}
		Da $\rho(0,0)=1$ ist $0$ absorbierend.\\
		\\
		Frage: Wie groß ist $\Prb(\text{Population stibt aus}\mid\text{Anfangspopulation=1})$. Also was ist $\Prb(\tau_0<\infty)$?\\
		Triviale Fälle:
		\[\rho(0)=\begin{cases}
		1&\text{dann ist }\Prb^1(\tau_0<\infty)=1\\
		0&\text{dann ist }\Prb^1(\tau_0<\infty)=0
		\end{cases}\]
		Für $0<\rho(0)<1$ gilt:
		\begin{enumerate}
			\item Für alle $k<,n\geq 0$ ist $\Prb^k(X_n=0)=\left[\Prb^1(X_n=0)\right]^k$.
			\begin{proof}Induktiv
				\begin{description}
					\item[$n=0$] stimmt
					\item[Induktionsschritt]:
					\begin{align*}
					%TODO Bew 7.15 VL 18.07.2017
					&=\sum_{\substack{l_1,...,l_k=0\\\sum_{i=1}^{k}l_i=l}}^{\infty}\rho(l_1)...\rho(l_k)\underbrace{\Prb^l(X_n=0)}_{\text{IndV.}=\Prb^1(X_n=0)^l}\\
					&=\sum_{l_1,...,l_k=0}^{\infty}\rho(l_1)...\rho(l_n)\left(\Prb^1(X_n=0)\right)^{l_1+l_2+...+l_k}\\
					&=\left(\sum_{l=0}^{\infty}p(1,l)\Prb^l(X_n=0)\right)^k\\
					&=\left(\Prb(X_{n+1}=0)\right)^k
					\end{align*}
				\end{description}
			\end{proof}
			\item Sei $q(k):=\Prb^k(\tau_0<\infty)$.\\
			\begin{align*}
			&=\lim\lim\limits_{n\to\infty}\Prb^k(\tau_0\leq n)=\lim\limits_{n\to\infty}\Prb^k(X_n=0)\\
			&\overset{a)}{=}\lim\limits_{n\to\infty}\left(\Prb^1(X_n=0)\right)^k=\left[\lim\limits_{n\to\infty}\Prb^1(X_n=0)\right]^k=q(1)^k
			\end{align*}
			\item $q(1)=h_0(1)$ ist harmonisch. d.h.
			\begin{align*}
			q(1)=Pq(1)&=\sum_{k=0}^{\infty}p(1,k)q(k)\\
			&\overset{b)}{=}\sum_{k=0}^\infty\rho(k)q(1)^k\\
			&=\varphi_\rho(q(1))
			\end{align*}
			Dabei ist $\varphi$ die erzeugenden FUnktion von $\rho$ (\ref{0432def}).\\
			Also ist $q(1)$ ein \textbf{Fixpunkt} von $\varphi_\rho$ un zwar der kleinste. Denn falls $s$ ein beliebiger Fixpunkt von $\varphi_\rho$ ist, dann gilt $\tilde h(k):=s^k:P(\tilde h)=\tilde h$.\\
			Also ist $\tilde h$ harmonisch bezüglich $P$. Da aber $q(1)$ die minimal harmonsiche Funktion ist gilt $s\geq q(1)$.\\
			Die Funktion $s\mapsto \varphi_\rho(s)=\sum_{k=0}^{\infty}\varrho(k)s^k$ hat immer den Fixpunkt $s=1$.\\
			Falls $\rho(0)+\rho(1)=1$, dann ist $\varphi_\rho''(x)=0$. Also 
		\end{enumerate}
	\end{exm}
\end{document}